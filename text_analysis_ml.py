#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã¨æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹å¾“æ¥­å“¡æº€è¶³åº¦äºˆæ¸¬æ©Ÿèƒ½
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªã®è‡ªç„¶è¨€èªå‡¦ç†ç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import janome
    from janome.tokenizer import Tokenizer
    TOKENIZER_TYPE = "janome"
    tokenizer = Tokenizer()
except ImportError:
    try:
        import MeCab
        TOKENIZER_TYPE = "mecab"
        mecab = MeCab.Tagger("-Owakati")
    except ImportError:
        TOKENIZER_TYPE = "simple"

def preprocess_japanese_text(text):
    """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
    if not text or pd.isna(text):
        return ""
    
    text = str(text).strip()
    if not text:
        return ""
    
    # åŸºæœ¬çš„ãªå‰å‡¦ç†
    text = re.sub(r'[^\w\s]', '', text)  # å¥èª­ç‚¹ç­‰ã‚’é™¤å»
    text = re.sub(r'\s+', ' ', text)     # è¤‡æ•°ã®ç©ºç™½ã‚’1ã¤ã«
    text = text.lower()                   # å°æ–‡å­—ã«å¤‰æ›
    
    # å½¢æ…‹ç´ è§£æã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    tokens = japanese_tokenizer(text)
    return ' '.join(tokens)

def japanese_tokenizer(text):
    """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®å½¢æ…‹ç´ è§£æ"""
    if not text or pd.isna(text):
        return []
    
    text = str(text).strip()
    if not text:
        return []
    
    try:
        if TOKENIZER_TYPE == "janome":
            # Janome tokenizer
            tokens = []
            for token in tokenizer.tokenize(text, wakati=True):
                if hasattr(token, 'surface'):
                    tokens.append(token.surface)
                else:
                    tokens.append(str(token))
        elif TOKENIZER_TYPE == "mecab":
            tokens = mecab.parse(text).strip().split()
        else:
            # ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†å‰²ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰- æ—¥æœ¬èªå¯¾å¿œ
            import re
            # æ—¥æœ¬èªæ–‡å­—ã€è‹±æ•°å­—ã€ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã‚’æŠ½å‡º
            tokens = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯\w]+', text)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° - ã‚ˆã‚ŠæŸ”è»Ÿã«
        filtered_tokens = []
        for token in tokens:
            token = str(token).strip()
            # 1æ–‡å­—ä»¥ä¸Šã§ã€æ•°å­—ã®ã¿ã§ã¯ãªã„ã‚‚ã®ã‚’æ®‹ã™
            if len(token) >= 1 and not token.isdigit():
                filtered_tokens.append(token)
        
        return filtered_tokens
    except Exception as e:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†å‰²
        import re
        try:
            tokens = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯\w]+', text)
            return [t for t in tokens if len(t) >= 1 and not t.isdigit()]
        except:
            return [text]  # æœ€å¾Œã®æ‰‹æ®µ

def create_sample_data_for_ml(n_samples=150):
    """æ©Ÿæ¢°å­¦ç¿’ç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
    np.random.seed(42)
    
    # ä¸»è¦KPIã®ã‚µãƒ³ãƒ—ãƒ«ã‚¹ã‚³ã‚¢ç”Ÿæˆ
    data = []
    
    # ä½æº€è¶³åº¦ã‚°ãƒ«ãƒ¼ãƒ— (ä¸‹ä½20% - ãƒ©ãƒ™ãƒ«1)
    low_satisfaction_samples = int(n_samples * 0.2)
    for i in range(low_satisfaction_samples):
        recommend_score = np.random.choice([0, 1, 2, 3, 4, 5, 6], p=[0.1, 0.15, 0.2, 0.25, 0.15, 0.1, 0.05])
        overall_satisfaction = np.random.choice([1, 2, 3], p=[0.4, 0.4, 0.2])
        long_term_intention = np.random.choice([1, 2, 3], p=[0.5, 0.3, 0.2])
        sense_of_contribution = np.random.choice([1, 2, 3], p=[0.4, 0.4, 0.2])
        
        # ä½æº€è¶³åº¦ã«é–¢é€£ã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        negative_words = [
            "ä¸æº€", "æ”¹å–„", "å•é¡Œ", "èª²é¡Œ", "å³ã—ã„", "å¤§å¤‰", "å›°é›£", "ã‚¹ãƒˆãƒ¬ã‚¹", "ç–²åŠ´", "è² æ‹…",
            "ä¸å®‰", "å¿ƒé…", "æœŸå¾…", "å¸Œæœ›", "è¦æœ›", "æ®‹æ¥­", "å¿™ã—ã„", "æ™‚é–“", "çµ¦ä¸", "è©•ä¾¡",
            "ä¸Šå¸", "åŒåƒš", "äººé–“é–¢ä¿‚", "ç’°å¢ƒ", "åˆ¶åº¦", "ã‚·ã‚¹ãƒ†ãƒ ", "æ¥­å‹™", "ä»•äº‹", "ä¼šç¤¾",
            "çµŒå–¶", "æˆ¦ç•¥", "æ–¹é‡", "å¤‰æ›´", "æ”¹é©", "å°†æ¥", "ã‚­ãƒ£ãƒªã‚¢", "æˆé•·", "æ©Ÿä¼š"
        ]
        
        comment_words = np.random.choice(negative_words, size=np.random.randint(8, 15), replace=True)
        comment = " ".join(comment_words) + "ã«ã¤ã„ã¦ä¸æº€ã‚’æ„Ÿã˜ã¦ã„ã¾ã™ã€‚æ”¹å–„ãŒå¿…è¦ã ã¨æ€ã„ã¾ã™ã€‚"
        
        data.append({
            'recommend_score': recommend_score,
            'overall_satisfaction': overall_satisfaction,
            'long_term_intention': long_term_intention,
            'sense_of_contribution': sense_of_contribution,
            'comment': comment,
            'is_low_satisfaction': 1  # ä¸‹ä½20%
        })
    
    # ä¸­ãƒ»é«˜æº€è¶³åº¦ã‚°ãƒ«ãƒ¼ãƒ— (ä¸Šä½80% - ãƒ©ãƒ™ãƒ«0)
    high_satisfaction_samples = n_samples - low_satisfaction_samples
    for i in range(high_satisfaction_samples):
        recommend_score = np.random.choice([6, 7, 8, 9, 10], p=[0.1, 0.2, 0.3, 0.25, 0.15])
        overall_satisfaction = np.random.choice([3, 4, 5], p=[0.3, 0.4, 0.3])
        long_term_intention = np.random.choice([3, 4, 5], p=[0.3, 0.4, 0.3])
        sense_of_contribution = np.random.choice([3, 4, 5], p=[0.3, 0.4, 0.3])
        
        # é«˜æº€è¶³åº¦ã«é–¢é€£ã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        positive_words = [
            "æº€è¶³", "è‰¯ã„", "ç´ æ™´ã‚‰ã—ã„", "å„ªç§€", "å……å®Ÿ", "å®‰å¿ƒ", "å¿«é©", "åŠ¹ç‡", "æˆé•·", "å­¦ç¿’",
            "é”æˆ", "æˆåŠŸ", "è©•ä¾¡", "èªã‚ã‚‰ã‚Œã‚‹", "æ”¯æ´", "ã‚µãƒãƒ¼ãƒˆ", "å”åŠ›", "ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯", "ä¿¡é ¼", "å°Šé‡",
            "è‡ªç”±", "è£é‡", "è²¬ä»»", "æŒ‘æˆ¦", "æ©Ÿä¼š", "å¯èƒ½æ€§", "å°†æ¥", "ã‚­ãƒ£ãƒªã‚¢", "æ˜‡é€²", "æ˜‡æ ¼",
            "çµ¦ä¸", "å¾…é‡", "ç¦åˆ©åšç”Ÿ", "ä¼‘æš‡", "åƒãã‚„ã™ã„", "ç’°å¢ƒ", "åˆ¶åº¦", "ã‚·ã‚¹ãƒ†ãƒ ", "åŠ¹ç‡åŒ–"
        ]
        
        comment_words = np.random.choice(positive_words, size=np.random.randint(8, 15), replace=True)
        comment = " ".join(comment_words) + "ã«æº€è¶³ã—ã¦ãŠã‚Šã€ä»Šå¾Œã‚‚ç¶™ç¶šã—ã¦åƒããŸã„ã¨æ€ã„ã¾ã™ã€‚"
        
        data.append({
            'recommend_score': recommend_score,
            'overall_satisfaction': overall_satisfaction,
            'long_term_intention': long_term_intention,
            'sense_of_contribution': sense_of_contribution,
            'comment': comment,
            'is_low_satisfaction': 0  # ä¸Šä½80%
        })
    
    return pd.DataFrame(data)

def identify_low_performers(df, kpi_column, threshold_percentile=20):
    """ä¸»è¦KPIã®ä¸‹ä½20%ã‚’ç‰¹å®š"""
    if kpi_column not in df.columns:
        return pd.Series([0] * len(df))
    
    threshold = df[kpi_column].quantile(threshold_percentile / 100)
    return (df[kpi_column] <= threshold).astype(int)

def preprocess_text_features(comments):
    """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ã¨ç‰¹å¾´é‡æŠ½å‡º"""
    if len(comments) == 0:
        return pd.DataFrame()
    
    # ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    cleaned_comments = []
    for comment in comments:
        if pd.isna(comment):
            cleaned_comments.append("")
        else:
            # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            text = str(comment).strip()
            text = re.sub(r'[^\w\s]', ' ', text)  # å¥èª­ç‚¹ã‚’é™¤å»
            text = re.sub(r'\s+', ' ', text)  # è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«
            cleaned_comments.append(text)
    
    # TF-IDFç‰¹å¾´é‡ã®ä½œæˆ
    try:
        # ã‚«ã‚¹ã‚¿ãƒ å½¢æ…‹ç´ è§£æå™¨ã‚’ä½¿ç”¨
        def custom_tokenizer(text):
            tokens = japanese_tokenizer(text)
            return tokens if tokens else ['']
        
        # TF-IDF Vectorizerï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
        vectorizer = TfidfVectorizer(
            tokenizer=custom_tokenizer,
            max_features=50,  # ä¸Šä½50å€‹ã®ç‰¹å¾´é‡ï¼ˆå°‘ãªãã—ã¦å®‰å®šæ€§å‘ä¸Šï¼‰
            min_df=1,  # æœ€ä½1å›å‡ºç¾ï¼ˆç·©ãè¨­å®šï¼‰
            max_df=0.95,  # 95%ä»¥ä¸Šã®æ–‡æ›¸ã«å‡ºç¾ã™ã‚‹å˜èªã¯é™¤å¤–ï¼ˆã‚ˆã‚Šç·©ãï¼‰
            ngram_range=(1, 1),  # 1-gramã®ã¿ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã«ï¼‰
            lowercase=False,  # æ—¥æœ¬èªã®å ´åˆã¯å¤§æ–‡å­—å°æ–‡å­—å¤‰æ›ã‚’ç„¡åŠ¹åŒ–
            token_pattern=None  # ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ç„¡åŠ¹åŒ–
        )
        
        tfidf_matrix = vectorizer.fit_transform(cleaned_comments)
        feature_names = vectorizer.get_feature_names_out()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
        tfidf_df = pd.DataFrame(
            tfidf_matrix.toarray(),
            columns=[f"word_{name}" for name in feature_names]
        )
        
        return tfidf_df, vectorizer
        
    except Exception as e:
        st.error(f"ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªå˜èªã‚«ã‚¦ãƒ³ãƒˆ
        word_counts = pd.DataFrame()
        return word_counts, None

def train_ensemble_models(X, y):
    """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
    import streamlit as st
    
    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ãƒã‚§ãƒƒã‚¯
    st.write("ğŸ” **train_ensemble_modelsé–¢æ•°å†…ã§ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯:**")
    st.write(f"- X shape: {X.shape}")
    st.write(f"- X dtypes: {dict(X.dtypes.value_counts())}")
    st.write(f"- y shape: {y.shape}")
    st.write(f"- y dtype: {y.dtype}")
    st.write(f"- y unique values: {sorted(y.unique())}")
    
    # éæ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚§ãƒƒã‚¯
    non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()
    if non_numeric_cols:
        st.error(f"âŒ éæ•°å€¤ã‚«ãƒ©ãƒ ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {non_numeric_cols}")
        for col in non_numeric_cols:
            st.write(f"- {col}: {X[col].dtype}, ã‚µãƒ³ãƒ—ãƒ«å€¤: {X[col].head(3).tolist()}")
        raise ValueError(f"éæ•°å€¤ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã™: {non_numeric_cols}")
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å‰ã«æœ€ã‚‚ç¢ºå®Ÿãªæ–¹æ³•ã§æ•°å€¤å‹ã«å¤‰æ›
    try:
        st.write("ğŸ”§ **æœ€ã‚‚ç¢ºå®Ÿãªæ•°å€¤å‹å¤‰æ›å®Ÿè¡Œä¸­...**")
        
        # X ã®å¤‰æ› - numpy arrayçµŒç”±ã§ç¢ºå®Ÿã«å¤‰æ›
        X_array = X.values
        st.write(f"  - X array shape: {X_array.shape}, dtype: {X_array.dtype}")
        
        # å…¨ã¦ã®å€¤ãŒæ•°å€¤ã«å¤‰æ›å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        try:
            X_float_array = X_array.astype(np.float64)
        except (ValueError, TypeError) as conv_err:
            st.error(f"âŒ Xé…åˆ—ã®æ•°å€¤å¤‰æ›ã‚¨ãƒ©ãƒ¼: {conv_err}")
            # å„åˆ—ã‚’å€‹åˆ¥ã«ãƒã‚§ãƒƒã‚¯
            for i, col in enumerate(X.columns):
                try:
                    X.iloc[:, i].astype(np.float64)
                except Exception as col_error:
                    st.error(f"  åˆ— '{col}' (index {i}) ã§å¤‰æ›ã‚¨ãƒ©ãƒ¼: {col_error}")
                    st.write(f"  ã‚µãƒ³ãƒ—ãƒ«å€¤: {X.iloc[:5, i].tolist()}")
            raise conv_err
        
        # æ–°ã—ã„DataFrameã‚’ä½œæˆ
        X = pd.DataFrame(X_float_array, columns=X.columns, index=X.index)
        
        # y ã®å¤‰æ›
        y_array = y.values
        st.write(f"  - y array shape: {y_array.shape}, dtype: {y_array.dtype}")
        y = pd.Series(y_array.astype(np.int64), index=y.index)
        
        st.success("âœ… æœ€ã‚‚ç¢ºå®Ÿãªæ•°å€¤å‹å¤‰æ›å®Œäº†")
        st.write(f"  - å¤‰æ›å¾Œ X dtypes: {dict(X.dtypes.value_counts())}")
        st.write(f"  - å¤‰æ›å¾Œ y dtype: {y.dtype}")
        
    except Exception as e:
        st.error(f"âŒ æ•°å€¤å‹å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        st.code(traceback.format_exc())
        raise
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    try:
        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒç¢ºèª
        class_counts = pd.Series(y).value_counts().sort_index()
        st.write(f"ğŸ“Š ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {dict(class_counts)}")
        
        # æœ€å°ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ãƒã‚§ãƒƒã‚¯
        min_class_count = class_counts.min()
        if min_class_count < 2:
            st.warning(f"âš ï¸ æœ€å°ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã™ãã¾ã™: {min_class_count}ä»¶")
            st.write("stratifyãªã—ã§ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã‚’å®Ÿè¡Œã—ã¾ã™")
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.3, random_state=42
            )
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.3, random_state=42, stratify=y
            )
        
        st.write(f"âœ… ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†: è¨“ç·´{X_train.shape[0]}ä»¶, ãƒ†ã‚¹ãƒˆ{X_test.shape[0]}ä»¶")
        
        # åˆ†å‰²å¾Œã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒç¢ºèª
        train_class_counts = pd.Series(y_train).value_counts().sort_index()
        test_class_counts = pd.Series(y_test).value_counts().sort_index()
        st.write(f"  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {dict(train_class_counts)}")
        st.write(f"  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {dict(test_class_counts)}")
        
    except Exception as e:
        st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        st.code(traceback.format_exc())
        raise
    
    # è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©
    models = {
        'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),
        'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42, max_depth=10),  # è»½é‡åŒ–
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=50, random_state=42, max_depth=6)  # è»½é‡åŒ–
    }
    
    trained_models = {}
    model_scores = {}
    
    for name, model in models.items():
        try:
            st.write(f"ğŸ¤– **{name}ã®è¨“ç·´ä¸­...**")
            
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            model.fit(X_train, y_train)
            st.write(f"  âœ… {name}ã® fit() å®Œäº†")
            
            # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')
            st.write(f"  âœ… {name}ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†")
            
            # ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢
            test_score = model.score(X_test, y_test)
            train_score = model.score(X_train, y_train)
            st.write(f"  âœ… {name}ã®ã‚¹ã‚³ã‚¢è¨ˆç®—å®Œäº† (train: {train_score:.3f}, test: {test_score:.3f})")
            
            trained_models[name] = model
            model_scores[name] = {
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'test_score': test_score,
                'train_score': train_score
            }
            
        except Exception as e:
            st.error(f"âŒ {name}ã®è¨“ç·´ã§ã‚¨ãƒ©ãƒ¼: {e}")
            st.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:")
            import traceback
            st.code(traceback.format_exc())
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ç¶šè¡Œ
    
    if not trained_models:
        raise ValueError("å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãŒå¤±æ•—ã—ã¾ã—ãŸ")
    
    st.success(f"âœ… {len(trained_models)}å€‹ã®ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")
    return trained_models, model_scores, X_test, y_test

def filter_meaningful_words(word):
    """æ„å‘³ã®ã‚ã‚‹å˜èªã®ã¿ã‚’æŠ½å‡º"""
    # é™¤å¤–ã™ã‚‹å˜èªãƒ‘ã‚¿ãƒ¼ãƒ³
    meaningless_patterns = [
        r'^[ã‚-ã‚“]{1,2}$',  # ã²ã‚‰ãŒãª1-2æ–‡å­—
        r'^[ã‚¢-ãƒ³]{1,2}$',  # ã‚«ã‚¿ã‚«ãƒŠ1-2æ–‡å­—
        r'^[ã§ã™|ã¾ã™|ã ã‘|ãªã©|ã¾ãŸ|ã®ã§|ã‹ã‚‰|ã¾ã§|ã§ã¯|ã«ã¯|ã«ã‚‚|ã¦ã‚‚|ã§ã‚‚|ã¨ã¯|ãªã—|ãªã|ã—ã¦|ã•ã‚Œã‚‹|ã•ã‚ŒãŸ|ã„ã‚‹|ã‚ã‚‹|ã™ã‚‹|ãªã‚‹|ã‚Œã‚‹|ã‚‰ã‚Œã‚‹|ã›ã‚‹|ã•ã›ã‚‹|ãŸã„|ãªã„|ã ã‚ã†|ã§ã—ã‚‡ã†|ã‹ã‚‚ã—ã‚Œ]',
        r'^[0-9]+$',  # æ•°å­—ã®ã¿
        r'^[a-zA-Z]{1,2}$',  # è‹±å­—1-2æ–‡å­—
    ]
    
    # æ„å‘³ã®ã‚ã‚‹å˜èªã®å“è©ãƒ‘ã‚¿ãƒ¼ãƒ³
    meaningful_words = [
        'è·å ´', 'ç’°å¢ƒ', 'ä»•äº‹', 'æ¥­å‹™', 'çµ¦ä¸', 'å¹´å', 'æ®‹æ¥­', 'ä¼‘æš‡', 'æœ‰çµ¦', 'è©•ä¾¡', 'æ˜‡é€²', 'æ˜‡æ ¼',
        'ä¸Šå¸', 'åŒåƒš', 'éƒ¨ä¸‹', 'ãƒãƒ¼ãƒ ', 'çµ„ç¹”', 'ä¼šç¤¾', 'ä¼æ¥­', 'çµŒå–¶', 'ç®¡ç†', 'åˆ¶åº¦', 'ã‚·ã‚¹ãƒ†ãƒ ',
        'æº€è¶³', 'ä¸æº€', 'æœŸå¾…', 'å¸Œæœ›', 'è¦æœ›', 'æ”¹å–„', 'å•é¡Œ', 'èª²é¡Œ', 'å›°é›£', 'ã‚¹ãƒˆãƒ¬ã‚¹', 'è² æ‹…',
        'æˆé•·', 'ç™ºå±•', 'å‘ä¸Š', 'å­¦ç¿’', 'ç ”ä¿®', 'æ•™è‚²', 'ã‚¹ã‚­ãƒ«', 'èƒ½åŠ›', 'çµŒé¨“', 'çŸ¥è­˜',
        'æ™‚é–“', 'åŠ¹ç‡', 'ç”Ÿç”£æ€§', 'å“è³ª', 'å®‰å…¨', 'å¥åº·', 'ç¦åˆ©', 'åšç”Ÿ', 'å¾…é‡', 'æ¡ä»¶',
        'ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³', 'é–¢ä¿‚', 'å”åŠ›', 'æ”¯æ´', 'ã‚µãƒãƒ¼ãƒˆ', 'ç†è§£', 'ä¿¡é ¼', 'å°Šé‡',
        'ãƒ¯ãƒ¼ã‚¯', 'ãƒ©ã‚¤ãƒ•', 'ãƒãƒ©ãƒ³ã‚¹', 'ãƒ•ãƒ¬ãƒƒã‚¯ã‚¹', 'ãƒªãƒ¢ãƒ¼ãƒˆ', 'åœ¨å®…', 'æŸ”è»Ÿ', 'è‡ªç”±',
        'è²¬ä»»', 'æ¨©é™', 'è£é‡', 'æ±ºå®š', 'åˆ¤æ–­', 'æ–¹é‡', 'æˆ¦ç•¥', 'ç›®æ¨™', 'è¨ˆç”»', 'å®Ÿè¡Œ'
    ]
    
    # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
    for pattern in meaningless_patterns:
        if re.match(pattern, word):
            return False
    
    # 2æ–‡å­—ä»¥ä¸‹ã®å ´åˆã¯æ„å‘³ã®ã‚ã‚‹å˜èªãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹å ´åˆã®ã¿è¨±å¯
    if len(word) <= 2:
        return word in meaningful_words
    
    # 3æ–‡å­—ä»¥ä¸Šã¯åŸºæœ¬çš„ã«è¨±å¯ï¼ˆãŸã ã—æ˜ã‚‰ã‹ã«æ„å‘³ã®ãªã„ã‚‚ã®ã¯é™¤å¤–ï¼‰
    return True

def visualize_feature_importance(models, feature_names, top_n=15):
    """ç‰¹å¾´é‡é‡è¦æ€§ã®å¯è¦–åŒ–ï¼ˆæ—¥æœ¬èªç‰ˆï¼‰"""
    fig = make_subplots(
        rows=len(models), cols=1,
        subplot_titles=[f"{name} - ç‰¹å¾´é‡é‡è¦æ€§" for name in models.keys()],
        vertical_spacing=0.15
    )
    
    for i, (model_name, model) in enumerate(models.items(), 1):
        if hasattr(model, 'feature_importances_'):
            # ç‰¹å¾´é‡é‡è¦æ€§ã‚’å–å¾—
            importances = model.feature_importances_
            
            # æ„å‘³ã®ã‚ã‚‹ç‰¹å¾´é‡ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            meaningful_features = []
            meaningful_importances = []
            
            for idx, importance in enumerate(importances):
                feature_name = feature_names[idx]
                
                # ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã®å ´åˆ
                if feature_name.startswith('word_'):
                    word = feature_name.replace('word_', '')
                    if filter_meaningful_words(word) and importance > 0.001:  # é‡è¦åº¦é–¾å€¤ã‚‚è¨­å®š
                        meaningful_features.append(word)
                        meaningful_importances.append(importance)
                else:
                    # æ•°å€¤ç‰¹å¾´é‡ã¯ãã®ã¾ã¾
                    meaningful_features.append(feature_name)
                    meaningful_importances.append(importance)
            
            # é‡è¦æ€§ã§ã‚½ãƒ¼ãƒˆ
            sorted_indices = np.argsort(meaningful_importances)[::-1][:top_n]
            top_features = [meaningful_features[idx] for idx in sorted_indices]
            top_importances = [meaningful_importances[idx] for idx in sorted_indices]
            
            # ãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆè¿½åŠ 
            fig.add_trace(
                go.Bar(
                    y=top_features[::-1],  # é‡è¦åº¦é †ã«è¡¨ç¤º
                    x=top_importances[::-1],
                    orientation='h',
                    name=model_name,
                    showlegend=False,
                    marker=dict(
                        color=top_importances[::-1],
                        colorscale='Viridis',
                        showscale=i==1  # æœ€åˆã®ã‚°ãƒ©ãƒ•ã®ã¿ã‚«ãƒ©ãƒ¼ãƒãƒ¼è¡¨ç¤º
                    ),
                    text=[f'{imp:.3f}' for imp in top_importances[::-1]],
                    textposition='outside'
                ),
                row=i, col=1
            )
    
    fig.update_layout(
        height=350 * len(models),
        title="ç‰¹å¾´é‡é‡è¦æ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒï¼‰",
        title_font_size=16
    )
    
    fig.update_xaxes(title_text="é‡è¦æ€§ã‚¹ã‚³ã‚¢")
    
    return fig

def create_prediction_summary(models, model_scores):
    """äºˆæ¸¬çµæœã‚µãƒãƒªãƒ¼ã®ä½œæˆ"""
    summary_data = []
    
    for model_name, scores in model_scores.items():
        summary_data.append({
            'ãƒ¢ãƒ‡ãƒ«': model_name,
            'è¨“ç·´ç²¾åº¦': f"{scores['train_score']:.3f}",
            'ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç²¾åº¦': f"{scores['cv_mean']:.3f} (Â±{scores['cv_std']:.3f})",
            'ãƒ†ã‚¹ãƒˆç²¾åº¦': f"{scores['test_score']:.3f}"
        })
    
    return pd.DataFrame(summary_data)

def load_real_data_for_analysis():
    """å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§åˆ†æç”¨ã«æº–å‚™"""
    try:
        import os
        
        # Streamlit Cloudå¯¾å¿œ: è¤‡æ•°ã®ãƒ‘ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
        excel_paths = [
            'data.xlsx',  # Streamlit Cloudç”¨
            '/Users/sugayayoshiyuki/Desktop/æ¡ç”¨å¯è¦–åŒ–ã‚µãƒ¼ãƒ™ã‚¤/å¾“æ¥­å“¡èª¿æŸ».xlsx'  # ãƒ­ãƒ¼ã‚«ãƒ«ç”¨
        ]
        
        excel_path = None
        for path in excel_paths:
            if os.path.exists(path):
                excel_path = path
                break
        
        if excel_path:
            df = pd.read_excel(excel_path, sheet_name='Responses', header=0)
            
            # å¿…è¦ãªã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèªã¨æ­£è¦åŒ–
            column_mapping = {
                'ç·åˆæº€è¶³åº¦ï¼šè‡ªç¤¾ã®ç¾åœ¨ã®åƒãç’°å¢ƒã‚„æ¡ä»¶ã€å‘¨ã‚Šã®äººé–“é–¢ä¿‚ãªã©ã‚‚å«ã‚ã‚ãªãŸã¯ã©ã®ç¨‹åº¦æº€è¶³ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ': 'overall_satisfaction',
                'ç·åˆè©•ä¾¡ï¼šè‡ªåˆ†ã®è¦ªã—ã„å‹äººã‚„å®¶æ—ã«å¯¾ã—ã¦ã€ã“ã®ä¼šç¤¾ã¸ã®è»¢è·ãƒ»å°±è·ã‚’ã©ã®ç¨‹åº¦å‹§ã‚ãŸã„ã¨æ€ã„ã¾ã™ã‹ï¼Ÿ': 'recommend_score',
                'ã‚ãªãŸã¯ã“ã®ä¼šç¤¾ã§ã“ã‚Œã‹ã‚‰ã‚‚é•·ãåƒããŸã„ã¨æ€ã‚ã‚Œã¾ã™ã‹ï¼Ÿ': 'long_term_intention',
                'æ´»èºè²¢çŒ®åº¦ï¼šç¾åœ¨ã®ä¼šç¤¾ã‚„æ‰€å±çµ„ç¹”ã§ã‚ãªãŸã¯ã©ã®ç¨‹åº¦ã€æ´»èºè²¢çŒ®ã§ãã¦ã„ã‚‹ã¨æ„Ÿã˜ã¾ã™ã‹ï¼Ÿ': 'sense_of_contribution'
            }
            
            # ã‚«ãƒ©ãƒ åã‚’æ­£è¦åŒ–
            df = df.rename(columns=column_mapping)
            
            # æ•°å€¤å‹ã«å¤‰æ›
            numeric_cols = ['overall_satisfaction', 'recommend_score', 'long_term_intention', 'sense_of_contribution']
            for col in numeric_cols:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ ã‚’æ¢ã™ï¼ˆè‡ªç”±è¨˜è¿°å›ç­”ï¼‰
            text_columns = []
            for col in df.columns:
                col_str = str(col)
                # ã‚ˆã‚Šå¹…åºƒã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ ã‚’æ¤œå‡º
                if any(keyword in col_str for keyword in ['é …ç›®ã«ã¤ã„ã¦', 'æº€è¶³åº¦ãŒé«˜ã„', 'æº€è¶³åº¦ãŒä½ã„', 'å…·ä½“çš„ã«', 'æ•™ãˆã¦ã„ãŸã ã‘', 'æœŸå¾…ã—ã¦ã„ã‚‹ã“ã¨']):
                    text_columns.append(col)
                    
            # ãƒ‡ãƒãƒƒã‚°: è¦‹ã¤ã‹ã£ãŸãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ ã‚’è¡¨ç¤º
            print(f"Debug: è¦‹ã¤ã‹ã£ãŸãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ æ•°: {len(text_columns)}")
            if text_columns:
                print(f"Debug: ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ : {text_columns[:3]}...")  # æœ€åˆã®3å€‹ã‚’è¡¨ç¤º
            
            # è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ ã‚’çµ„ã¿åˆã‚ã›ã¦ã‚³ãƒ¡ãƒ³ãƒˆä½œæˆ
            if text_columns:
                # å„ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ ã®å†…å®¹ã‚’çµåˆ
                comments = []
                for idx in df.index:
                    combined_comment = []
                    for col in text_columns:
                        value = df.loc[idx, col]
                        if pd.notna(value) and str(value).strip():
                            combined_comment.append(str(value).strip())
                    
                    # çµåˆã—ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ
                    if combined_comment:
                        comments.append(' '.join(combined_comment))
                    else:
                        comments.append('ã‚³ãƒ¡ãƒ³ãƒˆãªã—')
                
                df['comment'] = comments
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ€ãƒŸãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ
                sample_comments = [
                    'ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹ã‚’æ”¹å–„ã—ã¦ã»ã—ã„',
                    'çµ¦ä¸æ°´æº–ã®å‘ä¸Šã‚’æœŸå¾…ã—ã¦ã„ã¾ã™',
                    'ã‚­ãƒ£ãƒªã‚¢é–‹ç™ºã®æ©Ÿä¼šã‚’å¢—ã‚„ã—ã¦ã»ã—ã„',
                    'æ®‹æ¥­æ™‚é–“ã‚’æ¸›ã‚‰ã—ã¦ã»ã—ã„',
                    'èŒå ´ç’°å¢ƒã®æ”¹å–„ã‚’æœŸå¾…ã—ã¦ã„ã¾ã™'
                ]
                df['comment'] = np.random.choice(sample_comments, len(df))
            
            # ä½æº€è¶³åº¦ãƒ©ãƒ™ãƒ«ã‚’ä½œæˆï¼ˆç·åˆæº€è¶³åº¦ã®ä¸‹ä½20%ï¼‰
            if 'overall_satisfaction' in df.columns:
                # ã‚ˆã‚Šæ­£ç¢ºãªä¸‹ä½20%ã®è¨ˆç®—
                satisfaction_scores = df['overall_satisfaction']
                n_samples = len(satisfaction_scores)
                n_low_satisfaction = int(n_samples * 0.2)  # æ­£ç¢ºã«20%ã®ä»¶æ•°
                
                # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸‹ä½20%ã‚’ç‰¹å®š
                sorted_indices = satisfaction_scores.argsort()
                low_satisfaction_indices = sorted_indices[:n_low_satisfaction]
                
                # is_low_satisfactionãƒ©ãƒ™ãƒ«ã‚’ä½œæˆ
                df['is_low_satisfaction'] = 0
                df.loc[low_satisfaction_indices, 'is_low_satisfaction'] = 1
                
                # çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                threshold_value = satisfaction_scores.iloc[low_satisfaction_indices].max()
                print(f"Debug: ä¸‹ä½20%é–¾å€¤={threshold_value}, å¯¾è±¡ä»¶æ•°={n_low_satisfaction}/{n_samples}")
            else:
                df['is_low_satisfaction'] = 0
            
            return df, True
        else:
            st.error("âŒ å¾“æ¥­å“¡èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            st.info("ğŸ“ data.xlsx ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã«é…ç½®ã—ã¦ãã ã•ã„")
            return create_sample_data_for_ml(200), False
            
    except Exception as e:
        st.error(f"å¾“æ¥­å“¡èª¿æŸ»ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        st.info("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ã‚„é…ç½®ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        return create_sample_data_for_ml(200), False

def show_text_analysis_ml_page():
    """ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã¨æ©Ÿæ¢°å­¦ç¿’ãƒšãƒ¼ã‚¸ã®è¡¨ç¤º"""
    st.markdown("""
    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                color: white; padding: 2rem; border-radius: 12px; margin-bottom: 2rem; text-align: center;">
        <h2 style="margin: 0; color: white;">ğŸ¤– AIãƒ†ã‚­ã‚¹ãƒˆåˆ†æ</h2>
        <p style="margin: 0.5rem 0 0 0; opacity: 0.9;">ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æã«ã‚ˆã‚‹æº€è¶³åº¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«</p>
    </div>
    """, unsafe_allow_html=True)
    
    # å¾“æ¥­å“¡èª¿æŸ»ãƒ‡ãƒ¼ã‚¿150ä»¶ã‚’èª­ã¿è¾¼ã¿
    with st.spinner("å¾“æ¥­å“¡èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        df, is_real = load_real_data_for_analysis()
        if is_real:
            st.success(f"âœ… å¾“æ¥­å“¡èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(df)}ä»¶")
            st.info("ğŸ“Š ã“ã®åˆ†æã§ã¯150ä»¶ã®å®Ÿéš›ã®å¾“æ¥­å“¡èª¿æŸ»å›ç­”ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™")
        else:
            st.error("âŒ å¾“æ¥­å“¡èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            st.warning("âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ¢ç”¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")
            df = create_sample_data_for_ml(200)
    
    # åŸºæœ¬çµ±è¨ˆ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        low_satisfaction_count = df['is_low_satisfaction'].sum()
        st.metric("ä¸‹ä½20%ï¼ˆæ”¹å–„å¯¾è±¡ï¼‰", f"{low_satisfaction_count}äºº", 
                 f"{low_satisfaction_count/len(df)*100:.1f}%")
    
    with col2:
        avg_recommend = df['recommend_score'].mean()
        st.metric("å¹³å‡æ¨å¥¨ã‚¹ã‚³ã‚¢", f"{avg_recommend:.1f}")
    
    with col3:
        avg_satisfaction = df['overall_satisfaction'].mean()
        st.metric("å¹³å‡ç·åˆæº€è¶³åº¦", f"{avg_satisfaction:.1f}")
    
    with col4:
        unique_words = len(set(' '.join(df['comment']).split()))
        st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°", f"{unique_words:,}")
    
    # ã‚¿ãƒ–ã§æ©Ÿèƒ½ã‚’åˆ†å‰²
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†æ", "ğŸ¤– æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«", "ğŸ“ˆ ç‰¹å¾´é‡é‡è¦æ€§", "ğŸ¯ äºˆæ¸¬çµæœ"])
    
    with tab1:
        st.subheader("ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒåˆ†æ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # KPIåˆ†å¸ƒ
            fig_kpi = px.histogram(
                df, x='overall_satisfaction', color='is_low_satisfaction',
                title="ç·åˆæº€è¶³åº¦ã®åˆ†å¸ƒ", 
                labels={'is_low_satisfaction': 'ä¸‹ä½20%ãƒ•ãƒ©ã‚°'},
                color_discrete_map={0: '#2E86AB', 1: '#F24236'}
            )
            st.plotly_chart(fig_kpi, use_container_width=True)
        
        with col2:
            # æ¨å¥¨ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
            fig_nps = px.histogram(
                df, x='recommend_score', color='is_low_satisfaction',
                title="æ¨å¥¨ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ",
                labels={'is_low_satisfaction': 'ä¸‹ä½20%ãƒ•ãƒ©ã‚°'},
                color_discrete_map={0: '#2E86AB', 1: '#F24236'}
            )
            st.plotly_chart(fig_nps, use_container_width=True)
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°åˆ†æ
        st.subheader("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°åˆ†æ")
        
        # ã‚³ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ã¨åˆ†æ
        if 'comment' in df.columns:
            all_comments = ' '.join(df['comment'].dropna().astype(str))
            
            # å½¢æ…‹ç´ è§£æã¨ãƒ¯ãƒ¼ãƒ‰ã‚«ã‚¦ãƒ³ãƒˆ
            tokens = japanese_tokenizer(all_comments)
            meaningful_tokens = [token for token in tokens if filter_meaningful_words(token) and len(token) > 1]
            
            if meaningful_tokens:
                from collections import Counter
                word_counts = Counter(meaningful_tokens)
                top_words = word_counts.most_common(20)
                
                if top_words:
                    # é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰TOP20ã‚’è¦‹ã‚„ã™ãè¡¨ç¤º
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        # ãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆï¼ˆç¸¦å‘ãã§å¹…ã‚’åºƒãï¼‰
                        words, counts = zip(*top_words)
                        
                        fig_words = px.bar(
                            x=list(counts),
                            y=list(words),
                            orientation='h',
                            title="ğŸ“Š é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ TOP20",
                            labels={'x': 'å‡ºç¾å›æ•°', 'y': 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'},
                            color=list(counts),
                            color_continuous_scale='Viridis',
                            height=600  # é«˜ã•ã‚’å¤§ããè¨­å®š
                        )
                        
                        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ”¹å–„
                        fig_words.update_layout(
                            title_font_size=16,
                            xaxis_title="å‡ºç¾å›æ•°",
                            yaxis_title="",
                            paper_bgcolor='white',
                            plot_bgcolor='white',
                            font=dict(size=12),
                            margin=dict(l=120, r=50, t=50, b=50),  # å·¦ãƒãƒ¼ã‚¸ãƒ³ã‚’å¤§ãã
                            yaxis=dict(
                                categoryorder='total ascending',  # å€¤é †ã§ã‚½ãƒ¼ãƒˆ
                                tickfont=dict(size=11)
                            ),
                            xaxis=dict(
                                tickfont=dict(size=11),
                                range=[0, max(counts) * 1.1]  # xè»¸ã®ç¯„å›²ã‚’èª¿æ•´
                            )
                        )
                        
                        # ãƒ‡ãƒ¼ã‚¿ãƒ©ãƒ™ãƒ«è¿½åŠ 
                        fig_words.update_traces(
                            texttemplate='%{x}',
                            textposition='outside',
                            textfont_size=10
                        )
                        
                        st.plotly_chart(fig_words, use_container_width=True)
                    
                    with col2:
                        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é »åº¦ãƒ†ãƒ¼ãƒ–ãƒ«
                        st.markdown("#### ğŸ“ˆ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾é »åº¦")
                        
                        word_df = pd.DataFrame(top_words, columns=['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'å‡ºç¾å›æ•°'])
                        word_df['é †ä½'] = range(1, len(word_df) + 1)
                        word_df = word_df[['é †ä½', 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'å‡ºç¾å›æ•°']]
                        
                        # è‰²ä»˜ãã®è¡¨ç¤º
                        st.dataframe(
                            word_df,
                            use_container_width=True,
                            hide_index=True,
                            height=400
                        )
                        
                        # çµ±è¨ˆæƒ…å ±
                        st.markdown("##### ğŸ“Š çµ±è¨ˆã‚µãƒãƒªãƒ¼")
                        total_words = len(meaningful_tokens)
                        unique_words = len(set(meaningful_tokens))
                        avg_frequency = sum(counts) / len(counts)
                        
                        st.metric("ç·å˜èªæ•°", f"{total_words:,}")
                        st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°", f"{unique_words:,}")
                        st.metric("å¹³å‡å‡ºç¾é »åº¦", f"{avg_frequency:.1f}")
                        
                        # TOP5ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ
                        st.markdown("##### ğŸ”¥ TOP5ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
                        for i, (word, count) in enumerate(top_words[:5], 1):
                            percentage = (count / total_words) * 100
                            st.write(f"{i}. **{word}** - {count}å› ({percentage:.1f}%)")
                else:
                    st.info("æ„å‘³ã®ã‚ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                st.warning("åˆ†æå¯èƒ½ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        
        # ã‚³ãƒ¡ãƒ³ãƒˆã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        st.subheader("ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆã‚µãƒ³ãƒ—ãƒ«")
        st.markdown("ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã•ã‚ŒãŸå¾“æ¥­å“¡ã‚³ãƒ¡ãƒ³ãƒˆã®ä¾‹")
        
        sample_comments = df.sample(5)[['overall_satisfaction', 'recommend_score', 'is_low_satisfaction', 'comment']]
        sample_comments = sample_comments.rename(columns={
            'overall_satisfaction': 'ç·åˆæº€è¶³åº¦',
            'recommend_score': 'æ¨å¥¨ã‚¹ã‚³ã‚¢', 
            'is_low_satisfaction': 'ä½æº€è¶³åº¦ãƒ•ãƒ©ã‚°',
            'comment': 'ã‚³ãƒ¡ãƒ³ãƒˆ'
        })
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®è¡¨ç¤ºã‚’æ”¹è‰¯
        st.dataframe(
            sample_comments,
            use_container_width=True,
            hide_index=True,
            height=250
        )
    
    with tab2:
        st.subheader("æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´")
        
        if st.button("ğŸš€ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚’å®Ÿè¡Œ", type="primary"):
            with st.spinner("ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡æŠ½å‡ºä¸­..."):
                # ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡æŠ½å‡º
                text_features, vectorizer = preprocess_text_features(df['comment'])
                
                if len(text_features.columns) > 0:
                    try:
                        # æ•°å€¤ç‰¹å¾´é‡ã¨çµåˆï¼ˆãƒ‡ãƒ¼ã‚¿å‹ã‚’æ˜ç¤ºçš„ã«æ•°å€¤ã«å¤‰æ›ï¼‰
                        numeric_cols = ['recommend_score', 'overall_satisfaction', 'long_term_intention', 'sense_of_contribution']
                        numeric_features = df[numeric_cols].copy()
                        
                        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º
                        st.write("ğŸ“Š **ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã®è©³ç´°:**")
                        
                        # æ•°å€¤å‹ã«æ˜ç¤ºçš„ã«å¤‰æ›
                        for col in numeric_features.columns:
                            original_dtype = numeric_features[col].dtype
                            original_sample = numeric_features[col].head(3).tolist()
                            
                            numeric_features[col] = pd.to_numeric(numeric_features[col], errors='coerce')
                            
                            new_dtype = numeric_features[col].dtype
                            null_count = numeric_features[col].isnull().sum()
                            
                            st.write(f"- {col}: {original_dtype} â†’ {new_dtype} (Null: {null_count})")
                            if null_count > 0:
                                st.error(f"âš ï¸ {col}ã«æ•°å€¤å¤‰æ›ã§ããªã„å€¤ãŒã‚ã‚Šã¾ã™: {original_sample}")
                        
                        # ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã‚‚æ•°å€¤å‹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                        text_original_dtype = text_features.dtypes.unique()
                        st.write(f"- ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã®å…ƒãƒ‡ãƒ¼ã‚¿å‹: {text_original_dtype}")
                        
                        # ã‚ˆã‚Šç¢ºå®Ÿãªæ•°å€¤å‹å¤‰æ›
                        try:
                            # ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã®å¼·åˆ¶å¤‰æ›
                            st.write("ğŸ”§ ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã®å¼·åˆ¶å¤‰æ›ä¸­...")
                            text_features_array = text_features.values.astype(np.float64)
                            text_features = pd.DataFrame(
                                text_features_array, 
                                columns=text_features.columns,
                                index=text_features.index
                            )
                            st.write(f"- ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡å¤‰æ›å®Œäº†: {text_features.dtypes.unique()}")
                            
                            # æ•°å€¤ç‰¹å¾´é‡ã®å¼·åˆ¶å¤‰æ›
                            st.write("ğŸ”§ æ•°å€¤ç‰¹å¾´é‡ã®å¼·åˆ¶å¤‰æ›ä¸­...")
                            numeric_features_array = numeric_features.values.astype(np.float64)
                            numeric_features = pd.DataFrame(
                                numeric_features_array,
                                columns=numeric_features.columns,
                                index=numeric_features.index
                            )
                            st.write(f"- æ•°å€¤ç‰¹å¾´é‡å¤‰æ›å®Œäº†: {numeric_features.dtypes.unique()}")
                            
                        except Exception as conv_error:
                            st.error(f"âŒ å¼·åˆ¶å¤‰æ›ã‚¨ãƒ©ãƒ¼: {conv_error}")
                            raise conv_error
                        
                        # çµåˆå‰ã«å„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•´åˆæ€§ç¢ºèª
                        st.write("ğŸ” çµåˆå‰ãƒã‚§ãƒƒã‚¯:")
                        st.write(f"- numeric_features shape: {numeric_features.shape}")
                        st.write(f"- text_features shape: {text_features.shape}")
                        st.write(f"- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è‡´: {numeric_features.index.equals(text_features.index)}")
                        
                        # çµåˆ
                        X = pd.concat([numeric_features, text_features], axis=1)
                        
                        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®å‡¦ç†
                        y_raw = df['is_low_satisfaction']
                        st.write(f"- y_raw dtype: {y_raw.dtype}, sample: {y_raw.head(3).tolist()}")
                        y = pd.to_numeric(y_raw, errors='coerce').astype(np.int64)
                        
                        # æœ€çµ‚ç¢ºèª
                        st.write(f"- **çµåˆå¾Œã®Xå½¢çŠ¶**: {X.shape}")
                        st.write(f"- **Xã®ãƒ‡ãƒ¼ã‚¿å‹åˆ†å¸ƒ**: {dict(X.dtypes.value_counts())}")
                        st.write(f"- **yã®ãƒ‡ãƒ¼ã‚¿å‹**: {y.dtype}")
                        
                        st.success(f"âœ… ç‰¹å¾´é‡æº–å‚™å®Œäº†: {X.shape[1]}å€‹ã®ç‰¹å¾´é‡")
                        
                        # ãƒ‡ãƒ¼ã‚¿ã®å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯
                        if X.isnull().any().any():
                            null_cols = X.columns[X.isnull().any()].tolist()
                            st.warning(f"ãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ãŒã‚ã‚Šã¾ã™: {null_cols}")
                            X = X.fillna(0.0)
                            st.info("æ¬ æå€¤ã‚’0.0ã§è£œå®Œã—ã¾ã—ãŸã€‚")
                        
                        if y.isnull().any():
                            st.warning("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã«æ¬ æå€¤ãŒã‚ã‚Šã¾ã™ã€‚")
                            y = y.fillna(0)
                            st.info("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®æ¬ æå€¤ã‚’0ã§è£œå®Œã—ã¾ã—ãŸã€‚")
                        
                        # æœ€çµ‚çš„ãªãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
                        st.write("ğŸ” **ãƒ¢ãƒ‡ãƒ«è¨“ç·´å‰ã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯:**")
                        st.write(f"- Xå…¨ã¦æ•°å€¤å‹?: {X.select_dtypes(include=[np.number]).shape[1] == X.shape[1]}")
                        st.write(f"- yæ•°å€¤å‹?: {pd.api.types.is_numeric_dtype(y)}")
                        
                        with st.spinner("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­..."):
                            models, scores, X_test, y_test = train_ensemble_models(X, y)
                            
                        # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
                        st.session_state['ml_models'] = models
                        st.session_state['ml_scores'] = scores
                        st.session_state['ml_feature_names'] = X.columns.tolist()
                        st.session_state['ml_vectorizer'] = vectorizer
                        
                        st.success("âœ… ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†ï¼")
                        
                        # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚µãƒãƒªãƒ¼
                        summary_df = create_prediction_summary(models, scores)
                        st.subheader("ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ")
                        st.dataframe(summary_df, use_container_width=True)
                            
                    except Exception as e:
                        st.error(f"âŒ ç‰¹å¾´é‡å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        st.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:")
                        import traceback
                        st.code(traceback.format_exc())
                        return
                        
                else:
                    st.error("ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    with tab3:
        st.subheader("ç‰¹å¾´é‡é‡è¦æ€§åˆ†æ")
        
        if 'ml_models' in st.session_state:
            models = st.session_state['ml_models']
            feature_names = st.session_state['ml_feature_names']
            
            # ç‰¹å¾´é‡é‡è¦æ€§ã‚’å¯è¦–åŒ–
            fig_importance = visualize_feature_importance(models, feature_names, top_n=15)
            st.plotly_chart(fig_importance, use_container_width=True)
            
            # ãƒˆãƒƒãƒ—ç‰¹å¾´é‡ã®è©³ç´°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            st.subheader("ğŸ” é‡è¦ãªç‰¹å¾´é‡ã®è©³ç´°åˆ†æ")
            
            # ã‚¿ãƒ–ã§ãƒ¢ãƒ‡ãƒ«åˆ¥ã«è¡¨ç¤º
            model_tabs = st.tabs([name for name in models.keys()])
            
            for tab, (model_name, model) in zip(model_tabs, models.items()):
                with tab:
                    if hasattr(model, 'feature_importances_'):
                        importances = model.feature_importances_
                        
                        # æ„å‘³ã®ã‚ã‚‹ç‰¹å¾´é‡ã®ã¿æŠ½å‡º
                        meaningful_features_data = []
                        for idx, importance in enumerate(importances):
                            feature_name = feature_names[idx]
                            
                            if feature_name.startswith('word_'):
                                word = feature_name.replace('word_', '')
                                if filter_meaningful_words(word) and importance > 0.001:
                                    meaningful_features_data.append({
                                        'ç‰¹å¾´é‡': word,
                                        'ã‚¿ã‚¤ãƒ—': "ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡",
                                        'é‡è¦æ€§': importance,
                                        'é‡è¦æ€§_è¡¨ç¤º': f"{importance:.4f}"
                                    })
                            else:
                                # æ•°å€¤ç‰¹å¾´é‡ã®æ—¥æœ¬èªåå¤‰æ›ï¼ˆå®Œå…¨ç‰ˆï¼‰
                                feature_jp_name = {
                                    'recommend_score': 'æ¨å¥¨åº¦ã‚¹ã‚³ã‚¢',
                                    'overall_satisfaction': 'ç·åˆæº€è¶³åº¦', 
                                    'long_term_intention': 'å‹¤ç¶šæ„å‘',
                                    'sense_of_contribution': 'æ´»èºè²¢çŒ®åº¦',
                                    'annual_salary': 'æ¦‚ç®—å¹´å',
                                    'avg_monthly_overtime': 'æœˆé–“å¹³å‡æ®‹æ¥­æ™‚é–“',
                                    'paid_leave_usage_rate': 'å¹´é–“æœ‰çµ¦å–å¾—ç‡',
                                    'start_year': 'å…¥ç¤¾å¹´åº¦',
                                    'employment_type': 'é›‡ç”¨å½¢æ…‹',
                                    'department': 'æ‰€å±äº‹æ¥­éƒ¨',
                                    'position': 'å½¹è·',
                                    'job_type': 'è·ç¨®',
                                    'gender': 'æ€§åˆ¥',
                                    'age_group': 'å¹´ä»£',
                                    'tenure_years': 'å‹¤ç¶šå¹´æ•°',
                                    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ç‰¹å¾´é‡
                                    'work_environment': 'è·å ´ç’°å¢ƒ',
                                    'work_life_balance': 'ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹',
                                    'growth_development': 'æˆé•·ãƒ»ç™ºé”',
                                    'compensation_benefits': 'çµ¦ä¸ãƒ»ç¦åˆ©åšç”Ÿ',
                                    'management_strategy': 'çµŒå–¶æˆ¦ç•¥',
                                    'recognition_evaluation': 'è©•ä¾¡ãƒ»èªçŸ¥',
                                    'communication_relationship': 'ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»äººé–“é–¢ä¿‚'
                                }.get(feature_name, feature_name)
                                
                                meaningful_features_data.append({
                                    'ç‰¹å¾´é‡': feature_jp_name,
                                    'ã‚¿ã‚¤ãƒ—': "ğŸ“Š æ•°å€¤ç‰¹å¾´é‡",
                                    'é‡è¦æ€§': importance,
                                    'é‡è¦æ€§_è¡¨ç¤º': f"{importance:.4f}"
                                })
                        
                        # é‡è¦æ€§ã§ã‚½ãƒ¼ãƒˆã—ã¦ãƒˆãƒƒãƒ—15ã‚’è¡¨ç¤º
                        meaningful_features_data.sort(key=lambda x: x['é‡è¦æ€§'], reverse=True)
                        top_15_features = meaningful_features_data[:15]
                        
                        if top_15_features:
                            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¿½åŠ 
                            for i, feature in enumerate(top_15_features, 1):
                                feature['ãƒ©ãƒ³ã‚¯'] = i
                            
                            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ è¡¨ç¤º
                            display_df = pd.DataFrame(top_15_features)[['ãƒ©ãƒ³ã‚¯', 'ç‰¹å¾´é‡', 'ã‚¿ã‚¤ãƒ—', 'é‡è¦æ€§_è¡¨ç¤º']]
                            display_df.columns = ['ãƒ©ãƒ³ã‚¯', 'ç‰¹å¾´é‡', 'ã‚¿ã‚¤ãƒ—', 'é‡è¦æ€§ã‚¹ã‚³ã‚¢']
                            
                            st.dataframe(display_df, use_container_width=True, hide_index=True)
                            
                            # ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                text_features = [f for f in top_15_features if f['ã‚¿ã‚¤ãƒ—'] == "ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡"]
                                st.metric("é‡è¦ãªãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡", len(text_features))
                                if text_features:
                                    top_text = text_features[0]['ç‰¹å¾´é‡']
                                    st.write(f"æœ€é‡è¦å˜èª: **{top_text}**")
                            
                            with col2:
                                numeric_features = [f for f in top_15_features if f['ã‚¿ã‚¤ãƒ—'] == "ğŸ“Š æ•°å€¤ç‰¹å¾´é‡"]
                                st.metric("é‡è¦ãªæ•°å€¤ç‰¹å¾´é‡", len(numeric_features))
                                if numeric_features:
                                    top_numeric = numeric_features[0]['ç‰¹å¾´é‡']
                                    st.write(f"æœ€é‡è¦æŒ‡æ¨™: **{top_numeric}**")
                        else:
                            st.warning("æ„å‘³ã®ã‚ã‚‹ç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        else:
            st.info("ã¾ãšã€Œæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã€ã‚¿ãƒ–ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦ãã ã•ã„ã€‚")
    
    with tab4:
        st.subheader("äºˆæ¸¬ã¨è§£é‡ˆ")
        
        if 'ml_models' in st.session_state:
            models = st.session_state['ml_models']
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ï¼ˆå¤šæ•°æ±ºï¼‰
            st.write("### ğŸ¯ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã®ä»•çµ„ã¿")
            st.info("""
            **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã®åˆ©ç‚¹:**
            - è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§äºˆæ¸¬ç²¾åº¦ãŒå‘ä¸Š
            - å€‹ã€…ã®ãƒ¢ãƒ‡ãƒ«ã®å¼±ç‚¹ã‚’è£œå®Œ
            - ã‚ˆã‚Šå …ç‰¢ã§ä¿¡é ¼æ€§ã®é«˜ã„äºˆæ¸¬ãŒå¯èƒ½
            
            **ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:**
            - Decision Tree: è§£é‡ˆã—ã‚„ã™ã„ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
            - Random Forest: è¤‡æ•°ã®æ±ºå®šæœ¨ã«ã‚ˆã‚‹äºˆæ¸¬ã®å¹³å‡åŒ–
            - Gradient Boosting: æ®µéšçš„ã«äºˆæ¸¬ã‚’æ”¹å–„ã™ã‚‹æ‰‹æ³•
            """)
            
            # ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬
            sample_idx = st.slider("äºˆæ¸¬å¯¾è±¡ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ", 0, len(df)-1, 0)
            sample_data = df.iloc[sample_idx]
            
            st.write(f"**é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ« #{sample_idx}**")
            st.write(f"- æ¨å¥¨ã‚¹ã‚³ã‚¢: {sample_data['recommend_score']}")
            st.write(f"- ç·åˆæº€è¶³åº¦: {sample_data['overall_satisfaction']}")
            st.write(f"- å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«: {'ä¸‹ä½20%' if sample_data['is_low_satisfaction'] else 'ä¸Šä½80%'}")
            st.write(f"- ã‚³ãƒ¡ãƒ³ãƒˆ: {sample_data['comment'][:100]}...")
            
            # å®Ÿéš›ã®äºˆæ¸¬ã‚’å®Ÿè¡Œ
            st.write("### ğŸ“Š äºˆæ¸¬çµæœ")
            
            # é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã‚’æŠ½å‡º
            if sample_data['comment'] and len(sample_data['comment'].strip()) > 0:
                try:
                    # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
                    processed_text = preprocess_japanese_text(sample_data['comment'])
                    
                    # ç‰¹å¾´é‡æŠ½å‡ºï¼ˆè¨“ç·´æ™‚ã¨åŒã˜ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ï¼‰
                    if 'vectorizer' in st.session_state and st.session_state.vectorizer is not None:
                        text_features = st.session_state.vectorizer.transform([processed_text])
                        
                        # äºˆæ¸¬å®Ÿè¡Œ
                        if 'ml_models' in st.session_state and st.session_state['ml_models']:
                            predictions = {}
                            probabilities = {}
                            
                            for name, model in st.session_state['ml_models'].items():
                                try:
                                    pred = model.predict(text_features)[0]
                                    prob = model.predict_proba(text_features)[0]
                                    predictions[name] = pred
                                    probabilities[name] = prob
                                except Exception as model_error:
                                    st.warning(f"{name}ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {model_error}")
                                    continue
                            
                            if predictions:
                                # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ï¼ˆå¤šæ•°æ±ºï¼‰
                                from collections import Counter
                                vote_counts = Counter(predictions.values())
                                ensemble_pred = vote_counts.most_common(1)[0][0]
                                
                                # äºˆæ¸¬çµæœã®è¡¨ç¤º
                                col1, col2 = st.columns(2)
                                
                                with col1:
                                    st.write("**ğŸ¯ äºˆæ¸¬çµæœ**")
                                    actual_label = 'ä½æº€è¶³åº¦ç¾¤' if sample_data['is_low_satisfaction'] else 'é«˜æº€è¶³åº¦ç¾¤'
                                    pred_label = 'ä½æº€è¶³åº¦ç¾¤' if ensemble_pred else 'é«˜æº€è¶³åº¦ç¾¤'
                                    
                                    # äºˆæ¸¬ç²¾åº¦ã®è¡¨ç¤º
                                    is_correct = (ensemble_pred == sample_data['is_low_satisfaction'])
                                    accuracy_icon = "âœ…" if is_correct else "âŒ"
                                    
                                    st.write(f"- å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«: **{actual_label}**")
                                    st.write(f"- äºˆæ¸¬ãƒ©ãƒ™ãƒ«: **{pred_label}** {accuracy_icon}")
                                    st.write(f"- äºˆæ¸¬ç²¾åº¦: **{'æ­£è§£' if is_correct else 'ä¸æ­£è§£'}**")
                                
                                with col2:
                                    st.write("**ğŸ“ˆ å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡**")
                                    for name, prob in probabilities.items():
                                        if len(prob) >= 2:
                                            low_prob = prob[1] if len(prob) > 1 else 0  # ä½æº€è¶³åº¦ã®ç¢ºç‡
                                            high_prob = prob[0] if len(prob) > 0 else 0  # é«˜æº€è¶³åº¦ã®ç¢ºç‡
                                            st.write(f"- {name}: ä½æº€è¶³åº¦ {low_prob:.2f}, é«˜æº€è¶³åº¦ {high_prob:.2f}")
                                
                                # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
                                st.write("**ğŸ” äºˆæ¸¬ã®ä¿¡é ¼åº¦**")
                                confidence_scores = []
                                for prob in probabilities.values():
                                    if len(prob) >= 2:
                                        confidence = max(prob) - min(prob)  # ç¢ºç‡ã®å·®ãŒå¤§ãã„ã»ã©ä¿¡é ¼åº¦ãŒé«˜ã„
                                        confidence_scores.append(confidence)
                                
                                if confidence_scores:
                                    avg_confidence = np.mean(confidence_scores)
                                    confidence_level = "é«˜" if avg_confidence > 0.6 else "ä¸­" if avg_confidence > 0.3 else "ä½"
                                    st.write(f"å¹³å‡ä¿¡é ¼åº¦: **{avg_confidence:.3f}** ({confidence_level})")
                                    
                                    # ä¿¡é ¼åº¦ãƒãƒ¼
                                    st.progress(min(avg_confidence, 1.0))
                                
                                # é‡è¦ãªç‰¹å¾´èªã®è¡¨ç¤º
                                if 'feature_importance' in st.session_state and st.session_state.feature_importance is not None:
                                    st.write("**ğŸ“ äºˆæ¸¬ã«å½±éŸ¿ã—ãŸé‡è¦èªå¥**")
                                    
                                    # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦èªå¥ã‚’æŠ½å‡º
                                    words_in_text = processed_text.split()
                                    important_words = []
                                    
                                    feature_names = st.session_state.vectorizer.get_feature_names_out()
                                    importance_dict = dict(zip(feature_names, st.session_state.feature_importance))
                                    
                                    for word in words_in_text:
                                        if word in importance_dict and importance_dict[word] > 0.01:
                                            important_words.append((word, importance_dict[word]))
                                    
                                    # é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ
                                    important_words.sort(key=lambda x: x[1], reverse=True)
                                    
                                    if important_words:
                                        for word, importance in important_words[:10]:
                                            st.write(f"- **{word}**: {importance:.3f}")
                                    else:
                                        st.info("ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã«ã¯ç‰¹ã«é‡è¦ãªç‰¹å¾´èªå¥ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                            
                            else:
                                st.error("ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                        else:
                            st.warning("ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã€Œæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã€ã‚¿ãƒ–ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦ãã ã•ã„ã€‚")
                    else:
                        st.warning("ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã€Œæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã€ã‚¿ãƒ–ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦ãã ã•ã„ã€‚")
                        
                except Exception as pred_error:
                    st.error(f"äºˆæ¸¬å‡¦ç†ã‚¨ãƒ©ãƒ¼: {pred_error}")
                    st.info("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸäºˆæ¸¬ä¾‹ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
                    
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬çµæœ
                    st.write("**ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬çµæœ**")
                    import random
                    random.seed(sample_idx)  # å†ç¾å¯èƒ½æ€§ã®ãŸã‚
                    sample_pred = random.choice([True, False])
                    sample_confidence = random.uniform(0.6, 0.9)
                    
                    actual_label = 'ä½æº€è¶³åº¦ç¾¤' if sample_data['is_low_satisfaction'] else 'é«˜æº€è¶³åº¦ç¾¤'
                    pred_label = 'ä½æº€è¶³åº¦ç¾¤' if sample_pred else 'é«˜æº€è¶³åº¦ç¾¤'
                    is_correct = (sample_pred == sample_data['is_low_satisfaction'])
                    accuracy_icon = "âœ…" if is_correct else "âŒ"
                    
                    st.write(f"- å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«: **{actual_label}**")
                    st.write(f"- äºˆæ¸¬ãƒ©ãƒ™ãƒ«: **{pred_label}** {accuracy_icon}")
                    st.write(f"- ä¿¡é ¼åº¦: **{sample_confidence:.3f}**")
            else:
                st.warning("é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã«ã¯ã‚³ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            
        else:
            st.info("ã¾ãšã€Œæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã€ã‚¿ãƒ–ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    show_text_analysis_ml_page()