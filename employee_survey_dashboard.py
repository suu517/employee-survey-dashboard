#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾“æ¥­å“¡èª¿æŸ»å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ - å®Ÿãƒ‡ãƒ¼ã‚¿å¯¾å¿œç‰ˆ
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
from datetime import datetime
import os
import re
from collections import Counter
from janome.tokenizer import Tokenizer
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import r2_score, mean_squared_error
try:
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    from statsmodels.stats.diagnostic import durbin_watson, het_breuschpagan
    import statsmodels.api as sm
    STATSMODELS_AVAILABLE = True
except ImportError:
    STATSMODELS_AVAILABLE = False
from scipy import stats
from scipy.stats import probplot
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import locale
import warnings
warnings.filterwarnings('ignore')

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="å¾“æ¥­å“¡èª¿æŸ»å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
    page_icon="ğŸ‘¥",
    layout="wide",
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    html, body, [class*="css"] {
        font-family: 'Helvetica', 'Arial', 'Hiragino Sans', 'Yu Gothic', sans-serif;
        background-color: #f8fafc;
    }
    
    h1, h2, h3 {
        color: #1E293B;
    }
    
    /* æ”¹è‰¯ã•ã‚ŒãŸKPIã‚«ãƒ¼ãƒ‰ã‚¹ã‚¿ã‚¤ãƒ« */
    .kpi-card {
        background: linear-gradient(135deg, #ffffff 0%, #f8fafc 100%);
        border-radius: 16px;
        padding: 24px 20px;
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
        border: 1px solid rgba(255, 255, 255, 0.2);
        margin-bottom: 16px;
        min-height: 140px;
        display: flex;
        flex-direction: column;
        justify-content: space-between;
        position: relative;
        overflow: hidden;
        transition: all 0.3s ease;
    }
    
    .kpi-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 25px rgba(0, 0, 0, 0.12);
    }
    
    .kpi-card::before {
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        height: 4px;
        background: linear-gradient(90deg, #667eea, #764ba2);
    }
    
    .kpi-title {
        font-size: 13px;
        color: #64748b;
        margin-bottom: 12px;
        font-weight: 600;
        text-transform: uppercase;
        letter-spacing: 0.5px;
        line-height: 1.2;
    }
    
    .kpi-value {
        font-size: 32px;
        font-weight: 800;
        color: #1e293b;
        margin-bottom: 8px;
        line-height: 1.1;
        display: flex;
        align-items: baseline;
    }
    
    .kpi-unit {
        font-size: 18px;
        font-weight: 500;
        color: #64748b;
        margin-left: 4px;
    }
    
    .kpi-change {
        font-size: 13px;
        display: flex;
        align-items: center;
        color: #64748b;
        font-weight: 500;
    }
    
    .kpi-icon {
        margin-right: 8px;
        font-size: 20px;
        filter: drop-shadow(0 2px 4px rgba(0,0,0,0.1));
    }
    
    /* ã‚«ãƒ©ãƒ¼ä»˜ãKPIã‚«ãƒ¼ãƒ‰ */
    .kpi-card-green::before {
        background: linear-gradient(90deg, #22c55e, #16a34a);
    }
    .kpi-card-orange::before {
        background: linear-gradient(90deg, #f59e0b, #d97706);
    }
    .kpi-card-red::before {
        background: linear-gradient(90deg, #ef4444, #dc2626);
    }
    .kpi-card-blue::before {
        background: linear-gradient(90deg, #3b82f6, #2563eb);
    }
    
    /* ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼æ”¹å–„ */
    .section-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 20px 28px;
        border-radius: 16px;
        margin-bottom: 32px;
        box-shadow: 0 8px 32px rgba(102, 126, 234, 0.25);
        border: 1px solid rgba(255, 255, 255, 0.1);
    }
    
    .section-header h2 {
        color: white !important;
        margin: 0;
        font-size: 24px;
        font-weight: 700;
    }
    
    /* ãƒ‡ãƒ¼ã‚¿çŠ¶æ³è¡¨ç¤ºã®æ”¹å–„ */
    .data-status {
        background: linear-gradient(135deg, #e0f2fe 0%, #f0f9ff 100%);
        border: 1px solid #0ea5e9;
        border-radius: 12px;
        padding: 16px 20px;
        margin-bottom: 24px;
        box-shadow: 0 2px 8px rgba(14, 165, 233, 0.1);
    }
    
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ˜ãƒƒãƒ€ãƒ¼ */
    .sidebar-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 18px;
        border-radius: 12px;
        display: flex;
        align-items: center;
        margin-bottom: 24px;
        box-shadow: 0 4px 16px rgba(102, 126, 234, 0.2);
    }
    
    .sidebar-logo {
        background-color: rgba(255, 255, 255, 0.95);
        width: 48px;
        height: 48px;
        border-radius: 10px;
        display: flex;
        justify-content: center;
        align-items: center;
        margin-right: 14px;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }
    
    /* ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œ */
    @media (max-width: 768px) {
        .kpi-card {
            min-height: 120px;
            padding: 20px 16px;
        }
        .kpi-value {
            font-size: 28px;
        }
        .kpi-title {
            font-size: 12px;
        }
    }
</style>
""", unsafe_allow_html=True)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°
@st.cache_data
def load_employee_data():
    """å¾“æ¥­å“¡èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆãƒ‡ãƒ¢ç”¨ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼‰"""
    st.info("ğŸ“Š ãƒ‡ãƒ¢ç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™")
    return create_dummy_data()

def load_comment_data():
    """ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ãƒ»å‡¦ç†ã™ã‚‹"""
    try:
        excel_path = './data.xlsx'
        if not os.path.exists(excel_path):
            return None
            
        # Responsesã‚·ãƒ¼ãƒˆã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        df_raw = pd.read_excel(excel_path, sheet_name='Responses', header=None)
        
        if len(df_raw) < 3:
            return None
            
        # ã‚³ãƒ¡ãƒ³ãƒˆã‚«ãƒ©ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0ãƒ™ãƒ¼ã‚¹ï¼‰
        comment_columns = {
            'æœŸå¾…ã‚³ãƒ¡ãƒ³ãƒˆ': 60,   # æœ€ã‚‚æœŸå¾…ãŒé«˜ã„é …ç›®ã«ã¤ã„ã¦
            'æº€è¶³ã‚³ãƒ¡ãƒ³ãƒˆ': 103,  # æœ€ã‚‚æº€è¶³åº¦ãŒé«˜ã„é …ç›®ã«ã¤ã„ã¦  
            'ä¸æº€ã‚³ãƒ¡ãƒ³ãƒˆ': 104   # æº€è¶³åº¦ãŒä½ã„é …ç›®ã«ã¤ã„ã¦
        }
        
        comments = {}
        for comment_type, col_idx in comment_columns.items():
            if col_idx < len(df_raw.columns):
                # ãƒ‡ãƒ¼ã‚¿è¡Œï¼ˆ2è¡Œç›®ä»¥é™ï¼‰ã‹ã‚‰ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
                comment_data = []
                for row_idx in range(2, len(df_raw)):
                    comment = df_raw.iloc[row_idx, col_idx]
                    if pd.notna(comment) and str(comment).strip():
                        comment_data.append(str(comment).strip())
                
                comments[comment_type] = comment_data
        
        return comments
        
    except Exception as e:
        st.error(f"ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def preprocess_japanese_text(text):
    """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
    if not text or pd.isna(text):
        return ""
    
    # æ–‡å­—åˆ—ã«å¤‰æ›
    text = str(text)
    
    # ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
    text = re.sub(r'[^\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\u3400-\u4DBF\uFF00-\uFFEFa-zA-Z0-9\s]', '', text)
    
    # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def extract_keywords_janome(texts, min_length=2, max_features=100):
    """Janomeã‚’ä½¿ã£ã¦æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    if not texts:
        return []
    
    # Janomeãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    tokenizer = Tokenizer()
    
    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆé™¤å¤–ã™ã‚‹èªï¼‰
    stop_words = {
        'ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ã‚ˆã†', 'ãªã©', 'ã«ã¤ã„ã¦', 'ã«ãŠã„ã¦', 'ã¨ã—ã¦', 
        'ã«ã‚ˆã‚‹', 'ã«ã‚ˆã‚Š', 'ã«å¯¾ã—ã¦', 'ãã‚Œ', 'ã“ã‚Œ', 'ãã®', 'ã“ã®', 'ã‚ã‚‹',
        'ã„ã‚‹', 'ã™ã‚‹', 'ãªã‚‹', 'ã‚Œã‚‹', 'ã‚‰ã‚Œã‚‹', 'ã§ã™', 'ã§ã‚ã‚‹', 'ã ', 'ã§',
        'ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã®', 'ã¨', 'ã‚„', 'ã‹', 'ã‚‚', 'ã‹ã‚‰', 'ã¾ã§',
        '1', '2', '3', '4', '5', 'ã©ã¡ã‚‰', 'è¨€ãˆã‚‹', 'æº€è¶³', 'æœŸå¾…', 'é …ç›®'
    }
    
    all_keywords = []
    
    for text in texts:
        if not text:
            continue
            
        # å‰å‡¦ç†
        cleaned_text = preprocess_japanese_text(text)
        
        # å½¢æ…‹ç´ è§£æ
        tokens = tokenizer.tokenize(cleaned_text)
        
        for token in tokens:
            # åè©ã®ã¿æŠ½å‡º
            if token.part_of_speech.split(',')[0] in ['åè©']:
                word = token.surface
                
                # æ¡ä»¶ã«åˆã†ã‚‚ã®ã ã‘æŠ½å‡º
                if (len(word) >= min_length and 
                    word not in stop_words and
                    not word.isdigit() and
                    not re.match(r'^[ã-ã‚“]+$', word)):  # ã²ã‚‰ãŒãªã®ã¿ã®èªã‚’é™¤å¤–
                    all_keywords.append(word)
    
    # å‡ºç¾é »åº¦ã§ã‚½ãƒ¼ãƒˆ
    keyword_counts = Counter(all_keywords)
    return keyword_counts.most_common(max_features)

def build_cooccurrence_network(texts, min_cooccurrence=1):
    """å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰"""
    if not texts:
        return None, None
    
    tokenizer = Tokenizer()
    
    # å„ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åè©ã‚’æŠ½å‡º
    doc_keywords = []
    for text in texts:
        if not text:
            continue
            
        cleaned_text = preprocess_japanese_text(text)
        tokens = tokenizer.tokenize(cleaned_text)
        
        keywords = []
        for token in tokens:
            if token.part_of_speech.split(',')[0] in ['åè©']:
                word = token.surface
                if (len(word) >= 2 and 
                    not word.isdigit() and
                    word not in {'ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ã‚ˆã†', 'é …ç›®', 'æº€è¶³', 'æœŸå¾…'}):
                    keywords.append(word)
        
        doc_keywords.append(keywords)
    
    # å…±èµ·é–¢ä¿‚ã‚’è¨ˆç®—
    cooccurrence = {}
    for keywords in doc_keywords:
        for i, word1 in enumerate(keywords):
            for j, word2 in enumerate(keywords):
                if i != j:
                    pair = tuple(sorted([word1, word2]))
                    cooccurrence[pair] = cooccurrence.get(pair, 0) + 1
    
    # æœ€å°å…±èµ·å›æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_cooccurrence = {pair: count for pair, count in cooccurrence.items() 
                           if count >= min_cooccurrence}
    
    if not filtered_cooccurrence:
        return None, None
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
    G = nx.Graph()
    
    for (word1, word2), weight in filtered_cooccurrence.items():
        G.add_edge(word1, word2, weight=weight)
    
    return G, filtered_cooccurrence

def load_timestamp_data():
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ãƒ»å‡¦ç†ã™ã‚‹"""
    try:
        excel_path = './data.xlsx'
        if not os.path.exists(excel_path):
            return None
            
        # Responsesã‚·ãƒ¼ãƒˆã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        df_raw = pd.read_excel(excel_path, sheet_name='Responses', header=None)
        
        if len(df_raw) < 2:
            return None
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚«ãƒ©ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0ãƒ™ãƒ¼ã‚¹ï¼‰
        start_time_col = 2  # å›ç­”é–‹å§‹ï¼ˆåˆ—3ï¼‰
        end_time_col = 3    # å›ç­”å®Œäº†ï¼ˆåˆ—4ï¼‰
        
        timestamp_data = []
        
        # ãƒ‡ãƒ¼ã‚¿è¡Œï¼ˆ2è¡Œç›®ä»¥é™ï¼‰ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
        for row_idx in range(1, len(df_raw)):
            try:
                start_time_raw = df_raw.iloc[row_idx, start_time_col]
                end_time_raw = df_raw.iloc[row_idx, end_time_col]
                
                if pd.notna(start_time_raw) and pd.notna(end_time_raw):
                    # æ—¥æœ¬èªæ—¥æ™‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ãƒ‘ãƒ¼ã‚¹
                    start_time = parse_japanese_datetime(str(start_time_raw))
                    end_time = parse_japanese_datetime(str(end_time_raw))
                    
                    if start_time and end_time:
                        duration_minutes = (end_time - start_time).total_seconds() / 60
                        timestamp_data.append({
                            'response_id': row_idx,
                            'start_time': start_time,
                            'end_time': end_time,
                            'duration_minutes': duration_minutes,
                            'date': start_time.date(),
                            'hour': start_time.hour,
                            'weekday': start_time.weekday(),
                            'weekday_name': ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥'][start_time.weekday()]
                        })
            except Exception as e:
                continue
        
        return pd.DataFrame(timestamp_data) if timestamp_data else None
        
    except Exception as e:
        st.error(f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def parse_japanese_datetime(datetime_str):
    """æ—¥æœ¬èªã®æ—¥æ™‚æ–‡å­—åˆ—ã‚’è§£æã—ã¦datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
    try:
        # "6æœˆ 08, 2025 08:39:24 åˆå¾Œ" ã®ã‚ˆã†ãªå½¢å¼ã‚’å‡¦ç†
        import re
        
        # æœˆåã®æ—¥æœ¬èªã‚’è‹±èªã«å¤‰æ›
        month_map = {
            '1æœˆ': 'Jan', '2æœˆ': 'Feb', '3æœˆ': 'Mar', '4æœˆ': 'Apr',
            '5æœˆ': 'May', '6æœˆ': 'Jun', '7æœˆ': 'Jul', '8æœˆ': 'Aug', 
            '9æœˆ': 'Sep', '10æœˆ': 'Oct', '11æœˆ': 'Nov', '12æœˆ': 'Dec'
        }
        
        datetime_str = str(datetime_str).strip()
        
        # åˆå‰/åˆå¾Œã®å‡¦ç†
        is_pm = 'åˆå¾Œ' in datetime_str
        datetime_str = datetime_str.replace('åˆå‰', '').replace('åˆå¾Œ', '').strip()
        
        # æ—¥æœ¬èªã®æœˆã‚’è‹±èªã«å¤‰æ›
        for jp_month, en_month in month_map.items():
            if jp_month in datetime_str:
                datetime_str = datetime_str.replace(jp_month, en_month)
                break
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§æ—¥æ™‚è¦ç´ ã‚’æŠ½å‡º
        pattern = r'(\w+)\s+(\d+),\s+(\d+)\s+(\d+):(\d+):(\d+)'
        match = re.search(pattern, datetime_str)
        
        if match:
            month_str, day, year, hour, minute, second = match.groups()
            
            # æœˆåã‚’æ•°å€¤ã«å¤‰æ›
            month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
            month = month_names.index(month_str) + 1
            
            # åˆå¾Œã®å ´åˆã¯12æ™‚é–“ã‚’è¿½åŠ ï¼ˆãŸã ã—12æ™‚ã®å ´åˆã¯é™¤ãï¼‰
            hour_int = int(hour)
            if is_pm and hour_int != 12:
                hour_int += 12
            elif not is_pm and hour_int == 12:
                hour_int = 0
            
            return datetime(int(year), month, int(day), hour_int, int(minute), int(second))
        
        return None
        
    except Exception as e:
        return None

def process_real_survey_data(df):
    """å®Ÿéš›ã®èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹"""
    processed_data = []
    
    # å„å›ç­”è€…ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
    for idx, row in df.iterrows():
        try:
            # åŸºæœ¬æƒ…å ±ã®æŠ½å‡º
            employee_data = {
                'response_id': idx + 1,
                'department': extract_value(row, 'æ‰€å±äº‹æ¥­éƒ¨', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°éƒ¨'),
                'position': extract_value(row, 'å½¹è·', 'å½¹è·ãªã—'),
                'employment_type': extract_value(row, 'é›‡ç”¨å½¢æ…‹', 'æ­£ç¤¾å“¡'),
                'job_category': extract_value(row, 'è·ç¨®', 'ãã®ä»–'),
                'start_year': extract_numeric_value(row, 'å…¥ç¤¾å¹´åº¦ã‚’æ•™ãˆã¦ãã ã•ã„', 2019),
                'annual_salary': extract_numeric_value(row, 'æ¦‚ç®—å¹´åã‚’æ•™ãˆã¦ãã ã•ã„', 500),
                'monthly_overtime': extract_numeric_value(row, '1ãƒ¶æœˆå½“ãŸã‚Šã®å¹³å‡æ®‹æ¥­æ™‚é–“', 20),
                'paid_leave_rate': extract_numeric_value(row, '1å¹´é–“å½“ãŸã‚Šã®å¹³å‡æœ‰çµ¦ä¼‘æš‡å–å¾—ç‡', 50),
            }
            
            # ä¸»è¦è©•ä¾¡æŒ‡æ¨™ã®æŠ½å‡º
            employee_data.update({
                'nps_score': extract_nps_score(row, 'ç·åˆè©•ä¾¡ï¼šè‡ªåˆ†ã®è¦ªã—ã„å‹äººã‚„å®¶æ—ã«å¯¾ã—ã¦ã€ã“ã®ä¼šç¤¾ã¸ã®è»¢è·ãƒ»å°±è·ã‚’ã©ã®ç¨‹åº¦å‹§ã‚ãŸã„ã¨æ€ã„ã¾ã™ã‹ï¼Ÿ'),
                'overall_satisfaction': extract_satisfaction_score(row, 'ç·åˆæº€è¶³åº¦ï¼šè‡ªç¤¾ã®ç¾åœ¨ã®åƒãç’°å¢ƒã‚„æ¡ä»¶ã€å‘¨ã‚Šã®äººé–“é–¢ä¿‚ãªã©ã‚‚å«ã‚ã‚ãªãŸã¯ã©ã®ç¨‹åº¦æº€è¶³ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ'),
                'long_term_intention': extract_satisfaction_score(row, 'ã‚ãªãŸã¯ã“ã®ä¼šç¤¾ã§ã“ã‚Œã‹ã‚‰ã‚‚é•·ãåƒããŸã„ã¨æ€ã‚ã‚Œã¾ã™ã‹ï¼Ÿ'),
                'contribution_score': extract_satisfaction_score(row, 'æ´»èºè²¢çŒ®åº¦ï¼šç¾åœ¨ã®ä¼šç¤¾ã‚„æ‰€å±çµ„ç¹”ã§ã‚ãªãŸã¯ã©ã®ç¨‹åº¦ã€æ´»èºè²¢çŒ®ã§ãã¦ã„ã‚‹ã¨æ„Ÿã˜ã¾ã™ã‹ï¼Ÿ'),
            })
            
            # æœŸå¾…åº¦é …ç›®ã®æŠ½å‡º
            expectation_items = [
                ('å‹¤å‹™æ™‚é–“', 'è‡ªåˆ†ã«åˆã£ãŸå‹¤å‹™æ™‚é–“ã§åƒã‘ã‚‹è·å ´'),
                ('ä¼‘æ—¥ä¼‘æš‡', 'ä¼‘æ—¥ä¼‘æš‡ãŒã¡ã‚ƒã‚“ã¨å–ã‚Œã‚‹è·å ´'),
                ('æœ‰çµ¦ä¼‘æš‡', 'æœ‰çµ¦ä¼‘æš‡ãŒã¡ã‚ƒã‚“ã¨å–ã‚Œã‚‹è·å ´'),
                ('å‹¤å‹™ä½“ç³»', 'æŸ”è»Ÿãªå‹¤å‹™ä½“ç³»ï¼ˆãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã€æ™‚çŸ­å‹¤å‹™ã€ãƒ•ãƒ¬ãƒƒã‚¯ã‚¹åˆ¶ãªã©ï¼‰ã®ã‚‚ã¨ã§åƒã‘ã‚‹è·å ´'),
                ('æ˜‡çµ¦æ˜‡æ ¼', 'æˆæœã«å¿œã˜ã¦æ—©æœŸã®æ˜‡çµ¦ãƒ»æ˜‡æ ¼ãŒæœ›ã‚ã‚‹è·å ´'),
                ('äººé–“é–¢ä¿‚', 'äººé–“é–¢ä¿‚ãŒè‰¯å¥½ãªè·å ´'),
                ('åƒãç’°å¢ƒ', 'åƒãã‚„ã™ã„ä»•äº‹ç’°å¢ƒã‚„ã‚ªãƒ•ã‚£ã‚¹ç’°å¢ƒãŒã‚ã‚‹ä¼šç¤¾'),
                ('æˆé•·å®Ÿæ„Ÿ', 'å°‚é–€çš„ãªã‚¹ã‚­ãƒ«ã‚„æŠ€è¡“ãƒ»çŸ¥è­˜ã‚„çµŒé¨“ã‚’ç²å¾—ã§ãã‚‹è·å ´'),
                ('å°†æ¥ã‚­ãƒ£ãƒªã‚¢', 'è‡ªåˆ†ã«åˆã£ãŸå°†æ¥ã®ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ã‚’ã—ã£ã‹ã‚Šè¨­è¨ˆã—ã¦ãã‚Œã‚‹è·å ´'),
                ('ç¦åˆ©åšç”Ÿ', 'å……å®Ÿã—ãŸç¦åˆ©åšç”ŸãŒã‚ã‚‹è·å ´'),
                ('è©•ä¾¡åˆ¶åº¦', 'è‡ªèº«ã®è¡Œã£ãŸä»•äº‹ãŒæ­£å½“ã«è©•ä¾¡ã•ã‚Œã‚‹è·å ´'),
            ]
            
            for category, keyword in expectation_items:
                expectation_score = extract_expectation_score(row, keyword)
                employee_data[f'{category}_æœŸå¾…åº¦'] = expectation_score
            
            # æº€è¶³åº¦é …ç›®ã®æŠ½å‡º
            satisfaction_items = [
                ('å‹¤å‹™æ™‚é–“', 'è‡ªåˆ†ã«åˆã£ãŸå‹¤å‹™æ™‚é–“ã§åƒã‘ã‚‹'),
                ('ä¼‘æ—¥ä¼‘æš‡', 'ä¼‘æ—¥ä¼‘æš‡ãŒã¡ã‚ƒã‚“ã¨å–ã‚Œã‚‹'),
                ('æœ‰çµ¦ä¼‘æš‡', 'æœ‰çµ¦ä¼‘æš‡ãŒã¡ã‚ƒã‚“ã¨å–ã‚Œã‚‹'),
                ('å‹¤å‹™ä½“ç³»', 'æŸ”è»Ÿãªå‹¤å‹™ä½“ç³»ï¼ˆãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã€æ™‚çŸ­å‹¤å‹™ã€ãƒ•ãƒ¬ãƒƒã‚¯ã‚¹åˆ¶ãªã©ï¼‰ã®ã‚‚ã¨ã§åƒã‘ã‚‹'),
                ('æ˜‡çµ¦æ˜‡æ ¼', 'æˆæœã«å¿œã˜ã¦æ—©æœŸã®æ˜‡çµ¦ãƒ»æ˜‡æ ¼ãŒæœ›ã‚ã‚‹ä½“åˆ¶ã«ã¤ã„ã¦'),
                ('äººé–“é–¢ä¿‚', 'äººé–“é–¢ä¿‚ãŒè‰¯å¥½ãªç’°å¢ƒã«ã¤ã„ã¦'),
                ('åƒãç’°å¢ƒ', 'åƒãã‚„ã™ã„ä»•äº‹ç’°å¢ƒã‚„ã‚ªãƒ•ã‚£ã‚¹ç’°å¢ƒãŒã‚ã‚‹ä¼šç¤¾'),
                ('æˆé•·å®Ÿæ„Ÿ', 'å°‚é–€çš„ãªã‚¹ã‚­ãƒ«ã‚„æŠ€è¡“ãƒ»çŸ¥è­˜ã‚„çµŒé¨“ã®ç²å¾—ã«ã¤ã„ã¦'),
                ('å°†æ¥ã‚­ãƒ£ãƒªã‚¢', 'è‡ªåˆ†ã«åˆã£ãŸå°†æ¥ã®ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹è¨­è¨ˆã«ã¤ã„ã¦'),
                ('ç¦åˆ©åšç”Ÿ', 'å……å®Ÿã—ãŸç¦åˆ©åšç”Ÿã«ã¤ã„ã¦'),
                ('è©•ä¾¡åˆ¶åº¦', 'è‡ªèº«ã®è¡Œã£ãŸä»•äº‹ãŒæ­£å½“ã«è©•ä¾¡ã•ã‚Œã‚‹ä½“åˆ¶ã«ã¤ã„ã¦'),
            ]
            
            for category, keyword in satisfaction_items:
                satisfaction_score = extract_satisfaction_score_detailed(row, keyword)
                employee_data[f'{category}_æº€è¶³åº¦'] = satisfaction_score
            
            processed_data.append(employee_data)
            
        except Exception as e:
            st.warning(f"è¡Œ {idx + 1} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    if processed_data:
        return {'employee_data': pd.DataFrame(processed_data)}
    else:
        return create_dummy_data()

def extract_value(row, keyword, default=''):
    """è¡Œã‹ã‚‰ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€åˆ—ã®å€¤ã‚’æŠ½å‡º"""
    for col in row.index:
        if str(col).find(keyword) != -1:
            value = row[col]
            return str(value) if pd.notna(value) else default
    return default

def extract_numeric_value(row, keyword, default=0):
    """è¡Œã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º"""
    for col in row.index:
        if str(col).find(keyword) != -1:
            value = row[col]
            if pd.notna(value):
                try:
                    return float(str(value).replace(',', ''))
                except:
                    return default
    return default

def extract_nps_score(row, keyword):
    """NPS ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡ºï¼ˆ0-10ã®å€¤ï¼‰"""
    for col in row.index:
        if str(col).find(keyword) != -1:
            value = str(row[col])
            if 'Promoter' in value:
                return 9
            elif 'Passive' in value:
                return 7
            elif 'Detractor' in value:
                return 4
            else:
                try:
                    return int(float(value))
                except:
                    return 5
    return 5

def extract_satisfaction_score(row, keyword):
    """æº€è¶³åº¦ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡ºï¼ˆ1-5ã®å€¤ï¼‰"""
    for col in row.index:
        if str(col).find(keyword) != -1:
            value = str(row[col])
            if 'Promoter' in value:
                return 5
            elif 'Passive' in value:
                return 3
            elif 'Detractor' in value:
                return 2
            else:
                try:
                    return int(float(value))
                except:
                    return 3
    return 3

def extract_expectation_score(row, keyword):
    """æœŸå¾…åº¦ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º"""
    for col in row.index:
        if str(col).find(keyword) != -1:
            value = row[col]
            if pd.notna(value):
                try:
                    return int(float(value))
                except:
                    return 3
    return 3

def extract_satisfaction_score_detailed(row, keyword):
    """è©³ç´°æº€è¶³åº¦ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º"""
    for col in row.index:
        if str(col).find(keyword) != -1:
            value = row[col]
            if pd.notna(value):
                try:
                    return int(float(value))
                except:
                    return 3
    return 3

def create_dummy_data():
    """ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ãªã„å ´åˆï¼‰"""
    np.random.seed(42)
    n_employees = 30
    
    # ã‚ˆã‚Šç¾å®Ÿçš„ãªãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’ä½œæˆ
    departments = ['ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°éƒ¨', 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°éƒ¨', 'äººäº‹éƒ¨', 'å–¶æ¥­éƒ¨', 'ç·å‹™éƒ¨', 'çµŒç†éƒ¨']
    positions = ['å½¹è·ãªã—', 'ãƒãƒ¼ãƒ ãƒªãƒ¼ãƒ€ãƒ¼', 'ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼', 'ä¸»ä»»', 'èª²é•·']
    groups = ['Aã‚°ãƒ«ãƒ¼ãƒ—', 'Bã‚°ãƒ«ãƒ¼ãƒ—', 'Cã‚°ãƒ«ãƒ¼ãƒ—', 'Dã‚°ãƒ«ãƒ¼ãƒ—']
    workplaces = ['æ±äº¬æœ¬ç¤¾', 'å¤§é˜ªæ”¯ç¤¾', 'åå¤å±‹æ”¯ç¤¾', 'ç¦å²¡æ”¯ç¤¾', 'æ¨ªæµœã‚ªãƒ•ã‚£ã‚¹']
    business_types = ['å–¶æ¥­ç³»', 'æŠ€è¡“ç³»', 'ç®¡ç†ç³»', 'ã‚µãƒãƒ¼ãƒˆç³»', 'ä¼ç”»ç³»']
    regions = ['é–¢æ±', 'é–¢è¥¿', 'ä¸­éƒ¨', 'ä¹å·', 'é¦–éƒ½åœ']
    job_categories = ['æ­£ç¤¾å“¡', 'å¥‘ç´„ç¤¾å“¡', 'ãƒ‘ãƒ¼ãƒˆ', 'æ´¾é£']
    age_groups = ['20ä»£', '30ä»£', '40ä»£', '50ä»£ä»¥ä¸Š']
    
    # éƒ¨ç½²åˆ¥ã«ç‰¹å¾´ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    employee_data_list = []
    
    for i in range(n_employees):
        department = np.random.choice(departments, p=[0.25, 0.3, 0.1, 0.25, 0.05, 0.05])
        
        # éƒ¨ç½²ã«å¿œã˜ã¦ãƒã‚¸ã‚·ãƒ§ãƒ³ã®åˆ†å¸ƒã‚’èª¿æ•´
        if department in ['äººäº‹éƒ¨', 'ç·å‹™éƒ¨', 'çµŒç†éƒ¨']:
            position = np.random.choice(positions, p=[0.4, 0.3, 0.2, 0.05, 0.05])
        elif department == 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°éƒ¨':
            position = np.random.choice(positions, p=[0.6, 0.2, 0.1, 0.05, 0.05])
        else:
            position = np.random.choice(positions, p=[0.5, 0.25, 0.15, 0.05, 0.05])
        
        # å¹´é½¢å±¤ã‚’è¨­å®š
        age_group = np.random.choice(age_groups, p=[0.3, 0.35, 0.25, 0.1])
        
        # å¹´é½¢ã«å¿œã˜ã¦çµŒé¨“å¹´æ•°ã‚’èª¿æ•´
        if age_group == '20ä»£':
            start_year = np.random.choice(range(2020, 2025))
            base_salary = np.random.normal(350, 50)
        elif age_group == '30ä»£':
            start_year = np.random.choice(range(2015, 2023))
            base_salary = np.random.normal(500, 80)
        elif age_group == '40ä»£':
            start_year = np.random.choice(range(2010, 2020))
            base_salary = np.random.normal(650, 100)
        else:  # 50ä»£ä»¥ä¸Š
            start_year = np.random.choice(range(2005, 2018))
            base_salary = np.random.normal(750, 120)
        
        # ãƒã‚¸ã‚·ãƒ§ãƒ³ã«å¿œã˜ã¦çµ¦ä¸ã‚’èª¿æ•´
        if position == 'ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼':
            salary_multiplier = 1.3
        elif position == 'èª²é•·':
            salary_multiplier = 1.5
        elif position in ['ãƒãƒ¼ãƒ ãƒªãƒ¼ãƒ€ãƒ¼', 'ä¸»ä»»']:
            salary_multiplier = 1.1
        else:
            salary_multiplier = 1.0
        
        annual_salary = int(max(300, min(1200, base_salary * salary_multiplier)))
        
        # æº€è¶³åº¦æŒ‡æ¨™ï¼ˆç›¸é–¢ã‚’æŒãŸã›ã‚‹ï¼‰
        base_satisfaction = np.random.normal(3.2, 0.8)
        satisfaction_noise = np.random.normal(0, 0.3)
        
        overall_satisfaction = max(1, min(5, int(base_satisfaction + satisfaction_noise)))
        
        # æ¨å¥¨ã‚¹ã‚³ã‚¢ï¼ˆ10æ®µéšè©•ä¾¡ï¼‰ã‚’ç”Ÿæˆ
        base_recommend = 6.5  # åŸºæº–æ¨å¥¨ã‚¹ã‚³ã‚¢
        recommend_noise = np.random.normal(0, 1.2)
        recommend_score = max(1, min(10, int(base_recommend + recommend_noise)))
        
        # nps_scoreã‚‚åŒã˜å€¤ã‚’ä½¿ç”¨ï¼ˆ10æ®µéšã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        nps_score = recommend_score
        
        # ãã®ä»–ã®æŒ‡æ¨™
        contribution_score = max(1, min(5, int(base_satisfaction + np.random.normal(0, 0.4))))
        long_term_intention = max(1, min(5, int(base_satisfaction + np.random.normal(0, 0.5))))
        
        employee_data_list.append({
            'response_id': i + 1,
            'department': department,
            'position': position,
            'start_year': start_year,
            'annual_salary': annual_salary,
            'monthly_overtime': max(0, min(80, int(np.random.normal(25, 15)))),
            'paid_leave_rate': max(10, min(100, int(np.random.normal(65, 20)))),
            'recommend_score': recommend_score,
            'nps_score': nps_score,
            'overall_satisfaction': overall_satisfaction,
            'long_term_intention': long_term_intention,
            'contribution_score': contribution_score,
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç”¨é …ç›®
            'group': np.random.choice(groups),
            'workplace': np.random.choice(workplaces),
            'employee_number': f"{((i // 10) * 10 + 1):02d}-{((i // 10 + 1) * 10):02d}",
            'business_type': np.random.choice(business_types),
            'region': np.random.choice(regions),
            'job_category': np.random.choice(job_categories, p=[0.7, 0.15, 0.1, 0.05]),
            'age_group': age_group,
        })
    
    employee_data = pd.DataFrame(employee_data_list)
    
    categories = ['å‹¤å‹™æ™‚é–“', 'ä¼‘æ—¥ä¼‘æš‡', 'æœ‰çµ¦ä¼‘æš‡', 'å‹¤å‹™ä½“ç³»', 'æ˜‡çµ¦æ˜‡æ ¼', 'äººé–“é–¢ä¿‚', 
                 'åƒãç’°å¢ƒ', 'æˆé•·å®Ÿæ„Ÿ', 'å°†æ¥ã‚­ãƒ£ãƒªã‚¢', 'ç¦åˆ©åšç”Ÿ', 'è©•ä¾¡åˆ¶åº¦']
    
    for category in categories:
        employee_data[f'{category}_æœŸå¾…åº¦'] = np.random.choice(range(1, 6), n_employees)
        employee_data[f'{category}_æº€è¶³åº¦'] = np.random.choice(range(1, 6), n_employees)
    
    return {'employee_data': employee_data}

@st.cache_data
def calculate_kpis(data):
    """KPIã‚’è¨ˆç®—ã™ã‚‹"""
    if 'employee_data' not in data:
        return {}
    
    df = data['employee_data']
    
    # æ¨å¥¨åº¦è¨ˆç®—ï¼ˆ10æ®µéšè©•ä¾¡ã®å¹³å‡å€¤ï¼‰
    if 'recommend_score' in df.columns:
        nps = df['recommend_score'].mean()
    elif 'nps_score' in df.columns:
        nps = df['nps_score'].mean()
    else:
        nps = 6.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    # æº€è¶³åº¦ã‚«ãƒ†ã‚´ãƒªãƒ¼
    categories = ['å‹¤å‹™æ™‚é–“', 'ä¼‘æ—¥ä¼‘æš‡', 'æœ‰çµ¦ä¼‘æš‡', 'å‹¤å‹™ä½“ç³»', 'æ˜‡çµ¦æ˜‡æ ¼', 'äººé–“é–¢ä¿‚', 
                 'åƒãç’°å¢ƒ', 'æˆé•·å®Ÿæ„Ÿ', 'å°†æ¥ã‚­ãƒ£ãƒªã‚¢', 'ç¦åˆ©åšç”Ÿ', 'è©•ä¾¡åˆ¶åº¦']
    
    satisfaction_by_category = {}
    expectation_by_category = {}
    gap_by_category = {}
    
    for category in categories:
        sat_col = f'{category}_æº€è¶³åº¦'
        exp_col = f'{category}_æœŸå¾…åº¦'
        if sat_col in df.columns and exp_col in df.columns:
            satisfaction_by_category[category] = df[sat_col].mean()
            expectation_by_category[category] = df[exp_col].mean()
            gap_by_category[category] = df[sat_col].mean() - df[exp_col].mean()
    
    return {
        'total_employees': len(df),
        'nps': nps,
        'avg_nps_score': df['nps_score'].mean(),
        'avg_satisfaction': df['overall_satisfaction'].mean(),
        'avg_contribution': df['contribution_score'].mean(),
        'avg_long_term_intention': df['long_term_intention'].mean(),
        'avg_salary': df['annual_salary'].mean(),
        'avg_overtime': df['monthly_overtime'].mean(),
        'avg_leave_usage': df['paid_leave_rate'].mean(),
        'satisfaction_by_category': satisfaction_by_category,
        'expectation_by_category': expectation_by_category,
        'gap_by_category': gap_by_category,
    }

def show_kpi_overview(data, kpis):
    """KPIæ¦‚è¦ã‚’è¡¨ç¤ºï¼ˆStreamlitæ¨™æº–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆä½¿ç”¨ï¼‰"""
    st.header("ğŸ“Š ç·åˆKPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    
    # ãƒ‡ãƒ¼ã‚¿çŠ¶æ³ã®è¡¨ç¤º
    df = data['employee_data']
    
    # æƒ…å ±ãƒœãƒƒã‚¯ã‚¹
    st.info(f"ğŸ“… **ãƒ‡ãƒ¼ã‚¿æœ€çµ‚æ›´æ–°:** {datetime.now().strftime('%Y/%m/%d %H:%M')} | ğŸ“‹ **èª¿æŸ»é …ç›®:** {len(df.columns)}é …ç›®")
    
    if not kpis:
        st.error("KPIãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return
    
    st.subheader("ğŸ¯ ä¸»è¦æŒ‡æ¨™")
    
    # ãƒ¡ã‚¤ãƒ³KPIï¼ˆStreamlitæ¨™æº–ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½¿ç”¨ï¼‰
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        nps_delta = "ğŸ“ˆ è‰¯å¥½" if kpis['nps'] >= 7 else "ğŸ“‰ è¦æ”¹å–„" if kpis['nps'] <= 5 else "âš ï¸ æ™®é€š"
        nps_color = "normal" if kpis['nps'] >= 7 else "inverse" if kpis['nps'] <= 5 else "off"
        st.metric(
            label="ğŸ“ˆ eNPS",
            value=f"{kpis['nps']:.1f}",
            delta=nps_delta,
            delta_color=nps_color
        )
        st.caption("10æ®µéšè©•ä¾¡")
    
    with col2:
        satisfaction = kpis['avg_satisfaction']
        sat_delta = f"ğŸ˜Š è‰¯å¥½" if satisfaction >= 4 else "ğŸ˜” è¦æ”¹å–„" if satisfaction <= 2.5 else "ğŸ˜ æ™®é€š"
        sat_color = "normal" if satisfaction >= 4 else "inverse" if satisfaction <= 2.5 else "off"
        st.metric(
            label="ğŸ˜Š ç·åˆæº€è¶³åº¦",
            value=f"{satisfaction:.1f}/5",
            delta=sat_delta,
            delta_color=sat_color
        )
        st.caption("å…¨ä½“çš„ãªæº€è¶³åº¦ãƒ¬ãƒ™ãƒ«")
    
    with col3:
        contribution = kpis['avg_contribution']
        cont_delta = "â­ é«˜ã„" if contribution >= 4 else "ğŸ’” ä½ã„" if contribution <= 2.5 else "âš–ï¸ æ™®é€š"
        cont_color = "normal" if contribution >= 4 else "inverse" if contribution <= 2.5 else "off"
        st.metric(
            label="â­ æ´»èºè²¢çŒ®åº¦",
            value=f"{contribution:.1f}/5",
            delta=cont_delta,
            delta_color=cont_color
        )
        st.caption("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡")
    
    with col4:
        intention = kpis['avg_long_term_intention']
        int_delta = "ğŸ¢ é«˜ã„" if intention >= 4 else "âš ï¸ ä½ã„" if intention <= 2.5 else "â– æ™®é€š"
        int_color = "normal" if intention >= 4 else "inverse" if intention <= 2.5 else "off"
        st.metric(
            label="ğŸ¢ å‹¤ç¶šæ„å‘",
            value=f"{intention:.1f}/5",
            delta=int_delta,
            delta_color=int_color
        )
        st.caption("é•·æœŸå®šç€æ„å‘")
    
    st.divider()
    st.subheader("ğŸ“Š åŸºæœ¬æŒ‡æ¨™")
    
    # ã‚µãƒ–KPI
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            label="ğŸ’° å¹³å‡å¹´å",
            value=f"{kpis['avg_salary']:.0f}ä¸‡å††",
            delta="å¹´åãƒ¬ãƒ™ãƒ«"
        )
        st.caption("çµ¦ä¸æ°´æº–")
    
    with col2:
        overtime_delta = "âš ï¸ å¤šã„" if kpis['avg_overtime'] >= 40 else "âœ… é©æ­£" if kpis['avg_overtime'] <= 20 else "âš–ï¸ æ™®é€š"
        overtime_color = "inverse" if kpis['avg_overtime'] >= 40 else "normal" if kpis['avg_overtime'] <= 20 else "off"
        st.metric(
            label="â° æœˆå¹³å‡æ®‹æ¥­æ™‚é–“",
            value=f"{kpis['avg_overtime']:.1f}h",
            delta=overtime_delta,
            delta_color=overtime_color
        )
        st.caption("åŠ´åƒæ™‚é–“ç®¡ç†")
    
    with col3:
        leave_delta = "âœ… è‰¯å¥½" if kpis['avg_leave_usage'] >= 80 else "âš ï¸ ä½ã„" if kpis['avg_leave_usage'] <= 50 else "â– æ™®é€š"
        leave_color = "normal" if kpis['avg_leave_usage'] >= 80 else "inverse" if kpis['avg_leave_usage'] <= 50 else "off"
        st.metric(
            label="ğŸ–ï¸ æœ‰çµ¦å–å¾—ç‡",
            value=f"{kpis['avg_leave_usage']:.1f}%",
            delta=leave_delta,
            delta_color=leave_color
        )
        st.caption("ä¼‘æš‡åˆ©ç”¨çŠ¶æ³")
    
    with col4:
        avg_start_year = kpis.get('avg_start_year', datetime.now().year - 3)
        avg_tenure = datetime.now().year - avg_start_year
        tenure_delta = "ğŸ“ˆ ãƒ™ãƒ†ãƒ©ãƒ³" if avg_tenure > 5 else "ğŸŒ± æ–°é®®" if avg_tenure < 2 else "âš–ï¸ é©åº¦"
        tenure_color = "normal" if 2 <= avg_tenure <= 5 else "off"
        st.metric(
            label="ğŸ‘¤ å¹³å‡å‹¤ç¶šå¹´æ•°",
            value=f"{avg_tenure:.1f}å¹´",
            delta=tenure_delta,
            delta_color=tenure_color
        )
        st.caption("å¹³å‡å‹¤ç¶šæœŸé–“")

def show_satisfaction_analysis(data, kpis):
    """æº€è¶³åº¦åˆ†æã‚’è¡¨ç¤º"""
    st.header("ğŸ“ˆ æº€è¶³åº¦ãƒ»æœŸå¾…åº¦åˆ†æ")
    
    if not kpis or 'satisfaction_by_category' not in kpis:
        st.error("æº€è¶³åº¦ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return
    
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ", "ğŸ“‹ æº€è¶³åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "ğŸ¯ æœŸå¾…åº¦ã‚®ãƒ£ãƒƒãƒ—åˆ†æ"])
    
    with tab1:
        col1, col2 = st.columns(2)
        
        with col1:
            # æº€è¶³åº¦ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
            categories = list(kpis['satisfaction_by_category'].keys())
            satisfaction_values = list(kpis['satisfaction_by_category'].values())
            
            fig = go.Figure()
            fig.add_trace(go.Scatterpolar(
                r=satisfaction_values,
                theta=categories,
                fill='toself',
                name='æº€è¶³åº¦',
                marker_color='rgba(46, 204, 113, 0.6)',
                line=dict(color='rgba(46, 204, 113, 1)', width=3)
            ))
            
            fig.update_layout(
                polar=dict(
                    radialaxis=dict(visible=True, range=[0, 5], tickfont=dict(size=10)),
                    angularaxis=dict(tickfont=dict(size=9))
                ),
                showlegend=False,
                title="æº€è¶³åº¦ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ",
                height=400
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # æœŸå¾…åº¦ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
            if 'expectation_by_category' in kpis:
                expectation_values = list(kpis['expectation_by_category'].values())
                
                fig = go.Figure()
                fig.add_trace(go.Scatterpolar(
                    r=satisfaction_values,
                    theta=categories,
                    fill='toself',
                    name='æº€è¶³åº¦',
                    marker_color='rgba(46, 204, 113, 0.6)',
                    line=dict(color='rgba(46, 204, 113, 1)', width=2)
                ))
                fig.add_trace(go.Scatterpolar(
                    r=expectation_values,
                    theta=categories,
                    fill='toself',
                    name='æœŸå¾…åº¦',
                    marker_color='rgba(52, 152, 219, 0.4)',
                    line=dict(color='rgba(52, 152, 219, 1)', width=2)
                ))
                
                fig.update_layout(
                    polar=dict(
                        radialaxis=dict(visible=True, range=[0, 5], tickfont=dict(size=10)),
                        angularaxis=dict(tickfont=dict(size=9))
                    ),
                    title="æº€è¶³åº¦ vs æœŸå¾…åº¦",
                    height=400
                )
                
                st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        # æº€è¶³åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        categories = list(kpis['satisfaction_by_category'].keys())
        satisfaction_values = list(kpis['satisfaction_by_category'].values())
        
        satisfaction_df = pd.DataFrame({
            'ã‚«ãƒ†ã‚´ãƒª': categories,
            'æº€è¶³åº¦': satisfaction_values
        }).sort_values('æº€è¶³åº¦', ascending=True)
        
        fig = px.bar(
            satisfaction_df,
            x='æº€è¶³åº¦',
            y='ã‚«ãƒ†ã‚´ãƒª',
            orientation='h',
            title="ã‚«ãƒ†ã‚´ãƒªåˆ¥æº€è¶³åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
            color='æº€è¶³åº¦',
            color_continuous_scale='RdYlGn',
            range_color=[1, 5],
            height=600
        )
        
        fig.update_layout(
            xaxis_title="æº€è¶³åº¦ (1-5ç‚¹)",
            yaxis_title="",
            coloraxis_colorbar=dict(title="æº€è¶³åº¦ã‚¹ã‚³ã‚¢")
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with tab3:
        # æœŸå¾…åº¦ã‚®ãƒ£ãƒƒãƒ—åˆ†æ
        if ('gap_by_category' in kpis and 
            'expectation_by_category' in kpis and 
            len(satisfaction_values) > 0):
            
            try:
                gap_df = pd.DataFrame({
                    'ã‚«ãƒ†ã‚´ãƒª': list(kpis['gap_by_category'].keys()),
                    'ã‚®ãƒ£ãƒƒãƒ—': list(kpis['gap_by_category'].values()),
                    'æº€è¶³åº¦': satisfaction_values,
                    'æœŸå¾…åº¦': list(kpis['expectation_by_category'].values())
                })
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
                if gap_df.empty or len(gap_df) == 0:
                    st.warning("æœŸå¾…åº¦ã‚®ãƒ£ãƒƒãƒ—åˆ†æç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
                    return
                    
            except Exception as e:
                st.error(f"ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                return
            
            # 4è±¡é™ãƒ—ãƒ­ãƒƒãƒˆï¼ˆå¤§å¹…æ”¹å–„ç‰ˆï¼‰
            fig = go.Figure()
            
            # 4è±¡é™ã®èƒŒæ™¯è‰²ã‚’è¿½åŠ 
            mid_x, mid_y = 3, 3  # ä¸­å¤®å€¤
            
            # è±¡é™åˆ†é¡é–¢æ•°ã‚’äº‹å‰å®šç¾©
            def classify_quadrant(row):
                x, y = row['æº€è¶³åº¦'], row['æœŸå¾…åº¦']
                if x >= 3 and y >= 3:
                    return 'ğŸ’ª å¼·ã¿'
                elif x < 3 and y >= 3:
                    return 'ğŸ”´ å„ªå…ˆæ”¹å–„èª²é¡Œ'
                elif x >= 3 and y < 3:
                    return 'âœ… ç¾çŠ¶ç¶­æŒé …ç›®'
                else:
                    return 'âš« å¼±ã¿'
            
            # è±¡é™ã®èƒŒæ™¯è‰²ï¼ˆæ‹¡å¤§ç‰ˆ - æ–‡å­—ã®è¦–èªæ€§å‘ä¸Šï¼‰
            fig.add_shape(
                type="rect", x0=0.5, y0=mid_y, x1=mid_x, y1=5.5,
                fillcolor="rgba(245, 101, 101, 0.15)", line=dict(width=0),
                name="å„ªå…ˆæ”¹å–„èª²é¡Œï¼ˆä½æº€è¶³ãƒ»é«˜æœŸå¾…ï¼‰"
            )
            fig.add_shape(
                type="rect", x0=mid_x, y0=mid_y, x1=5.5, y1=5.5,
                fillcolor="rgba(72, 187, 120, 0.15)", line=dict(width=0),
                name="å¼·ã¿ï¼ˆé«˜æº€è¶³ãƒ»é«˜æœŸå¾…ï¼‰"
            )
            fig.add_shape(
                type="rect", x0=0.5, y0=0.5, x1=mid_x, y1=mid_y,
                fillcolor="rgba(237, 137, 54, 0.15)", line=dict(width=0),
                name="å¼±ã¿ï¼ˆä½æº€è¶³ãƒ»ä½æœŸå¾…ï¼‰"
            )
            fig.add_shape(
                type="rect", x0=mid_x, y0=0.5, x1=5.5, y1=mid_y,
                fillcolor="rgba(159, 122, 234, 0.15)", line=dict(width=0),
                name="ç¾çŠ¶ç¶­æŒé …ç›®ï¼ˆé«˜æº€è¶³ãƒ»ä½æœŸå¾…ï¼‰"
            )
            
            # åŒºåˆ‡ã‚Šç·šã‚’è¿½åŠ 
            fig.add_hline(y=mid_y, line_dash="dash", line_color="rgba(128, 128, 128, 0.8)", line_width=2)
            fig.add_vline(x=mid_x, line_dash="dash", line_color="rgba(128, 128, 128, 0.8)", line_width=2)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’è¿½åŠ ï¼ˆãƒ†ã‚­ã‚¹ãƒˆé‡ãªã‚Šå›é¿ç‰ˆï¼‰
            colors = []
            sizes = []
            symbols = []
            text_positions = []
            
            # ãƒ†ã‚­ã‚¹ãƒˆä½ç½®ã‚’å‹•çš„ã«æ±ºå®šã™ã‚‹é–¢æ•°
            def get_optimal_text_position(x, y, index, total_points):
                positions = [
                    "top center", "bottom center", "middle left", "middle right",
                    "top left", "top right", "bottom left", "bottom right"
                ]
                
                # è±¡é™ãƒ™ãƒ¼ã‚¹ã®åŸºæœ¬ä½ç½®ã‚’æ±ºå®š
                if x >= mid_x and y >= mid_y:  # å¼·ã¿
                    base_pos = ["top center", "top right", "middle right"]
                elif x < mid_x and y >= mid_y:  # å„ªå…ˆæ”¹å–„èª²é¡Œ
                    base_pos = ["top center", "top left", "middle left"]
                elif x >= mid_x and y < mid_y:  # ç¾çŠ¶ç¶­æŒé …ç›®
                    base_pos = ["bottom center", "bottom right", "middle right"]
                else:  # å¼±ã¿
                    base_pos = ["bottom center", "bottom left", "middle left"]
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åŸºã¥ã„ã¦ä½ç½®ã‚’å¾ªç’°é¸æŠ
                return base_pos[index % len(base_pos)]
            
            for i, (_, row) in enumerate(gap_df.iterrows()):
                x, y = row['æº€è¶³åº¦'], row['æœŸå¾…åº¦']
                gap = row['ã‚®ãƒ£ãƒƒãƒ—']
                
                # è±¡é™ã«ã‚ˆã£ã¦è‰²ã‚’æ±ºå®šï¼ˆã™ã¹ã¦å††å½¢ã§çµ±ä¸€ï¼‰
                if x >= mid_x and y >= mid_y:
                    colors.append('#48BB78')  # ç·‘ - å¼·ã¿
                    symbols.append('circle')
                elif x < mid_x and y >= mid_y:
                    colors.append('#F56565')  # èµ¤ - å„ªå…ˆæ”¹å–„èª²é¡Œ
                    symbols.append('circle')
                elif x >= mid_x and y < mid_y:
                    colors.append('#9F7AEA')  # ç´« - ç¾çŠ¶ç¶­æŒé …ç›®
                    symbols.append('circle')
                else:
                    colors.append('#ED8936')  # ã‚ªãƒ¬ãƒ³ã‚¸ - å¼±ã¿
                    symbols.append('circle')
                
                sizes.append(20)  # çµ±ä¸€ã‚µã‚¤ã‚º
                text_positions.append(get_optimal_text_position(x, y, i, len(gap_df)))
            
            # ãƒãƒ¼ã‚«ãƒ¼ã®ã¿ã‚’è¡¨ç¤ºï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯åˆ†é›¢ï¼‰
            fig.add_trace(go.Scatter(
                x=gap_df['æº€è¶³åº¦'],
                y=gap_df['æœŸå¾…åº¦'],
                mode='markers',
                marker=dict(
                    size=sizes,
                    color=colors,
                    symbol=symbols,
                    line=dict(width=2, color='white'),
                    opacity=0.9
                ),
                hovertemplate='<b>%{customdata[0]}</b><br>' +
                            'æº€è¶³åº¦: %{x:.1f}<br>' +
                            'æœŸå¾…åº¦: %{y:.1f}<br>' +
                            'ã‚®ãƒ£ãƒƒãƒ—: %{customdata[1]:.2f}<br>' +
                            'è±¡é™: %{customdata[2]}<extra></extra>',
                customdata=list(zip(
                    gap_df['ã‚«ãƒ†ã‚´ãƒª'], 
                    gap_df['ã‚®ãƒ£ãƒƒãƒ—'],
                    [classify_quadrant(row) for _, row in gap_df.iterrows()]
                )),
                showlegend=False,
                name=""
            ))
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’å€‹åˆ¥ã«è¿½åŠ ï¼ˆé‡ãªã‚Šå›é¿ï¼‰
            for i, (_, row) in enumerate(gap_df.iterrows()):
                x, y = row['æº€è¶³åº¦'], row['æœŸå¾…åº¦']
                category = row['ã‚«ãƒ†ã‚´ãƒª']
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¨ˆç®—ï¼ˆè±¡é™ãƒ©ãƒ™ãƒ«ã¨ã®é‡ãªã‚Šã‚’é¿ã‘ã‚‹ï¼‰
                offset_map = {
                    "top center": (0, 0.2),
                    "bottom center": (0, -0.2),
                    "middle left": (-0.3, 0),
                    "middle right": (0.3, 0),
                    "top left": (-0.25, 0.2),
                    "top right": (0.25, 0.2),
                    "bottom left": (-0.25, -0.2),
                    "bottom right": (0.25, -0.2)
                }
                
                text_pos = text_positions[i]
                offset_x, offset_y = offset_map.get(text_pos, (0, 0.15))
                
                fig.add_annotation(
                    x=x + offset_x,
                    y=y + offset_y,
                    text=f"<b>{category}</b>",
                    showarrow=True,
                    arrowhead=2,
                    arrowsize=1,
                    arrowwidth=1,
                    arrowcolor=colors[i],
                    ax=0,
                    ay=-15 if "top" in text_pos else 15 if "bottom" in text_pos else 0,
                    font=dict(size=9, color='#1f2937'),
                    bgcolor='rgba(255, 255, 255, 0.8)',
                    bordercolor=colors[i],
                    borderwidth=1
                )
            
            # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š
            fig.update_layout(
                title={
                    'text': "æœŸå¾…åº¦ vs æº€è¶³åº¦ 4è±¡é™åˆ†æ",
                    'x': 0.5,
                    'font': {'size': 18, 'color': '#1f2937'}
                },
                xaxis=dict(
                    title="æº€è¶³åº¦ â†’",
                    range=[0.3, 5.7],
                    showgrid=True,
                    gridcolor='rgba(128, 128, 128, 0.2)',
                    dtick=1
                ),
                yaxis=dict(
                    title="â†‘ æœŸå¾…åº¦",
                    range=[0.3, 5.7],
                    showgrid=True,
                    gridcolor='rgba(128, 128, 128, 0.2)',
                    dtick=1
                ),
                height=650,
                plot_bgcolor='white',
                paper_bgcolor='white'
            )
            
            # è±¡é™ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ ï¼ˆé‡ãªã‚Šå›é¿ã®ãŸã‚å¤–å´ã«é…ç½®ï¼‰
            annotations = [
                dict(x=4.8, y=4.8, text="<b>ğŸ’ª å¼·ã¿</b><br>(é«˜æº€è¶³ãƒ»é«˜æœŸå¾…)", 
                     showarrow=False, font=dict(size=10, color='#22543d'), 
                     bgcolor='rgba(72, 187, 120, 0.2)', bordercolor='#48BB78',
                     xanchor='center', yanchor='middle'),
                dict(x=1.2, y=4.8, text="<b>ğŸ”´ å„ªå…ˆæ”¹å–„èª²é¡Œ</b><br>(ä½æº€è¶³ãƒ»é«˜æœŸå¾…)", 
                     showarrow=False, font=dict(size=10, color='#742a2a'), 
                     bgcolor='rgba(245, 101, 101, 0.2)', bordercolor='#F56565',
                     xanchor='center', yanchor='middle'),
                dict(x=4.8, y=1.2, text="<b>âœ… ç¾çŠ¶ç¶­æŒé …ç›®</b><br>(é«˜æº€è¶³ãƒ»ä½æœŸå¾…)", 
                     showarrow=False, font=dict(size=10, color='#553c9a'), 
                     bgcolor='rgba(159, 122, 234, 0.2)', bordercolor='#9F7AEA',
                     xanchor='center', yanchor='middle'),
                dict(x=1.2, y=1.2, text="<b>âš« å¼±ã¿</b><br>(ä½æº€è¶³ãƒ»ä½æœŸå¾…)", 
                     showarrow=False, font=dict(size=10, color='#9c4221'), 
                     bgcolor='rgba(237, 137, 54, 0.2)', bordercolor='#ED8936',
                     xanchor='center', yanchor='middle')
            ]
            
            for ann in annotations:
                fig.add_annotation(**ann)
            
            st.plotly_chart(fig, use_container_width=True)
            
            # è±¡é™åˆ¥ã®èª¬æ˜
            st.info("""
            **ğŸ“Š 4è±¡é™ã®è§£é‡ˆ**
            - ğŸ’ª **å¼·ã¿ï¼ˆå³ä¸Šï¼‰**: æº€è¶³åº¦ãƒ»æœŸå¾…åº¦ã¨ã‚‚ã«é«˜ã„é …ç›®ã€‚ç«¶äº‰å„ªä½æ€§ã®æºæ³‰
            - ğŸ”´ **å„ªå…ˆæ”¹å–„èª²é¡Œï¼ˆå·¦ä¸Šï¼‰**: æœŸå¾…ã¯é«˜ã„ãŒæº€è¶³åº¦ãŒä½ã„é …ç›®ã€‚æœ€å„ªå…ˆã§æ”¹å–„ãŒå¿…è¦
            - âœ… **ç¾çŠ¶ç¶­æŒé …ç›®ï¼ˆå³ä¸‹ï¼‰**: æº€è¶³åº¦ã¯é«˜ã„ãŒæœŸå¾…åº¦ãŒä½ã„é …ç›®ã€‚ç¾çŠ¶ã®å“è³ªã‚’ç¶­æŒ
            - âš« **å¼±ã¿ï¼ˆå·¦ä¸‹ï¼‰**: æœŸå¾…ãƒ»æº€è¶³ã¨ã‚‚ã«ä½ã„é …ç›®ã€‚é•·æœŸçš„ãªæ”¹å–„æ¤œè¨ãŒå¿…è¦
            """)
            
            # ã‚®ãƒ£ãƒƒãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆæ”¹å–„ç‰ˆï¼‰
            st.subheader("ğŸ“‹ è±¡é™åˆ¥åˆ†æçµæœ")
            
            try:
                gap_display = gap_df.copy()
                gap_display['è±¡é™'] = gap_display.apply(classify_quadrant, axis=1)
                gap_display['ã‚®ãƒ£ãƒƒãƒ—è©•ä¾¡'] = gap_display['ã‚®ãƒ£ãƒƒãƒ—'].apply(
                    lambda x: 'ğŸ˜Š æº€è¶³>æœŸå¾…' if x > 0.3 else 'ğŸ˜” æœŸå¾…>æº€è¶³' if x < -0.3 else 'ğŸ˜ ã»ã¼åŒç­‰'
                )
                
                # å„ªå…ˆåº¦ã‚’è¨­å®š
                priority_map = {'ğŸ”´ å„ªå…ˆæ”¹å–„èª²é¡Œ': 1, 'âš« å¼±ã¿': 2, 'âœ… ç¾çŠ¶ç¶­æŒé …ç›®': 3, 'ğŸ’ª å¼·ã¿': 4}
                gap_display['å„ªå…ˆåº¦'] = gap_display['è±¡é™'].map(priority_map)
                
                # NaNã®å‡¦ç†
                gap_display['å„ªå…ˆåº¦'] = gap_display['å„ªå…ˆåº¦'].fillna(5)
                
                # å¿…è¦ãªã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                required_columns = ['ã‚«ãƒ†ã‚´ãƒª', 'æº€è¶³åº¦', 'æœŸå¾…åº¦', 'ã‚®ãƒ£ãƒƒãƒ—', 'è±¡é™', 'ã‚®ãƒ£ãƒƒãƒ—è©•ä¾¡', 'å„ªå…ˆåº¦']
                if all(col in gap_display.columns for col in required_columns):
                    # è¡¨ç¤ºç”¨ã«æ•´ç†
                    display_df = gap_display[required_columns].sort_values('å„ªå…ˆåº¦')
                    display_df = display_df.round({'æº€è¶³åº¦': 1, 'æœŸå¾…åº¦': 1, 'ã‚®ãƒ£ãƒƒãƒ—': 2})
                    display_df = display_df.drop_duplicates().reset_index(drop=True)
                    
                    # å„ªå…ˆåº¦ã‚«ãƒ©ãƒ ã‚’é™¤å¤–ã—ã¦è¡¨ç¤º
                    st.dataframe(
                        display_df[['ã‚«ãƒ†ã‚´ãƒª', 'æº€è¶³åº¦', 'æœŸå¾…åº¦', 'ã‚®ãƒ£ãƒƒãƒ—', 'è±¡é™', 'ã‚®ãƒ£ãƒƒãƒ—è©•ä¾¡']], 
                        use_container_width=True,
                        hide_index=True
                    )
                else:
                    st.error("å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
                    
            except Exception as e:
                st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                st.info("åŸºæœ¬çš„ãªæº€è¶³åº¦ãƒ»æœŸå¾…åº¦ãƒ‡ãƒ¼ã‚¿ã¯åˆ©ç”¨å¯èƒ½ã§ã™ã€‚è©³ç´°ãªåˆ†æè¡¨ç¤ºã®ã¿ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚")
                
        else:
            st.warning("æœŸå¾…åº¦ã‚®ãƒ£ãƒƒãƒ—åˆ†æã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚æº€è¶³åº¦ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯æœŸå¾…åº¦ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")

def show_text_mining_analysis():
    """ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°åˆ†æã‚’è¡¨ç¤º"""
    st.header("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°åˆ†æ")
    
    # ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    with st.spinner("ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        comments = load_comment_data()
    
    if not comments:
        st.error("ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    # ã‚¿ãƒ–ã‚’ä½œæˆ
    tabs = st.tabs(["ğŸ“Š ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ", "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ“‹ ã‚³ãƒ¡ãƒ³ãƒˆä¸€è¦§"])
    
    with tabs[0]:  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
        st.subheader("ğŸ” é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ")
        
        comment_type = st.selectbox(
            "åˆ†æã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆç¨®åˆ¥ã‚’é¸æŠ:",
            list(comments.keys()),
            key="keyword_analysis_type"
        )
        
        if comment_type in comments and comments[comment_type]:
            keywords = extract_keywords_janome(comments[comment_type], max_features=20)
            
            if keywords:
                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
                keyword_df = pd.DataFrame(keywords, columns=['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'å‡ºç¾å›æ•°'])
                
                # æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º
                fig = px.bar(
                    keyword_df, 
                    x='å‡ºç¾å›æ•°', 
                    y='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰',
                    orientation='h',
                    title=f"{comment_type} - é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
                    color='å‡ºç¾å›æ•°',
                    color_continuous_scale='viridis'
                )
                fig.update_layout(height=600, yaxis={'categoryorder':'total ascending'})
                st.plotly_chart(fig, use_container_width=True)
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
                st.subheader("ğŸ“‹ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è©³ç´°")
                st.dataframe(keyword_df, use_container_width=True, hide_index=True)
            else:
                st.info("æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            st.info(f"{comment_type}ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    with tabs[1]:  # å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        st.subheader("ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ")
        
        comment_type = st.selectbox(
            "åˆ†æã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆç¨®åˆ¥ã‚’é¸æŠ:",
            list(comments.keys()),
            key="network_analysis_type"
        )
        
        min_cooccurrence = st.slider("æœ€å°å…±èµ·å›æ•°", 1, 5, 1)
        
        if comment_type in comments and comments[comment_type]:
            G, cooccurrence = build_cooccurrence_network(comments[comment_type], min_cooccurrence)
            
            if G and len(G.nodes()) > 0:
                # NetworkXã‚’ä½¿ã£ã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’ä½œæˆ
                pos = nx.spring_layout(G, k=3, iterations=50)
                
                # ã‚¨ãƒƒã‚¸ã®æƒ…å ±ã‚’å–å¾—
                edge_x = []
                edge_y = []
                edge_info = []
                
                for edge in G.edges():
                    x0, y0 = pos[edge[0]]
                    x1, y1 = pos[edge[1]]
                    edge_x.extend([x0, x1, None])
                    edge_y.extend([y0, y1, None])
                    weight = G[edge[0]][edge[1]]['weight']
                    edge_info.append(f"{edge[0]} - {edge[1]}: {weight}å›")
                
                # ãƒãƒ¼ãƒ‰ã®æƒ…å ±ã‚’å–å¾—
                node_x = []
                node_y = []
                node_text = []
                node_size = []
                
                for node in G.nodes():
                    x, y = pos[node]
                    node_x.append(x)
                    node_y.append(y)
                    node_text.append(node)
                    node_size.append(10 + G.degree(node) * 5)
                
                # Plotlyã§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’ä½œæˆ
                fig = go.Figure()
                
                # ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
                fig.add_trace(go.Scatter(
                    x=edge_x, y=edge_y,
                    line=dict(width=2, color='#888'),
                    hoverinfo='none',
                    mode='lines',
                    showlegend=False
                ))
                
                # ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
                fig.add_trace(go.Scatter(
                    x=node_x, y=node_y,
                    mode='markers+text',
                    hoverinfo='text',
                    text=node_text,
                    textposition="middle center",
                    textfont=dict(size=12, color='white'),
                    marker=dict(
                        size=node_size,
                        color=node_size,
                        colorscale='viridis',
                        line=dict(width=2, color='white')
                    ),
                    showlegend=False
                ))
                
                fig.update_layout(
                    title=f"{comment_type} - å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³",
                    showlegend=False,
                    hovermode='closest',
                    margin=dict(b=20,l=5,r=5,t=40),
                    annotations=[ dict(
                        text="ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚º: æ¥ç¶šæ•°ã€ç·š: å…±èµ·é–¢ä¿‚",
                        showarrow=False,
                        xref="paper", yref="paper",
                        x=0.005, y=-0.002 ,
                        xanchor="left", yanchor="bottom",
                        font=dict(color="gray", size=12)
                    )],
                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                    height=600
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # å…±èµ·é–¢ä¿‚ã®è©³ç´°
                st.subheader("ğŸ“Š å…±èµ·é–¢ä¿‚è©³ç´°")
                cooccurrence_df = pd.DataFrame([
                    {'èª1': pair[0], 'èª2': pair[1], 'å…±èµ·å›æ•°': count}
                    for pair, count in sorted(cooccurrence.items(), key=lambda x: x[1], reverse=True)
                ])
                st.dataframe(cooccurrence_df, use_container_width=True, hide_index=True)
            else:
                st.info("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æœ€å°å…±èµ·å›æ•°ã‚’ä¸‹ã’ã¦ã¿ã¦ãã ã•ã„ã€‚")
        else:
            st.info(f"{comment_type}ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    with tabs[2]:  # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
        st.subheader("â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
        
        comment_type = st.selectbox(
            "åˆ†æã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆç¨®åˆ¥ã‚’é¸æŠ:",
            list(comments.keys()),
            key="wordcloud_analysis_type"
        )
        
        if comment_type in comments and comments[comment_type]:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            keywords = extract_keywords_janome(comments[comment_type], max_features=50)
            
            if keywords:
                # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
                wordcloud_dict = dict(keywords)
                
                try:
                    # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ
                    wc = WordCloud(
                        font_path=None,  # ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨
                        width=800, height=400,
                        background_color='white',
                        max_words=50,
                        colormap='viridis'
                    ).generate_from_frequencies(wordcloud_dict)
                    
                    # matplotlib ã§è¡¨ç¤º
                    fig, ax = plt.subplots(figsize=(12, 6))
                    ax.imshow(wc, interpolation='bilinear')
                    ax.axis('off')
                    ax.set_title(f'{comment_type} - ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰', fontsize=16, pad=20)
                    
                    st.pyplot(fig)
                    
                except Exception as e:
                    st.warning(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                    st.info("ä»£ã‚ã‚Šã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’è¡¨ç¤ºã—ã¾ã™:")
                    keyword_df = pd.DataFrame(keywords, columns=['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'å‡ºç¾å›æ•°'])
                    st.dataframe(keyword_df, use_container_width=True, hide_index=True)
            else:
                st.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.info(f"{comment_type}ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    with tabs[3]:  # ã‚³ãƒ¡ãƒ³ãƒˆä¸€è¦§
        st.subheader("ğŸ“‹ ã‚³ãƒ¡ãƒ³ãƒˆä¸€è¦§")
        
        comment_type = st.selectbox(
            "è¡¨ç¤ºã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆç¨®åˆ¥ã‚’é¸æŠ:",
            list(comments.keys()),
            key="comment_list_type"
        )
        
        if comment_type in comments and comments[comment_type]:
            st.write(f"**{comment_type}** ({len(comments[comment_type])}ä»¶)")
            
            for i, comment in enumerate(comments[comment_type], 1):
                with st.expander(f"ã‚³ãƒ¡ãƒ³ãƒˆ {i}"):
                    st.write(comment)
        else:
            st.info(f"{comment_type}ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

def show_time_series_analysis():
    """KPIæ™‚ç³»åˆ—åˆ†æã‚’è¡¨ç¤º"""
    st.header("ğŸ“ˆ KPIæ™‚ç³»åˆ—åˆ†æ")
    
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    with st.spinner("ğŸ“Š èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        data = load_employee_data()
        
    if not data or 'employee_data' not in data:
        st.error("èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    # ç¾åœ¨ã¯1å›åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã—ã‹ãªã„ãŸã‚ã€ãƒ‡ãƒ¢ç”¨ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    st.info("ğŸ“Š ç¾åœ¨ã¯1å›åˆ†ã®èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ã®ã¿ã®ãŸã‚ã€ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¾ã™")
    
    # ãƒ‡ãƒ¢ç”¨ã®æ™‚ç³»åˆ—KPIãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    monthly_kpi_data = create_dummy_monthly_kpi_data()
    
    # ãƒ¡ã‚¤ãƒ³æŒ‡æ¨™ã®æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•
    st.subheader("ğŸ“ˆ ä¸»è¦KPIæŒ‡æ¨™ã®æœˆåˆ¥æ¨ç§»")
    
    # 4ã¤ã®ä¸»è¦æŒ‡æ¨™ã‚’2x2ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§è¡¨ç¤º
    col1, col2 = st.columns(2)
    
    with col1:
        # eNPSæ¨ç§»
        fig_nps = px.line(
            monthly_kpi_data,
            x='å¹´æœˆ',
            y='eNPS',
            title='eNPS (Employee Net Promoter Score) æ¨ç§»',
            markers=True,
            color_discrete_sequence=['#FF6B6B']
        )
        fig_nps.update_layout(
            height=300,
            yaxis_title='eNPS (%)',
            xaxis_title='å¹´æœˆ'
        )
        fig_nps.add_hline(y=0, line_dash="dash", line_color="gray", annotation_text="åŸºæº–ç·š (0)")
        st.plotly_chart(fig_nps, use_container_width=True)
        
        # æ´»èºè²¢çŒ®åº¦æ¨ç§»
        fig_contribution = px.line(
            monthly_kpi_data,
            x='å¹´æœˆ',
            y='æ´»èºè²¢çŒ®åº¦',
            title='æ´»èºè²¢çŒ®åº¦ æ¨ç§»',
            markers=True,
            color_discrete_sequence=['#4ECDC4']
        )
        fig_contribution.update_layout(
            height=300,
            yaxis_title='æ´»èºè²¢çŒ®åº¦ (1-5ç‚¹)',
            xaxis_title='å¹´æœˆ',
            yaxis=dict(range=[1, 5])
        )
        st.plotly_chart(fig_contribution, use_container_width=True)
    
    with col2:
        # ç·åˆæº€è¶³åº¦æ¨ç§»
        fig_satisfaction = px.line(
            monthly_kpi_data,
            x='å¹´æœˆ',
            y='ç·åˆæº€è¶³åº¦',
            title='ç·åˆæº€è¶³åº¦ æ¨ç§»',
            markers=True,
            color_discrete_sequence=['#45B7D1']
        )
        fig_satisfaction.update_layout(
            height=300,
            yaxis_title='ç·åˆæº€è¶³åº¦ (1-5ç‚¹)',
            xaxis_title='å¹´æœˆ',
            yaxis=dict(range=[1, 5])
        )
        st.plotly_chart(fig_satisfaction, use_container_width=True)
        
        # å‹¤ç¶šæ„å‘æ¨ç§»
        fig_retention = px.line(
            monthly_kpi_data,
            x='å¹´æœˆ',
            y='å‹¤ç¶šæ„å‘',
            title='å‹¤ç¶šæ„å‘ æ¨ç§»',
            markers=True,
            color_discrete_sequence=['#96CEB4']
        )
        fig_retention.update_layout(
            height=300,
            yaxis_title='å‹¤ç¶šæ„å‘ (1-5ç‚¹)',
            xaxis_title='å¹´æœˆ',
            yaxis=dict(range=[1, 5])
        )
        st.plotly_chart(fig_retention, use_container_width=True)
    
    # å…¨æŒ‡æ¨™ã‚’1ã¤ã®ã‚°ãƒ©ãƒ•ã§æ¯”è¼ƒ
    st.subheader("ğŸ“Š ä¸»è¦æŒ‡æ¨™æ¯”è¼ƒ (æ­£è¦åŒ–)")
    
    # æ­£è¦åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆ0-1ã®ç¯„å›²ã«æ­£è¦åŒ–ï¼‰
    normalized_data = monthly_kpi_data.copy()
    normalized_data['eNPS_æ­£è¦åŒ–'] = (normalized_data['eNPS'] + 100) / 200  # -100~100 â†’ 0~1
    normalized_data['ç·åˆæº€è¶³åº¦_æ­£è¦åŒ–'] = (normalized_data['ç·åˆæº€è¶³åº¦'] - 1) / 4  # 1~5 â†’ 0~1
    normalized_data['æ´»èºè²¢çŒ®åº¦_æ­£è¦åŒ–'] = (normalized_data['æ´»èºè²¢çŒ®åº¦'] - 1) / 4  # 1~5 â†’ 0~1
    normalized_data['å‹¤ç¶šæ„å‘_æ­£è¦åŒ–'] = (normalized_data['å‹¤ç¶šæ„å‘'] - 1) / 4  # 1~5 â†’ 0~1
    
    # æ¯”è¼ƒã‚°ãƒ©ãƒ•
    fig_compare = go.Figure()
    
    fig_compare.add_trace(go.Scatter(
        x=normalized_data['å¹´æœˆ'],
        y=normalized_data['eNPS_æ­£è¦åŒ–'],
        mode='lines+markers',
        name='eNPS',
        line=dict(color='#FF6B6B')
    ))
    
    fig_compare.add_trace(go.Scatter(
        x=normalized_data['å¹´æœˆ'],
        y=normalized_data['ç·åˆæº€è¶³åº¦_æ­£è¦åŒ–'],
        mode='lines+markers',
        name='ç·åˆæº€è¶³åº¦',
        line=dict(color='#45B7D1')
    ))
    
    fig_compare.add_trace(go.Scatter(
        x=normalized_data['å¹´æœˆ'],
        y=normalized_data['æ´»èºè²¢çŒ®åº¦_æ­£è¦åŒ–'],
        mode='lines+markers',
        name='æ´»èºè²¢çŒ®åº¦',
        line=dict(color='#4ECDC4')
    ))
    
    fig_compare.add_trace(go.Scatter(
        x=normalized_data['å¹´æœˆ'],
        y=normalized_data['å‹¤ç¶šæ„å‘_æ­£è¦åŒ–'],
        mode='lines+markers',
        name='å‹¤ç¶šæ„å‘',
        line=dict(color='#96CEB4')
    ))
    
    fig_compare.update_layout(
        title='ä¸»è¦KPIæŒ‡æ¨™ã®æ¨ç§»æ¯”è¼ƒ (æ­£è¦åŒ–æ¸ˆã¿)',
        xaxis_title='å¹´æœˆ',
        yaxis_title='æ­£è¦åŒ–å€¤ (0-1)',
        height=400,
        hovermode='x unified'
    )
    
    st.plotly_chart(fig_compare, use_container_width=True)
    
    # æœˆåˆ¥è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
    st.subheader("ğŸ“‹ æœˆåˆ¥KPIè©³ç´°ãƒ‡ãƒ¼ã‚¿")
    
    # è¡¨ç¤ºç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
    display_data = monthly_kpi_data.copy()
    display_data['eNPS'] = display_data['eNPS'].round(1)
    display_data['ç·åˆæº€è¶³åº¦'] = display_data['ç·åˆæº€è¶³åº¦'].round(2)
    display_data['æ´»èºè²¢çŒ®åº¦'] = display_data['æ´»èºè²¢çŒ®åº¦'].round(2)
    display_data['å‹¤ç¶šæ„å‘'] = display_data['å‹¤ç¶šæ„å‘'].round(2)
    display_data['å›ç­”è€…æ•°'] = display_data['å›ç­”è€…æ•°'].astype(int)
    
    st.dataframe(display_data, use_container_width=True, hide_index=True)
    
    # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
    st.subheader("ğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        nps_trend = monthly_kpi_data['eNPS'].iloc[-1] - monthly_kpi_data['eNPS'].iloc[0]
        st.metric(
            "eNPSå¤‰åŒ–", 
            f"{monthly_kpi_data['eNPS'].iloc[-1]:.1f}%",
            delta=f"{nps_trend:+.1f}%"
        )
    
    with col2:
        satisfaction_trend = monthly_kpi_data['ç·åˆæº€è¶³åº¦'].iloc[-1] - monthly_kpi_data['ç·åˆæº€è¶³åº¦'].iloc[0]
        st.metric(
            "ç·åˆæº€è¶³åº¦å¤‰åŒ–",
            f"{monthly_kpi_data['ç·åˆæº€è¶³åº¦'].iloc[-1]:.2f}ç‚¹",
            delta=f"{satisfaction_trend:+.2f}ç‚¹"
        )
    
    with col3:
        contribution_trend = monthly_kpi_data['æ´»èºè²¢çŒ®åº¦'].iloc[-1] - monthly_kpi_data['æ´»èºè²¢çŒ®åº¦'].iloc[0]
        st.metric(
            "æ´»èºè²¢çŒ®åº¦å¤‰åŒ–",
            f"{monthly_kpi_data['æ´»èºè²¢çŒ®åº¦'].iloc[-1]:.2f}ç‚¹",
            delta=f"{contribution_trend:+.2f}ç‚¹"
        )
    
    with col4:
        retention_trend = monthly_kpi_data['å‹¤ç¶šæ„å‘'].iloc[-1] - monthly_kpi_data['å‹¤ç¶šæ„å‘'].iloc[0]
        st.metric(
            "å‹¤ç¶šæ„å‘å¤‰åŒ–",
            f"{monthly_kpi_data['å‹¤ç¶šæ„å‘'].iloc[-1]:.2f}ç‚¹",
            delta=f"{retention_trend:+.2f}ç‚¹"
        )

def create_dummy_monthly_kpi_data():
    """ãƒ‡ãƒ¢ç”¨ã®æœˆåˆ¥KPIãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    np.random.seed(42)
    
    # éå»12ãƒ¶æœˆã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    months = []
    base_date = datetime.now().replace(day=1) - timedelta(days=365)
    
    for i in range(12):
        current_date = base_date + timedelta(days=30*i)
        months.append(current_date.strftime('%Y-%m'))
    
    # åŸºæº–å€¤ã‹ã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    base_recommend = 6.5  # åŸºæº–æ¨å¥¨ã‚¹ã‚³ã‚¢ï¼ˆ10æ®µéšï¼‰
    base_satisfaction = 3.2  # åŸºæº–æº€è¶³åº¦
    base_contribution = 3.5  # åŸºæº–æ´»èºè²¢çŒ®åº¦
    base_retention = 3.1  # åŸºæº–å‹¤ç¶šæ„å‘
    
    monthly_data = []
    
    for i, month in enumerate(months):
        # å­£ç¯€æ€§ã¨ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å«ã‚“ã ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        seasonal_factor = 0.1 * np.sin(2 * np.pi * i / 12)  # å­£ç¯€å¤‰å‹•
        trend_factor = i * 0.02  # ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰
        
        # ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚º
        noise_recommend = np.random.normal(0, 0.3)
        noise_satisfaction = np.random.normal(0, 0.1)
        noise_contribution = np.random.normal(0, 0.1)
        noise_retention = np.random.normal(0, 0.1)
        
        # æ¨å¥¨ã‚¹ã‚³ã‚¢ã‚’10æ®µéšã§ç”Ÿæˆ
        recommend = base_recommend + trend_factor * 3 + seasonal_factor * 0.5 + noise_recommend
        recommend = max(1, min(10, recommend))
        
        # æ¨å¥¨ã‚¹ã‚³ã‚¢ï¼ˆ10æ®µéšè©•ä¾¡ã®å¹³å‡å€¤ï¼‰
        enps = recommend
        
        satisfaction = base_satisfaction + trend_factor * 2 + seasonal_factor * 0.2 + noise_satisfaction
        contribution = base_contribution + trend_factor * 1.5 + seasonal_factor * 0.15 + noise_contribution
        retention = base_retention + trend_factor * 1.8 + seasonal_factor * 0.18 + noise_retention
        
        # å€¤ã®ç¯„å›²åˆ¶é™
        satisfaction = max(1, min(5, satisfaction))
        contribution = max(1, min(5, contribution))
        retention = max(1, min(5, retention))
        
        monthly_data.append({
            'å¹´æœˆ': month,
            'eNPS': enps,
            'ç·åˆæº€è¶³åº¦': satisfaction,
            'æ´»èºè²¢çŒ®åº¦': contribution,
            'å‹¤ç¶šæ„å‘': retention,
            'å›ç­”è€…æ•°': np.random.randint(45, 55)  # å›ç­”è€…æ•°ã‚‚å¤‰å‹•
        })
    
    return pd.DataFrame(monthly_data)

def show_department_analysis(data, kpis):
    """éƒ¨ç½²åˆ¥åˆ†æã‚’è¡¨ç¤º"""
    st.header("ğŸ¢ éƒ¨ç½²åˆ¥ãƒ»è©³ç´°åˆ†æ")
    
    if 'employee_data' not in data:
        st.error("éƒ¨ç½²åˆ¥ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return
    
    df = data['employee_data']
    
    # ã‚¿ãƒ–ã‚’ä½œæˆã—ã¦å†…å®¹ã‚’æ•´ç†
    tabs = st.tabs(["ğŸ“Š éƒ¨ç½²åˆ¥åˆ†æ", "ğŸ’ª å¼·ã¿ãƒ»å¼±ã¿åˆ†æ"])
    
    with tabs[0]:  # éƒ¨ç½²åˆ¥åˆ†æã‚¿ãƒ–
        # éƒ¨ç½²åˆ¥çµ±è¨ˆ
        if 'department' in df.columns:
            dept_stats = df.groupby('department').agg({
                'overall_satisfaction': 'mean',
                'nps_score': 'mean',
                'contribution_score': 'mean',
                'long_term_intention': 'mean',
                'annual_salary': 'mean',
                'monthly_overtime': 'mean',
                'response_id': 'count'
            }).round(2)
            
            dept_stats.columns = ['ç·åˆæº€è¶³åº¦', 'NPS', 'æ´»èºè²¢çŒ®åº¦', 'å‹¤ç¶šæ„å‘', 'å¹³å‡å¹´å', 'å¹³å‡æ®‹æ¥­æ™‚é–“', 'å›ç­”è€…æ•°']
            
            # éƒ¨ç½²åˆ¥æº€è¶³åº¦æ¯”è¼ƒ
            fig = px.bar(
                dept_stats.reset_index(),
                x='department',
                y='ç·åˆæº€è¶³åº¦',
                title='éƒ¨ç½²åˆ¥ç·åˆæº€è¶³åº¦',
                color='ç·åˆæº€è¶³åº¦',
                color_continuous_scale='RdYlGn',
                text='ç·åˆæº€è¶³åº¦'
            )
            fig.update_traces(texttemplate='%{text:.2f}', textposition='outside')
            fig.update_layout(height=400, xaxis_title='éƒ¨ç½²', yaxis_title='æº€è¶³åº¦')
            st.plotly_chart(fig, use_container_width=True)
            
            # è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
            st.subheader("éƒ¨ç½²åˆ¥è©³ç´°ãƒ‡ãƒ¼ã‚¿")
            st.dataframe(dept_stats, use_container_width=True)
        else:
            st.info("éƒ¨ç½²æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€å€‹åˆ¥å›ç­”è€…ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¾ã™")
            
            # å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            display_cols = ['response_id', 'overall_satisfaction', 'nps_score', 'contribution_score', 
                           'annual_salary', 'monthly_overtime', 'paid_leave_rate']
            available_cols = [col for col in display_cols if col in df.columns]
            
            if available_cols:
                st.dataframe(df[available_cols], use_container_width=True)
    
    with tabs[1]:  # å¼·ã¿ãƒ»å¼±ã¿åˆ†æã‚¿ãƒ–
        show_strengths_weaknesses_analysis(data, kpis)

def show_strengths_weaknesses_analysis(data, kpis):
    """å¼·ã¿ãƒ»å¼±ã¿åˆ†æã‚’è¡¨ç¤º"""
    st.subheader("ğŸ’ª çµ„ç¹”ã®å¼·ã¿ãƒ»å¼±ã¿åˆ†æ")
    
    if 'satisfaction_by_category' not in kpis or not kpis['satisfaction_by_category']:
        st.warning("æº€è¶³åº¦ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return
    
    satisfaction_data = kpis['satisfaction_by_category']
    
    # æº€è¶³åº¦ã‚’é™é †ã§ã‚½ãƒ¼ãƒˆ
    sorted_satisfaction = sorted(satisfaction_data.items(), key=lambda x: x[1], reverse=True)
    
    # TOP5ã¨BOTTOM5ã‚’æŠ½å‡º
    top5_strengths = sorted_satisfaction[:5]
    bottom5_weaknesses = sorted_satisfaction[-5:]
    
    # 2åˆ—ã«åˆ†ã‘ã¦è¡¨ç¤º
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### ğŸŒŸ **TOP5 å¼·ã¿é ˜åŸŸ**")
        
        # å¼·ã¿é ˜åŸŸã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        strengths_df = pd.DataFrame(top5_strengths, columns=['ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'æº€è¶³åº¦'])
        strengths_df['æº€è¶³åº¦'] = strengths_df['æº€è¶³åº¦'].round(2)
        
        # å¼·ã¿é ˜åŸŸã®æ£’ã‚°ãƒ©ãƒ•
        fig_strengths = px.bar(
            strengths_df,
            x='æº€è¶³åº¦',
            y='ã‚«ãƒ†ã‚´ãƒªãƒ¼',
            orientation='h',
            title='çµ„ç¹”ã®å¼·ã¿ TOP5',
            color='æº€è¶³åº¦',
            color_continuous_scale='Greens',
            text='æº€è¶³åº¦',
            height=400
        )
        fig_strengths.update_traces(texttemplate='%{text:.2f}', textposition='outside')
        fig_strengths.update_layout(
            yaxis={'categoryorder':'total ascending'},
            xaxis=dict(range=[0, 5])
        )
        st.plotly_chart(fig_strengths, use_container_width=True)
        
        # å¼·ã¿è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
        st.markdown("#### ğŸ“ˆ å¼·ã¿è©³ç´°")
        for i, (category, score) in enumerate(top5_strengths, 1):
            st.markdown(f"**{i}ä½** {category}: **{score:.2f}ç‚¹**")
    
    with col2:
        st.markdown("### âš ï¸ **æ”¹å–„è¦æœ› TOP5**")
        
        # å¼±ã¿é ˜åŸŸã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆï¼ˆæ˜‡é †ã§è¡¨ç¤ºï¼‰
        weaknesses_df = pd.DataFrame(bottom5_weaknesses, columns=['ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'æº€è¶³åº¦'])
        weaknesses_df['æº€è¶³åº¦'] = weaknesses_df['æº€è¶³åº¦'].round(2)
        
        # å¼±ã¿é ˜åŸŸã®æ£’ã‚°ãƒ©ãƒ•
        fig_weaknesses = px.bar(
            weaknesses_df,
            x='æº€è¶³åº¦',
            y='ã‚«ãƒ†ã‚´ãƒªãƒ¼',
            orientation='h',
            title='æ”¹å–„è¦æœ› TOP5',
            color='æº€è¶³åº¦',
            color_continuous_scale='Reds',
            text='æº€è¶³åº¦',
            height=400
        )
        fig_weaknesses.update_traces(texttemplate='%{text:.2f}', textposition='outside')
        fig_weaknesses.update_layout(
            yaxis={'categoryorder':'total ascending'},
            xaxis=dict(range=[0, 5])
        )
        st.plotly_chart(fig_weaknesses, use_container_width=True)
        
        # å¼±ã¿è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
        st.markdown("#### ğŸ“‰ æ”¹å–„è¦æœ›è©³ç´°")
        for i, (category, score) in enumerate(reversed(bottom5_weaknesses), 1):
            st.markdown(f"**{i}ä½** {category}: **{score:.2f}ç‚¹**")
    
    # å…¨ä½“ã®æº€è¶³åº¦åˆ†å¸ƒ
    st.markdown("---")
    st.subheader("ğŸ“Š å…¨ã‚«ãƒ†ã‚´ãƒªãƒ¼æº€è¶³åº¦åˆ†å¸ƒ")
    
    # å…¨ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    all_categories_df = pd.DataFrame(sorted_satisfaction, columns=['ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'æº€è¶³åº¦'])
    all_categories_df['æº€è¶³åº¦'] = all_categories_df['æº€è¶³åº¦'].round(2)
    all_categories_df['é †ä½'] = range(1, len(all_categories_df) + 1)
    
    # è‰²åˆ†ã‘ç”¨ã®åˆ—ã‚’è¿½åŠ 
    all_categories_df['é ˜åŸŸ'] = all_categories_df.apply(
        lambda row: 'å¼·ã¿' if row['é †ä½'] <= 5 else ('æ”¹å–„è¦æœ›' if row['é †ä½'] > len(all_categories_df) - 5 else 'æ¨™æº–'), 
        axis=1
    )
    
    # å…¨ä½“ã®æ£’ã‚°ãƒ©ãƒ•
    fig_all = px.bar(
        all_categories_df,
        x='æº€è¶³åº¦',
        y='ã‚«ãƒ†ã‚´ãƒªãƒ¼',
        orientation='h',
        title='å…¨ã‚«ãƒ†ã‚´ãƒªãƒ¼æº€è¶³åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°',
        color='é ˜åŸŸ',
        color_discrete_map={
            'å¼·ã¿': '#2E8B57',
            'æ¨™æº–': '#4682B4', 
            'æ”¹å–„è¦æœ›': '#DC143C'
        },
        text='æº€è¶³åº¦',
        height=600
    )
    fig_all.update_traces(texttemplate='%{text:.2f}', textposition='outside')
    fig_all.update_layout(
        yaxis={'categoryorder':'total ascending'},
        xaxis=dict(range=[0, 5])
    )
    st.plotly_chart(fig_all, use_container_width=True)
    
    # æº€è¶³åº¦ã‚µãƒãƒªãƒ¼
    avg_satisfaction = sum(satisfaction_data.values()) / len(satisfaction_data)
    st.markdown("---")
    st.markdown("### ğŸ“‹ æº€è¶³åº¦ã‚µãƒãƒªãƒ¼")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("å¹³å‡æº€è¶³åº¦", f"{avg_satisfaction:.2f}ç‚¹", delta=f"{avg_satisfaction - 3:.2f}")
    with col2:
        top_score = top5_strengths[0][1]
        st.metric("æœ€é«˜æº€è¶³åº¦", f"{top_score:.2f}ç‚¹", delta=f"{top_score - avg_satisfaction:.2f}")
    with col3:
        bottom_score = bottom5_weaknesses[0][1]
        st.metric("æœ€ä½æº€è¶³åº¦", f"{bottom_score:.2f}ç‚¹", delta=f"{bottom_score - avg_satisfaction:.2f}")

def apply_filters(data, filters):
    """ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é©ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’çµã‚Šè¾¼ã‚€"""
    if not data or 'employee_data' not in data:
        return data
    
    df = data['employee_data'].copy()
    
    # å„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é©ç”¨
    if filters['group'] and 'group' in df.columns:
        df = df[df['group'].isin(filters['group'])]
    
    if filters['workplace'] and 'workplace' in df.columns:
        df = df[df['workplace'].isin(filters['workplace'])]
    
    if filters['employee_number'] and 'employee_number' in df.columns:
        df = df[df['employee_number'].isin(filters['employee_number'])]
    
    if filters['business_type'] and 'business_type' in df.columns:
        df = df[df['business_type'].isin(filters['business_type'])]
    
    if filters['region'] and 'region' in df.columns:
        df = df[df['region'].isin(filters['region'])]
    
    if filters['job_category'] and 'job_category' in df.columns:
        df = df[df['job_category'].isin(filters['job_category'])]
    
    if filters['age_group'] and 'age_group' in df.columns:
        df = df[df['age_group'].isin(filters['age_group'])]
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
    filtered_data = data.copy()
    filtered_data['employee_data'] = df
    
    return filtered_data

def main():
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.markdown("""
        <div class="sidebar-header">
            <div class="sidebar-logo">
                <span style='color: #667eea; font-size: 24px;'>ğŸ‘¥</span>
            </div>
            <div style='color: white; font-weight: bold; font-size: 18px;'>å¾“æ¥­å“¡èª¿æŸ»åˆ†æ</div>
        </div>
        """, unsafe_allow_html=True)
        
        st.divider()
        
        # ãƒšãƒ¼ã‚¸é¸æŠ
        page = st.radio(
            "ğŸ“‹ åˆ†æãƒšãƒ¼ã‚¸é¸æŠ",
            ["ğŸ“Š KPIæ¦‚è¦", "ğŸ“ˆ æº€è¶³åº¦åˆ†æ", "ğŸ¢ è©³ç´°åˆ†æ", "ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°", "â° æ™‚ç³»åˆ—åˆ†æ", "ğŸ”¬ é‡å›å¸°åˆ†æ", "ğŸ¤– AI ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ"],
            index=0
        )
        
        st.divider()
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®š
        st.markdown("### ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é¸æŠè‚¢ã‚’ä½œæˆ
        temp_data = load_employee_data()
        
        filters = {}
        
        if temp_data and 'employee_data' in temp_data:
            df = temp_data['employee_data']
            
            # ã‚°ãƒ«ãƒ¼ãƒ—ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if 'group' in df.columns:
                filters['group'] = st.multiselect(
                    "ã‚°ãƒ«ãƒ¼ãƒ—",
                    options=sorted(df['group'].dropna().unique()),
                    default=None,
                    key="filter_group"
                )
            else:
                filters['group'] = []
            
            # å‹¤å‹™åœ°ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if 'workplace' in df.columns:
                filters['workplace'] = st.multiselect(
                    "å‹¤å‹™åœ°",
                    options=sorted(df['workplace'].dropna().unique()),
                    default=None,
                    key="filter_workplace"
                )
            else:
                filters['workplace'] = []
            
            # äººæ•°ç•ªå·ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if 'employee_number' in df.columns:
                filters['employee_number'] = st.multiselect(
                    "äººæ•°ç•ªå·",
                    options=sorted(df['employee_number'].dropna().unique()),
                    default=None,
                    key="filter_employee_number"
                )
            else:
                filters['employee_number'] = []
            
            # å‹¤å‹™æ¥­å‹™ç­‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if 'business_type' in df.columns:
                filters['business_type'] = st.multiselect(
                    "å‹¤å‹™æ¥­å‹™ç­‰",
                    options=sorted(df['business_type'].dropna().unique()),
                    default=None,
                    key="filter_business_type"
                )
            else:
                filters['business_type'] = []
            
            # åœ°åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if 'region' in df.columns:
                filters['region'] = st.multiselect(
                    "åœ°åŸŸ",
                    options=sorted(df['region'].dropna().unique()),
                    default=None,
                    key="filter_region"
                )
            else:
                filters['region'] = []
            
            # è·å‹™ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if 'job_category' in df.columns:
                filters['job_category'] = st.multiselect(
                    "è·å‹™",
                    options=sorted(df['job_category'].dropna().unique()),
                    default=None,
                    key="filter_job_category"
                )
            else:
                filters['job_category'] = []
            
            # å¹´é½¢ç­‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if 'age_group' in df.columns:
                filters['age_group'] = st.multiselect(
                    "å¹´é½¢ç­‰",
                    options=sorted(df['age_group'].dropna().unique()),
                    default=None,
                    key="filter_age_group"
                )
            else:
                filters['age_group'] = []
        else:
            filters = {
                'group': [],
                'workplace': [],
                'employee_number': [],
                'business_type': [],
                'region': [],
                'job_category': [],
                'age_group': []
            }
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
        if st.button("ğŸ”„ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒªã‚»ãƒƒãƒˆ", use_container_width=True):
            st.cache_data.clear()
            st.rerun()
        
        st.divider()
        
        # ãƒ‡ãƒ¼ã‚¿æ›´æ–°ãƒœã‚¿ãƒ³
        if st.button("ğŸ”„ ãƒ‡ãƒ¼ã‚¿æ›´æ–°", use_container_width=True):
            st.cache_data.clear()
            st.rerun()
        
        st.info("ğŸ’¡ Excelãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°å¾Œã¯ã€Œãƒ‡ãƒ¼ã‚¿æ›´æ–°ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„")
        
        st.divider()
        
        # ãƒ¬ãƒãƒ¼ãƒˆæƒ…å ±
        st.markdown("### ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆæƒ…å ±")
        
        st.write(f"ğŸ• **æœ€çµ‚æ›´æ–°:** {datetime.now().strftime('%Y/%m/%d')}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        st.subheader("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        if st.button("ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›", use_container_width=True):
            st.success("å®Ÿè£…æ™‚ã«ã¯åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’PDF/Excelã§å‡ºåŠ›ã§ãã¾ã™")
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    st.title("ğŸ‘¥ å¾“æ¥­å“¡èª¿æŸ»å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    st.markdown("---")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with st.spinner("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        data = load_employee_data()
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é©ç”¨
        filtered_data = apply_filters(data, filters)
        kpis = calculate_kpis(filtered_data)
    
    # ãƒšãƒ¼ã‚¸è¡¨ç¤º
    if page == "ğŸ“Š KPIæ¦‚è¦":
        show_kpi_overview(filtered_data, kpis)
    elif page == "ğŸ“ˆ æº€è¶³åº¦åˆ†æ":
        show_satisfaction_analysis(filtered_data, kpis)
    elif page == "ğŸ¢ è©³ç´°åˆ†æ":
        show_department_analysis(filtered_data, kpis)
    elif page == "ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°":
        show_text_mining_analysis()
    elif page == "â° æ™‚ç³»åˆ—åˆ†æ":
        show_time_series_analysis()
    elif page == "ğŸ”¬ é‡å›å¸°åˆ†æ":
        show_regression_analysis(filtered_data, kpis)
    
    elif page == "ğŸ¤– AI ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ":
        # æ–°ã—ã„AIãƒ†ã‚­ã‚¹ãƒˆåˆ†ææ©Ÿèƒ½ã‚’è¡¨ç¤º
        from text_analysis_ml import show_text_analysis_ml_page
        show_text_analysis_ml_page()

def show_regression_analysis(data, kpis):
    """é‡å›å¸°åˆ†æã‚’è¡¨ç¤º"""
    st.header("ğŸ”¬ é‡å›å¸°åˆ†æ")
    st.markdown("ä¸»è¦æŒ‡æ¨™ã«å¯¾ã™ã‚‹æº€è¶³åº¦é …ç›®ã®å½±éŸ¿åŠ›ã‚’åˆ†æã—ã¾ã™")
    
    if not data or 'employee_data' not in data:
        st.error("åˆ†æã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        return
    
    df = data['employee_data']
    
    # ç›®çš„å¤‰æ•°ã®é¸æŠï¼ˆåˆ©ç”¨å¯èƒ½ãªåˆ—åã‚’è‡ªå‹•æ¤œå‡ºï¼‰
    target_options = {}
    
    # eNPSé–¢é€£ã®åˆ—åã‚’æ¤œç´¢
    enps_candidates = ['nps_score', 'recommend_score']
    for candidate in enps_candidates:
        if candidate in df.columns:
            target_options['eNPS (æ¨å¥¨åº¦)'] = candidate
            break
    
    # å®Ÿéš›ã®Excelãƒ‡ãƒ¼ã‚¿ã®åˆ—åã‚‚æ¤œç´¢
    for col in df.columns:
        col_str = str(col)
        if any(keyword in col_str for keyword in ['æ¨å¥¨', 'è¦ªã—ã„å‹äºº', 'å®¶æ—', 'è»¢è·', 'å°±è·', 'ç·åˆè©•ä¾¡']):
            target_options['eNPS (æ¨å¥¨åº¦)'] = col
            break
    
    # ãã®ä»–ã®ç›®çš„å¤‰æ•°ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
    if 'overall_satisfaction' in df.columns:
        target_options['ç·åˆæº€è¶³åº¦'] = 'overall_satisfaction'
    else:
        # å®Ÿéš›ã®Excelãƒ‡ãƒ¼ã‚¿ã®ç·åˆæº€è¶³åº¦åˆ—ã‚’æ¤œç´¢
        for col in df.columns:
            if 'ç·åˆæº€è¶³åº¦' in str(col):
                target_options['ç·åˆæº€è¶³åº¦'] = col
                break
    
    if 'long_term_intention' in df.columns:
        target_options['å‹¤ç¶šæ„å‘'] = 'long_term_intention'
    else:
        # å®Ÿéš›ã®Excelãƒ‡ãƒ¼ã‚¿ã®å‹¤ç¶šæ„å‘åˆ—ã‚’æ¤œç´¢
        for col in df.columns:
            if any(keyword in str(col) for keyword in ['é•·ãåƒããŸã„', 'å‹¤ç¶š', 'æ„å‘']):
                target_options['å‹¤ç¶šæ„å‘'] = col
                break
                
    if 'contribution_score' in df.columns:
        target_options['æ´»èºè²¢çŒ®åº¦'] = 'contribution_score'
    else:
        # å®Ÿéš›ã®Excelãƒ‡ãƒ¼ã‚¿ã®æ´»èºè²¢çŒ®åº¦åˆ—ã‚’æ¤œç´¢
        for col in df.columns:
            if any(keyword in str(col) for keyword in ['æ´»èºè²¢çŒ®', 'è²¢çŒ®ã§ãã¦ã„ã‚‹']):
                target_options['æ´»èºè²¢çŒ®åº¦'] = col
                break
    
    if not target_options:
        st.error("åˆ†æå¯èƒ½ãªç›®çš„å¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ã®åˆ—åã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.info("åˆ©ç”¨å¯èƒ½ãªåˆ—å:")
        st.write(list(df.columns))
        return
    
    selected_target = st.selectbox(
        "ğŸ¯ åˆ†æå¯¾è±¡ï¼ˆç›®çš„å¤‰æ•°ï¼‰ã‚’é¸æŠã—ã¦ãã ã•ã„",
        list(target_options.keys())
    )
    
    target_col = target_options[selected_target]
    
    # æº€è¶³åº¦é …ç›®ï¼ˆèª¬æ˜å¤‰æ•°ï¼‰ã‚’å®šç¾©
    satisfaction_categories = [
        'å‹¤å‹™æ™‚é–“', 'ä¼‘æ—¥ä¼‘æš‡', 'æœ‰çµ¦ä¼‘æš‡', 'å‹¤å‹™ä½“ç³»', 'æ˜‡çµ¦æ˜‡æ ¼', 'äººé–“é–¢ä¿‚',
        'åƒãç’°å¢ƒ', 'æˆé•·å®Ÿæ„Ÿ', 'å°†æ¥ã‚­ãƒ£ãƒªã‚¢', 'ç¦åˆ©åšç”Ÿ', 'è©•ä¾¡åˆ¶åº¦'
    ]
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šåˆ©ç”¨å¯èƒ½ãªåˆ—åã‚’è¡¨ç¤º
    with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆåˆ©ç”¨å¯èƒ½ãªåˆ—åï¼‰"):
        st.write("**å…¨ã¦ã®åˆ—å:**")
        st.write(list(df.columns))
        st.write(f"**é¸æŠã•ã‚ŒãŸç›®çš„å¤‰æ•°:** {target_col}")
        st.write("**ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶:**", df.shape)
    
    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    try:
        # ç›®çš„å¤‰æ•°ã®ç¢ºèª
        if target_col not in df.columns:
            st.error(f"ç›®çš„å¤‰æ•° '{target_col}' ãŒãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return
            
        # ç›®çš„å¤‰æ•°ã®æ•°å€¤å¤‰æ›
        y = df[target_col].copy()
        
        # eNPSï¼ˆæ¨å¥¨åº¦ï¼‰ã®å ´åˆã®ç‰¹åˆ¥å‡¦ç†
        if 'eNPS' in selected_target:
            # "7 (Passive)" ã®ã‚ˆã†ãªæ–‡å­—åˆ—ã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º
            y = y.astype(str).str.extract(r'(\d+)', expand=False).astype(float)
        else:
            # ãã®ä»–ã®å ´åˆã¯æ•°å€¤å¤‰æ›ã‚’è©¦è¡Œ
            try:
                y = pd.to_numeric(y, errors='coerce')
            except:
                st.error(f"ç›®çš„å¤‰æ•° '{target_col}' ã‚’æ•°å€¤ã«å¤‰æ›ã§ãã¾ã›ã‚“")
                return
        
        # èª¬æ˜å¤‰æ•°ã®æº–å‚™ï¼ˆæº€è¶³åº¦é …ç›®ï¼‰
        X_data = []
        available_features = []
        
        # æº€è¶³åº¦ç³»ã®åˆ—åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        satisfaction_patterns = ['æº€è¶³åº¦', 'è©•ä¾¡', 'åº¦åˆã„']
        
        for category in satisfaction_categories:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ã‚«ãƒ†ã‚´ãƒª_æº€è¶³åº¦
            sat_col = f"{category}_æº€è¶³åº¦"
            if sat_col in df.columns:
                X_data.append(df[sat_col])
                available_features.append(sat_col)
                continue
                
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚«ãƒ†ã‚´ãƒªåãã®ã‚‚ã®
            if category in df.columns:
                X_data.append(df[category])
                available_features.append(category)
                continue
        
        # è¿½åŠ ã§æº€è¶³åº¦ç³»ã®åˆ—ã‚’è‡ªå‹•æ¤œå‡º
        for col in df.columns:
            if any(pattern in col for pattern in satisfaction_patterns) and col not in available_features:
                # ç›®çš„å¤‰æ•°ã¨åŒã˜åˆ—ã¯é™¤å¤–
                if col != target_col:
                    try:
                        # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã‚’è©¦è¡Œ
                        numeric_data = pd.to_numeric(df[col], errors='coerce')
                        # æ¬ æå€¤ãŒ80%æœªæº€ã®å ´åˆã®ã¿æ¡ç”¨
                        if numeric_data.notna().sum() / len(numeric_data) > 0.2:
                            X_data.append(numeric_data)
                            available_features.append(col)
                    except:
                        pass
        
        if not X_data:
            st.error("èª¬æ˜å¤‰æ•°ã¨ãªã‚‹æº€è¶³åº¦ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            st.info("åˆ©ç”¨å¯èƒ½ãªåˆ—åã‹ã‚‰æº€è¶³åº¦ç³»ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã—ã¾ã—ãŸãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
            
        X = pd.DataFrame(X_data).T
        X.columns = available_features
        
        # é¸æŠã•ã‚ŒãŸèª¬æ˜å¤‰æ•°ã‚’è¡¨ç¤º
        st.info(f"ğŸ“‹ **æ¤œå‡ºã•ã‚ŒãŸèª¬æ˜å¤‰æ•°:** {len(available_features)}å€‹")
        with st.expander("ä½¿ç”¨ã™ã‚‹èª¬æ˜å¤‰æ•°ä¸€è¦§"):
            for i, feature in enumerate(available_features, 1):
                st.write(f"{i}. {feature}")
        
        # æ¬ æå€¤ã®å‡¦ç†
        X = X.fillna(X.mean())
        y = y.fillna(y.mean())
        
        # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨
        valid_idx = ~(X.isna().any(axis=1) | y.isna())
        X = X[valid_idx]
        y = y[valid_idx]
        
        if len(X) < 10:
            st.warning("åˆ†æã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆæœ€ä½10ä»¶å¿…è¦ï¼‰")
            return
            
        st.success(f"âœ… åˆ†æãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(X)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
        
        # ã‚¿ãƒ–ã§çµæœã‚’æ•´ç†
        tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“Š åˆ†æçµæœ", "ğŸ” å¤šé‡å…±ç·šæ€§è¨ºæ–­", "ğŸ“ˆ ä¿‚æ•°å¯è¦–åŒ–", "ğŸ¯ äºˆæ¸¬ç²¾åº¦", "ğŸ“ˆ è©³ç´°çµ±è¨ˆ"])
        
        with tab1:
            st.subheader("ğŸ“Š é‡å›å¸°åˆ†æçµæœ")
            
            # æ¨™æº–åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)
            
            # é‡å›å¸°åˆ†æå®Ÿè¡Œ
            model = LinearRegression()
            model.fit(X_scaled_df, y)
            
            # äºˆæ¸¬å€¤è¨ˆç®—
            y_pred = model.predict(X_scaled_df)
            
            # æ±ºå®šä¿‚æ•°ã¨RMSE
            r2 = r2_score(y, y_pred)
            rmse = np.sqrt(mean_squared_error(y, y_pred))
            
            # çµæœè¡¨ç¤º
            col1, col2 = st.columns(2)
            with col1:
                st.metric("æ±ºå®šä¿‚æ•° (RÂ²)", f"{r2:.3f}")
            with col2:
                st.metric("RMSE", f"{rmse:.3f}")
            
            # çµ±è¨ˆçš„æ¤œå®šã®ãŸã‚ã®OLSå›å¸°ã‚’å®Ÿè¡Œ
            if STATSMODELS_AVAILABLE:
                X_with_const = sm.add_constant(X_scaled_df)
                ols_model = sm.OLS(y, X_with_const).fit()
            else:
                X_with_const = X_scaled_df
                ols_model = None
            
            # å›å¸°ä¿‚æ•°ã®è¡¨ç¤ºï¼ˆçµ±è¨ˆçš„æœ‰æ„æ€§ã‚’å«ã‚€ï¼‰
            coef_data = []
            for i, var in enumerate(X.columns):
                coef = model.coef_[i]
                if STATSMODELS_AVAILABLE and ols_model is not None:
                    p_value = ols_model.pvalues[var] if var in ols_model.pvalues.index else None
                    conf_int = ols_model.conf_int().loc[var] if var in ols_model.conf_int().index else [None, None]
                else:
                    p_value = None
                    conf_int = [None, None]
                
                significance = ""
                if p_value is not None:
                    if p_value < 0.001:
                        significance = "***"
                    elif p_value < 0.01:
                        significance = "**"
                    elif p_value < 0.05:
                        significance = "*"
                    elif p_value < 0.1:
                        significance = "â€ "
                
                coef_data.append({
                    'èª¬æ˜å¤‰æ•°': var,
                    'æ¨™æº–åŒ–ä¿‚æ•°': coef,
                    'på€¤': p_value,
                    'æœ‰æ„æ€§': significance,
                    '95%ä¿¡é ¼åŒºé–“ä¸‹é™': conf_int[0],
                    '95%ä¿¡é ¼åŒºé–“ä¸Šé™': conf_int[1],
                    'çµ¶å¯¾å€¤': np.abs(coef)
                })
            
            coef_df = pd.DataFrame(coef_data).sort_values('çµ¶å¯¾å€¤', ascending=False)
            
            st.subheader("ğŸ“‹ å›å¸°ä¿‚æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆçµ±è¨ˆçš„æœ‰æ„æ€§ä»˜ãï¼‰")
            
            # æœ‰æ„æ€§ã®èª¬æ˜
            st.markdown("""
            **æœ‰æ„æ€§ãƒ¬ãƒ™ãƒ«:** *** p<0.001, ** p<0.01, * p<0.05, â€  p<0.1
            """)
            
            # è¡¨ç¤ºç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
            display_coef_df = coef_df[['èª¬æ˜å¤‰æ•°', 'æ¨™æº–åŒ–ä¿‚æ•°', 'på€¤', 'æœ‰æ„æ€§', '95%ä¿¡é ¼åŒºé–“ä¸‹é™', '95%ä¿¡é ¼åŒºé–“ä¸Šé™']].copy()
            display_coef_df['æ¨™æº–åŒ–ä¿‚æ•°'] = display_coef_df['æ¨™æº–åŒ–ä¿‚æ•°'].round(4)
            display_coef_df['på€¤'] = display_coef_df['på€¤'].round(4)
            display_coef_df['95%ä¿¡é ¼åŒºé–“ä¸‹é™'] = display_coef_df['95%ä¿¡é ¼åŒºé–“ä¸‹é™'].round(4)
            display_coef_df['95%ä¿¡é ¼åŒºé–“ä¸Šé™'] = display_coef_df['95%ä¿¡é ¼åŒºé–“ä¸Šé™'].round(4)
            
            st.dataframe(
                display_coef_df,
                use_container_width=True,
                hide_index=True
            )
            
        with tab2:
            st.subheader("ğŸ” å¤šé‡å…±ç·šæ€§è¨ºæ–­")
            
            # VIFè¨ˆç®—
            def calculate_vif(X_df):
                if not STATSMODELS_AVAILABLE:
                    return pd.DataFrame({"èª¬æ˜å¤‰æ•°": X_df.columns, "VIF": ["åˆ©ç”¨ä¸å¯"] * len(X_df.columns)})
                vif_data = pd.DataFrame()
                vif_data["èª¬æ˜å¤‰æ•°"] = X_df.columns
                vif_data["VIF"] = [variance_inflation_factor(X_df.values, i) 
                                 for i in range(len(X_df.columns))]
                return vif_data.sort_values('VIF', ascending=False)
            
            try:
                # å®šæ•°é …ã‚’è¿½åŠ ã—ã¦VIFè¨ˆç®—
                if STATSMODELS_AVAILABLE:
                    X_with_const = sm.add_constant(X_scaled_df)
                else:
                    X_with_const = X_scaled_df
                vif_df = calculate_vif(X_scaled_df)
                
                st.dataframe(vif_df.round(2), use_container_width=True, hide_index=True)
                
                # VIFè§£é‡ˆã®èª¬æ˜
                high_vif = vif_df[vif_df['VIF'] > 5]
                if len(high_vif) > 0:
                    st.warning("""
                    âš ï¸ **å¤šé‡å…±ç·šæ€§ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™**
                    - VIF > 5: å¤šé‡å…±ç·šæ€§ã®ç–‘ã„ã‚ã‚Š
                    - VIF > 10: æ·±åˆ»ãªå¤šé‡å…±ç·šæ€§
                    """)
                    
                    # PCAå®Ÿè¡Œã®ææ¡ˆ
                    if st.button("ğŸ”„ ä¸»æˆåˆ†åˆ†æã§æ¬¡å…ƒåœ§ç¸®ã‚’å®Ÿè¡Œ"):
                        st.info("ä¸»æˆåˆ†åˆ†æã‚’å®Ÿè¡Œã—ã¦å¤šé‡å…±ç·šæ€§ã‚’è§£æ±ºã—ã¾ã™...")
                        
                        # PCAå®Ÿè¡Œ
                        pca = PCA()
                        X_pca = pca.fit_transform(X_scaled_df)
                        
                        # ç´¯ç©å¯„ä¸ç‡95%ã¾ã§ã®æˆåˆ†ã‚’é¸æŠ
                        cumsum_ratio = np.cumsum(pca.explained_variance_ratio_)
                        n_components = np.argmax(cumsum_ratio >= 0.95) + 1
                        
                        st.write(f"ğŸ“Š 95%ã®åˆ†æ•£ã‚’èª¬æ˜ã™ã‚‹ã®ã«å¿…è¦ãªä¸»æˆåˆ†æ•°: {n_components}")
                        
                        # ä¸»æˆåˆ†ã§ã®åˆ†æ
                        pca_selected = PCA(n_components=n_components)
                        X_pca_selected = pca_selected.fit_transform(X_scaled_df)
                        
                        model_pca = LinearRegression()
                        model_pca.fit(X_pca_selected, y)
                        y_pred_pca = model_pca.predict(X_pca_selected)
                        
                        r2_pca = r2_score(y, y_pred_pca)
                        rmse_pca = np.sqrt(mean_squared_error(y, y_pred_pca))
                        
                        st.success(f"ğŸ¯ PCAå¾Œã®ç²¾åº¦: RÂ² = {r2_pca:.3f}, RMSE = {rmse_pca:.3f}")
                        
                        # ä¸»æˆåˆ†ã®è§£é‡ˆ
                        components_df = pd.DataFrame(
                            pca_selected.components_.T,
                            columns=[f'PC{i+1}' for i in range(n_components)],
                            index=X.columns
                        )
                        
                        st.subheader("ğŸ“‹ ä¸»æˆåˆ†ã®æ§‹æˆ")
                        st.dataframe(components_df.round(3), use_container_width=True)
                        
                else:
                    st.success("âœ… å¤šé‡å…±ç·šæ€§ã®å•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                    
            except Exception as e:
                st.error(f"VIFè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                
        with tab3:
            st.subheader("ğŸ“ˆ å›å¸°ä¿‚æ•°ã®å¯è¦–åŒ–")
            
            # çµ±è¨ˆçš„æœ‰æ„æ€§ã‚’è‰²ã§è¡¨ç¾
            coef_df['æœ‰æ„æ€§_æ•°å€¤'] = coef_df['på€¤'].apply(lambda x: 
                0 if pd.isna(x) else 
                4 if x < 0.001 else 
                3 if x < 0.01 else 
                2 if x < 0.05 else 
                1 if x < 0.1 else 0
            )
            
            # ä¿‚æ•°ã®æ£’ã‚°ãƒ©ãƒ•ï¼ˆæœ‰æ„æ€§ã§è‰²åˆ†ã‘ï¼‰
            fig = px.bar(
                coef_df,
                x='æ¨™æº–åŒ–ä¿‚æ•°',
                y='èª¬æ˜å¤‰æ•°',
                orientation='h',
                title=f'{selected_target}ã«å¯¾ã™ã‚‹å„è¦å› ã®å½±éŸ¿åŠ›ï¼ˆæœ‰æ„æ€§ã§è‰²åˆ†ã‘ï¼‰',
                color='æœ‰æ„æ€§_æ•°å€¤',
                color_continuous_scale=['lightgray', 'lightblue', 'yellow', 'orange', 'red'],
                hover_data=['på€¤', 'æœ‰æ„æ€§']
            )
            fig.update_layout(
                height=600,
                coloraxis_colorbar=dict(
                    title="æœ‰æ„æ€§ãƒ¬ãƒ™ãƒ«",
                    tickvals=[0, 1, 2, 3, 4],
                    ticktext=["n.s.", "p<0.1", "p<0.05", "p<0.01", "p<0.001"]
                )
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # ä¿¡é ¼åŒºé–“ã®ãƒ—ãƒ­ãƒƒãƒˆ
            st.subheader("ğŸ“Š å›å¸°ä¿‚æ•°ã®ä¿¡é ¼åŒºé–“")
            
            # ä¿¡é ¼åŒºé–“ã®ãƒ—ãƒ­ãƒƒãƒˆç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
            plot_data = coef_df.copy()
            plot_data = plot_data.sort_values('æ¨™æº–åŒ–ä¿‚æ•°')
            
            fig_ci = go.Figure()
            
            # ç‚¹æ¨å®šå€¤
            fig_ci.add_trace(go.Scatter(
                x=plot_data['æ¨™æº–åŒ–ä¿‚æ•°'],
                y=plot_data['èª¬æ˜å¤‰æ•°'],
                mode='markers',
                marker=dict(size=10, color='blue'),
                name='ç‚¹æ¨å®šå€¤',
                text=plot_data['æœ‰æ„æ€§'],
                textposition="middle right"
            ))
            
            # ä¿¡é ¼åŒºé–“
            for idx, row in plot_data.iterrows():
                if not pd.isna(row['95%ä¿¡é ¼åŒºé–“ä¸‹é™']) and not pd.isna(row['95%ä¿¡é ¼åŒºé–“ä¸Šé™']):
                    fig_ci.add_trace(go.Scatter(
                        x=[row['95%ä¿¡é ¼åŒºé–“ä¸‹é™'], row['95%ä¿¡é ¼åŒºé–“ä¸Šé™']],
                        y=[row['èª¬æ˜å¤‰æ•°'], row['èª¬æ˜å¤‰æ•°']],
                        mode='lines',
                        line=dict(color='gray', width=2),
                        showlegend=False,
                        hoverinfo='skip'
                    ))
            
            # ã‚¼ãƒ­ãƒ©ã‚¤ãƒ³
            fig_ci.add_vline(x=0, line_dash="dash", line_color="red", annotation_text="ã‚¼ãƒ­ãƒ©ã‚¤ãƒ³")
            
            fig_ci.update_layout(
                title="å›å¸°ä¿‚æ•°ã®95%ä¿¡é ¼åŒºé–“",
                xaxis_title="æ¨™æº–åŒ–å›å¸°ä¿‚æ•°",
                yaxis_title="èª¬æ˜å¤‰æ•°",
                height=600
            )
            
            st.plotly_chart(fig_ci, use_container_width=True)
            
            # å½±éŸ¿åŠ›ã®è§£é‡ˆ
            st.subheader("ğŸ“ çµæœã®è§£é‡ˆ")
            top_positive = coef_df[coef_df['æ¨™æº–åŒ–ä¿‚æ•°'] > 0].head(3)
            top_negative = coef_df[coef_df['æ¨™æº–åŒ–ä¿‚æ•°'] < 0].head(3)
            
            if len(top_positive) > 0:
                st.write("**ğŸ”º æ­£ã®å½±éŸ¿ï¼ˆå‘ä¸Šè¦å› ï¼‰:**")
                for _, row in top_positive.iterrows():
                    st.write(f"- {row['èª¬æ˜å¤‰æ•°']}: {row['æ¨™æº–åŒ–ä¿‚æ•°']:.3f}")
                    
            if len(top_negative) > 0:
                st.write("**ğŸ”» è² ã®å½±éŸ¿ï¼ˆé˜»å®³è¦å› ï¼‰:**")
                for _, row in top_negative.iterrows():
                    st.write(f"- {row['èª¬æ˜å¤‰æ•°']}: {row['æ¨™æº–åŒ–ä¿‚æ•°']:.3f}")
                    
        with tab4:
            st.subheader("ğŸ¯ äºˆæ¸¬ç²¾åº¦ã®è©•ä¾¡")
            
            # å®Ÿæ¸¬å€¤ vs äºˆæ¸¬å€¤ã®ãƒ—ãƒ­ãƒƒãƒˆ
            fig = px.scatter(
                x=y, 
                y=y_pred,
                title='å®Ÿæ¸¬å€¤ vs äºˆæ¸¬å€¤',
                labels={'x': f'å®Ÿæ¸¬å€¤ ({selected_target})', 'y': f'äºˆæ¸¬å€¤ ({selected_target})'}
            )
            
            # å®Œå…¨äºˆæ¸¬ç·šã‚’è¿½åŠ 
            min_val, max_val = min(y.min(), y_pred.min()), max(y.max(), y_pred.max())
            fig.add_shape(
                type="line",
                x0=min_val, y0=min_val,
                x1=max_val, y1=max_val,
                line=dict(color="red", dash="dash")
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
            residuals = y - y_pred
            fig_residual = px.scatter(
                x=y_pred,
                y=residuals,
                title='æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ',
                labels={'x': 'äºˆæ¸¬å€¤', 'y': 'æ®‹å·®'}
            )
            fig_residual.add_hline(y=0, line_dash="dash", line_color="red")
            st.plotly_chart(fig_residual, use_container_width=True)
            
        with tab5:
            st.subheader("ğŸ“ˆ è©³ç´°çµ±è¨ˆæƒ…å ±")
            
            # ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®çµ±è¨ˆæƒ…å ±
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if STATSMODELS_AVAILABLE and ols_model is not None:
                    st.metric("èª¿æ•´æ¸ˆã¿RÂ²", f"{ols_model.rsquared_adj:.4f}")
                    st.metric("AIC", f"{ols_model.aic:.2f}")
                else:
                    st.metric("èª¿æ•´æ¸ˆã¿RÂ²", "åˆ©ç”¨ä¸å¯")
                    st.metric("AIC", "åˆ©ç”¨ä¸å¯")
                
            with col2:
                if STATSMODELS_AVAILABLE and ols_model is not None:
                    st.metric("BIC", f"{ols_model.bic:.2f}")
                    st.metric("Fçµ±è¨ˆé‡", f"{ols_model.fvalue:.2f}")
                else:
                    st.metric("BIC", "åˆ©ç”¨ä¸å¯")
                    st.metric("Fçµ±è¨ˆé‡", "åˆ©ç”¨ä¸å¯")
                
            with col3:
                if STATSMODELS_AVAILABLE and ols_model is not None:
                    st.metric("Fæ¤œå®špå€¤", f"{ols_model.f_pvalue:.4f}")
                    st.metric("å°¤åº¦æ¯”", f"{ols_model.llf:.2f}")
                else:
                    st.metric("Fæ¤œå®špå€¤", "åˆ©ç”¨ä¸å¯")
                    st.metric("å°¤åº¦æ¯”", "åˆ©ç”¨ä¸å¯")
            
            # å›å¸°åˆ†æã®å‰ææ¡ä»¶ã®æ¤œè¨¼
            st.subheader("ğŸ” å›å¸°åˆ†æã®å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯")
            
            # æ­£è¦æ€§æ¤œå®šï¼ˆShapiro-Wilkæ¤œå®šï¼‰
            shapiro_stat, shapiro_p = stats.shapiro(residuals)
            
            # Durbin-Watsonçµ±è¨ˆé‡ï¼ˆè‡ªå·±ç›¸é–¢ã®æ¤œå®šï¼‰
            if STATSMODELS_AVAILABLE:
                dw_stat = durbin_watson(residuals)
                # Breusch-Paganæ¤œå®šï¼ˆç­‰åˆ†æ•£æ€§ã®æ¤œå®šï¼‰
                bp_stat, bp_p, bp_f_stat, bp_f_p = het_breuschpagan(residuals, X_with_const)
            else:
                dw_stat = None
                bp_stat, bp_p, bp_f_stat, bp_f_p = None, None, None, None
            
            # å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯çµæœ
            st.markdown("### ğŸ“‹ è¨ºæ–­çµæœ")
            
            # æ­£è¦æ€§
            normality_status = "âœ… æ­£è¦æ€§OK" if shapiro_p > 0.05 else "âš ï¸ æ­£è¦æ€§ã«å•é¡Œã‚ã‚Š"
            st.write(f"**æ®‹å·®ã®æ­£è¦æ€§:** {normality_status} (Shapiro-Wilk p={shapiro_p:.4f})")
            
            # è‡ªå·±ç›¸é–¢
            if STATSMODELS_AVAILABLE and dw_stat is not None:
                autocorr_status = "âœ… è‡ªå·±ç›¸é–¢ãªã—" if 1.5 <= dw_stat <= 2.5 else "âš ï¸ è‡ªå·±ç›¸é–¢ã®å¯èƒ½æ€§"
                st.write(f"**è‡ªå·±ç›¸é–¢:** {autocorr_status} (Durbin-Watson={dw_stat:.3f})")
            else:
                st.write("**è‡ªå·±ç›¸é–¢:** âŒ åˆ©ç”¨ä¸å¯ (statsmodelsæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)")
            
            # ç­‰åˆ†æ•£æ€§
            if STATSMODELS_AVAILABLE and bp_p is not None:
                homoscedasticity_status = "âœ… ç­‰åˆ†æ•£æ€§OK" if bp_p > 0.05 else "âš ï¸ ä¸ç­‰åˆ†æ•£ã®å¯èƒ½æ€§"
                st.write(f"**ç­‰åˆ†æ•£æ€§:** {homoscedasticity_status} (Breusch-Pagan p={bp_p:.4f})")
            else:
                st.write("**ç­‰åˆ†æ•£æ€§:** âŒ åˆ©ç”¨ä¸å¯ (statsmodelsæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)")
            
            # è©³ç´°ãªè¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆ
            st.subheader("ğŸ“Š è¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆ")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Q-Qãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ­£è¦æ€§ç¢ºèªï¼‰
                fig_qq = go.Figure()
                
                # Q-Qãƒ—ãƒ­ãƒƒãƒˆã®ãƒ‡ãƒ¼ã‚¿
                qq_data = probplot(residuals, dist="norm")
                theoretical_quantiles = qq_data[0][0]
                sample_quantiles = qq_data[0][1]
                
                # ãƒ—ãƒ­ãƒƒãƒˆ
                fig_qq.add_trace(go.Scatter(
                    x=theoretical_quantiles,
                    y=sample_quantiles,
                    mode='markers',
                    name='æ®‹å·®',
                    marker=dict(color='blue', size=6)
                ))
                
                # ç†è«–ç›´ç·š
                fig_qq.add_trace(go.Scatter(
                    x=theoretical_quantiles,
                    y=qq_data[1][0] * theoretical_quantiles + qq_data[1][1],
                    mode='lines',
                    name='ç†è«–ç›´ç·š',
                    line=dict(color='red', dash='dash')
                ))
                
                fig_qq.update_layout(
                    title="Q-Qãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ­£è¦æ€§ç¢ºèªï¼‰",
                    xaxis_title="ç†è«–åˆ†ä½æ•°",
                    yaxis_title="æ¨™æœ¬åˆ†ä½æ•°",
                    height=400
                )
                
                st.plotly_chart(fig_qq, use_container_width=True)
                
            with col2:
                # Cookè·é›¢ï¼ˆå¤–ã‚Œå€¤æ¤œå‡ºï¼‰
                if STATSMODELS_AVAILABLE and ols_model is not None:
                    influence = ols_model.get_influence()
                    cooks_d = influence.cooks_distance[0]
                else:
                    cooks_d = np.zeros(len(residuals))  # Fallback to zeros
                
                fig_cook = px.scatter(
                    x=range(len(cooks_d)),
                    y=cooks_d,
                    title="Cookè·é›¢ï¼ˆå¤–ã‚Œå€¤æ¤œå‡ºï¼‰",
                    labels={'x': 'ã‚µãƒ³ãƒ—ãƒ«ç•ªå·', 'y': 'Cookè·é›¢'}
                )
                
                # Cookè·é›¢ã®é–¾å€¤ç·š
                threshold = 4 / len(X)
                fig_cook.add_hline(y=threshold, line_dash="dash", line_color="red", 
                                 annotation_text=f"é–¾å€¤={threshold:.3f}")
                
                fig_cook.update_layout(height=400)
                st.plotly_chart(fig_cook, use_container_width=True)
            
            # å®Ÿç”¨çš„ãªè§£é‡ˆã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹
            st.subheader("ğŸ’¡ åˆ†æçµæœã®å®Ÿç”¨çš„è§£é‡ˆ")
            
            # çµ±è¨ˆçš„ã«æœ‰æ„ãªå¤‰æ•°ã®æŠ½å‡º
            significant_vars = coef_df[coef_df['på€¤'] < 0.05]
            
            if len(significant_vars) > 0:
                st.success(f"ğŸ“ˆ **çµ±è¨ˆçš„ã«æœ‰æ„ãªå½±éŸ¿è¦å› : {len(significant_vars)}å€‹**")
                
                for _, row in significant_vars.head(5).iterrows():
                    direction = "å‘ä¸Š" if row['æ¨™æº–åŒ–ä¿‚æ•°'] > 0 else "ä½ä¸‹"
                    impact_size = "å¤§" if abs(row['æ¨™æº–åŒ–ä¿‚æ•°']) > 0.3 else "ä¸­" if abs(row['æ¨™æº–åŒ–ä¿‚æ•°']) > 0.1 else "å°"
                    
                    st.write(f"- **{row['èª¬æ˜å¤‰æ•°']}**: {selected_target}ã‚’{direction}ã•ã›ã‚‹ï¼ˆå½±éŸ¿åº¦: {impact_size}, ä¿‚æ•°: {row['æ¨™æº–åŒ–ä¿‚æ•°']:.3f}ï¼‰")
                
                # æ”¹å–„ææ¡ˆ
                st.subheader("ğŸ¯ æ”¹å–„ææ¡ˆ")
                
                top_positive = significant_vars[significant_vars['æ¨™æº–åŒ–ä¿‚æ•°'] > 0].head(3)
                top_negative = significant_vars[significant_vars['æ¨™æº–åŒ–ä¿‚æ•°'] < 0].head(3)
                
                if len(top_positive) > 0:
                    st.info("**ğŸš€ é‡ç‚¹çš„ã«æ”¹å–„ã™ã¹ãé ˜åŸŸï¼ˆæ­£ã®å½±éŸ¿ï¼‰:**")
                    for _, row in top_positive.iterrows():
                        st.write(f"â€¢ {row['èª¬æ˜å¤‰æ•°']}ã®å‘ä¸Š â†’ {selected_target}ãŒ{abs(row['æ¨™æº–åŒ–ä¿‚æ•°']):.1%}å‘ä¸ŠæœŸå¾…")
                
                if len(top_negative) > 0:
                    st.warning("**âš ï¸ æ³¨æ„ãŒå¿…è¦ãªé ˜åŸŸï¼ˆè² ã®å½±éŸ¿ï¼‰:**")
                    for _, row in top_negative.iterrows():
                        st.write(f"â€¢ {row['èª¬æ˜å¤‰æ•°']}ã®å•é¡Œ â†’ {selected_target}ãŒ{abs(row['æ¨™æº–åŒ–ä¿‚æ•°']):.1%}ä½ä¸‹ãƒªã‚¹ã‚¯")
                
            else:
                st.warning("çµ±è¨ˆçš„ã«æœ‰æ„ãªå½±éŸ¿è¦å› ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®å¢—åŠ ã‚„ãƒ‡ãƒ¼ã‚¿å“è³ªã®å‘ä¸Šã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
            
    except Exception as e:
        st.error(f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        st.info("ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ã‚„å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    main()