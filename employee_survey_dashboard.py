#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾“æ¥­å“¡èª¿æŸ»å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ - å®Ÿãƒ‡ãƒ¼ã‚¿å¯¾å¿œç‰ˆ
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
from datetime import datetime
import os
import re
from collections import Counter
from janome.tokenizer import Tokenizer
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import locale

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="å¾“æ¥­å“¡èª¿æŸ»å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
    page_icon="ğŸ‘¥",
    layout="wide",
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    html, body, [class*="css"] {
        font-family: 'Helvetica', 'Arial', 'Hiragino Sans', 'Yu Gothic', sans-serif;
        background-color: #f8fafc;
    }
    
    h1, h2, h3 {
        color: #1E293B;
    }
    
    /* æ”¹è‰¯ã•ã‚ŒãŸKPIã‚«ãƒ¼ãƒ‰ã‚¹ã‚¿ã‚¤ãƒ« */
    .kpi-card {
        background: linear-gradient(135deg, #ffffff 0%, #f8fafc 100%);
        border-radius: 16px;
        padding: 24px 20px;
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
        border: 1px solid rgba(255, 255, 255, 0.2);
        margin-bottom: 16px;
        min-height: 140px;
        display: flex;
        flex-direction: column;
        justify-content: space-between;
        position: relative;
        overflow: hidden;
        transition: all 0.3s ease;
    }
    
    .kpi-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 25px rgba(0, 0, 0, 0.12);
    }
    
    .kpi-card::before {
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        height: 4px;
        background: linear-gradient(90deg, #667eea, #764ba2);
    }
    
    .kpi-title {
        font-size: 13px;
        color: #64748b;
        margin-bottom: 12px;
        font-weight: 600;
        text-transform: uppercase;
        letter-spacing: 0.5px;
        line-height: 1.2;
    }
    
    .kpi-value {
        font-size: 32px;
        font-weight: 800;
        color: #1e293b;
        margin-bottom: 8px;
        line-height: 1.1;
        display: flex;
        align-items: baseline;
    }
    
    .kpi-unit {
        font-size: 18px;
        font-weight: 500;
        color: #64748b;
        margin-left: 4px;
    }
    
    .kpi-change {
        font-size: 13px;
        display: flex;
        align-items: center;
        color: #64748b;
        font-weight: 500;
    }
    
    .kpi-icon {
        margin-right: 8px;
        font-size: 20px;
        filter: drop-shadow(0 2px 4px rgba(0,0,0,0.1));
    }
    
    /* ã‚«ãƒ©ãƒ¼ä»˜ãKPIã‚«ãƒ¼ãƒ‰ */
    .kpi-card-green::before {
        background: linear-gradient(90deg, #22c55e, #16a34a);
    }
    .kpi-card-orange::before {
        background: linear-gradient(90deg, #f59e0b, #d97706);
    }
    .kpi-card-red::before {
        background: linear-gradient(90deg, #ef4444, #dc2626);
    }
    .kpi-card-blue::before {
        background: linear-gradient(90deg, #3b82f6, #2563eb);
    }
    
    /* ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼æ”¹å–„ */
    .section-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 20px 28px;
        border-radius: 16px;
        margin-bottom: 32px;
        box-shadow: 0 8px 32px rgba(102, 126, 234, 0.25);
        border: 1px solid rgba(255, 255, 255, 0.1);
    }
    
    .section-header h2 {
        color: white !important;
        margin: 0;
        font-size: 24px;
        font-weight: 700;
    }
    
    /* ãƒ‡ãƒ¼ã‚¿çŠ¶æ³è¡¨ç¤ºã®æ”¹å–„ */
    .data-status {
        background: linear-gradient(135deg, #e0f2fe 0%, #f0f9ff 100%);
        border: 1px solid #0ea5e9;
        border-radius: 12px;
        padding: 16px 20px;
        margin-bottom: 24px;
        box-shadow: 0 2px 8px rgba(14, 165, 233, 0.1);
    }
    
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ˜ãƒƒãƒ€ãƒ¼ */
    .sidebar-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 18px;
        border-radius: 12px;
        display: flex;
        align-items: center;
        margin-bottom: 24px;
        box-shadow: 0 4px 16px rgba(102, 126, 234, 0.2);
    }
    
    .sidebar-logo {
        background-color: rgba(255, 255, 255, 0.95);
        width: 48px;
        height: 48px;
        border-radius: 10px;
        display: flex;
        justify-content: center;
        align-items: center;
        margin-right: 14px;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }
    
    /* ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œ */
    @media (max-width: 768px) {
        .kpi-card {
            min-height: 120px;
            padding: 20px 16px;
        }
        .kpi-value {
            font-size: 28px;
        }
        .kpi-title {
            font-size: 12px;
        }
    }
</style>
""", unsafe_allow_html=True)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°
@st.cache_data
def load_employee_data():
    """å¾“æ¥­å“¡èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
    try:
        excel_path = './data.xlsx'
        
        if not os.path.exists(excel_path):
            st.error("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« 'data.xlsx' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return create_dummy_data()
        
        # Responsesã‚·ãƒ¼ãƒˆã‹ã‚‰å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        try:
            responses_df = pd.read_excel(excel_path, sheet_name='Responses')
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆ1è¡Œç›®ï¼‰ã‚’å–å¾—ã—ã¦åˆ—åã¨ã—ã¦ä½¿ç”¨
            if len(responses_df) >= 1:
                # 1è¡Œç›®ã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦è¨­å®š
                header_row = responses_df.iloc[0]
                responses_df = responses_df.iloc[1:].reset_index(drop=True)
                responses_df.columns = header_row
                
                # ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
                if len(responses_df) > 0:
                    processed_data = process_real_survey_data(responses_df)
                    return processed_data
                
        except Exception as e:
            st.error(f"Responsesã‚·ãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # å®Ÿãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        return create_dummy_data()
        
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return create_dummy_data()

def load_comment_data():
    """ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ãƒ»å‡¦ç†ã™ã‚‹"""
    try:
        excel_path = './data.xlsx'
        if not os.path.exists(excel_path):
            return None
            
        # Responsesã‚·ãƒ¼ãƒˆã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        df_raw = pd.read_excel(excel_path, sheet_name='Responses', header=None)
        
        if len(df_raw) < 3:
            return None
            
        # ã‚³ãƒ¡ãƒ³ãƒˆã‚«ãƒ©ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0ãƒ™ãƒ¼ã‚¹ï¼‰
        comment_columns = {
            'æœŸå¾…ã‚³ãƒ¡ãƒ³ãƒˆ': 60,   # æœ€ã‚‚æœŸå¾…ãŒé«˜ã„é …ç›®ã«ã¤ã„ã¦
            'æº€è¶³ã‚³ãƒ¡ãƒ³ãƒˆ': 103,  # æœ€ã‚‚æº€è¶³åº¦ãŒé«˜ã„é …ç›®ã«ã¤ã„ã¦  
            'ä¸æº€ã‚³ãƒ¡ãƒ³ãƒˆ': 104   # æº€è¶³åº¦ãŒä½ã„é …ç›®ã«ã¤ã„ã¦
        }
        
        comments = {}
        for comment_type, col_idx in comment_columns.items():
            if col_idx < len(df_raw.columns):
                # ãƒ‡ãƒ¼ã‚¿è¡Œï¼ˆ2è¡Œç›®ä»¥é™ï¼‰ã‹ã‚‰ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
                comment_data = []
                for row_idx in range(2, len(df_raw)):
                    comment = df_raw.iloc[row_idx, col_idx]
                    if pd.notna(comment) and str(comment).strip():
                        comment_data.append(str(comment).strip())
                
                comments[comment_type] = comment_data
        
        return comments
        
    except Exception as e:
        st.error(f"ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def preprocess_japanese_text(text):
    """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
    if not text or pd.isna(text):
        return ""
    
    # æ–‡å­—åˆ—ã«å¤‰æ›
    text = str(text)
    
    # ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
    text = re.sub(r'[^\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\u3400-\u4DBF\uFF00-\uFFEFa-zA-Z0-9\s]', '', text)
    
    # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def extract_keywords_janome(texts, min_length=2, max_features=100):
    """Janomeã‚’ä½¿ã£ã¦æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    if not texts:
        return []
    
    # Janomeãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    tokenizer = Tokenizer()
    
    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆé™¤å¤–ã™ã‚‹èªï¼‰
    stop_words = {
        'ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ã‚ˆã†', 'ãªã©', 'ã«ã¤ã„ã¦', 'ã«ãŠã„ã¦', 'ã¨ã—ã¦', 
        'ã«ã‚ˆã‚‹', 'ã«ã‚ˆã‚Š', 'ã«å¯¾ã—ã¦', 'ãã‚Œ', 'ã“ã‚Œ', 'ãã®', 'ã“ã®', 'ã‚ã‚‹',
        'ã„ã‚‹', 'ã™ã‚‹', 'ãªã‚‹', 'ã‚Œã‚‹', 'ã‚‰ã‚Œã‚‹', 'ã§ã™', 'ã§ã‚ã‚‹', 'ã ', 'ã§',
        'ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã®', 'ã¨', 'ã‚„', 'ã‹', 'ã‚‚', 'ã‹ã‚‰', 'ã¾ã§',
        '1', '2', '3', '4', '5', 'ã©ã¡ã‚‰', 'è¨€ãˆã‚‹', 'æº€è¶³', 'æœŸå¾…', 'é …ç›®'
    }
    
    all_keywords = []
    
    for text in texts:
        if not text:
            continue
            
        # å‰å‡¦ç†
        cleaned_text = preprocess_japanese_text(text)
        
        # å½¢æ…‹ç´ è§£æ
        tokens = tokenizer.tokenize(cleaned_text)
        
        for token in tokens:
            # åè©ã®ã¿æŠ½å‡º
            if token.part_of_speech.split(',')[0] in ['åè©']:
                word = token.surface
                
                # æ¡ä»¶ã«åˆã†ã‚‚ã®ã ã‘æŠ½å‡º
                if (len(word) >= min_length and 
                    word not in stop_words and
                    not word.isdigit() and
                    not re.match(r'^[ã-ã‚“]+$', word)):  # ã²ã‚‰ãŒãªã®ã¿ã®èªã‚’é™¤å¤–
                    all_keywords.append(word)
    
    # å‡ºç¾é »åº¦ã§ã‚½ãƒ¼ãƒˆ
    keyword_counts = Counter(all_keywords)
    return keyword_counts.most_common(max_features)

def build_cooccurrence_network(texts, min_cooccurrence=1):
    """å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰"""
    if not texts:
        return None, None
    
    tokenizer = Tokenizer()
    
    # å„ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åè©ã‚’æŠ½å‡º
    doc_keywords = []
    for text in texts:
        if not text:
            continue
            
        cleaned_text = preprocess_japanese_text(text)
        tokens = tokenizer.tokenize(cleaned_text)
        
        keywords = []
        for token in tokens:
            if token.part_of_speech.split(',')[0] in ['åè©']:
                word = token.surface
                if (len(word) >= 2 and 
                    not word.isdigit() and
                    word not in {'ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ã‚ˆã†', 'é …ç›®', 'æº€è¶³', 'æœŸå¾…'}):
                    keywords.append(word)
        
        doc_keywords.append(keywords)
    
    # å…±èµ·é–¢ä¿‚ã‚’è¨ˆç®—
    cooccurrence = {}
    for keywords in doc_keywords:
        for i, word1 in enumerate(keywords):
            for j, word2 in enumerate(keywords):
                if i != j:
                    pair = tuple(sorted([word1, word2]))
                    cooccurrence[pair] = cooccurrence.get(pair, 0) + 1
    
    # æœ€å°å…±èµ·å›æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_cooccurrence = {pair: count for pair, count in cooccurrence.items() 
                           if count >= min_cooccurrence}
    
    if not filtered_cooccurrence:
        return None, None
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
    G = nx.Graph()
    
    for (word1, word2), weight in filtered_cooccurrence.items():
        G.add_edge(word1, word2, weight=weight)
    
    return G, filtered_cooccurrence

def load_timestamp_data():
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ãƒ»å‡¦ç†ã™ã‚‹"""
    try:
        excel_path = './data.xlsx'
        if not os.path.exists(excel_path):
            return None
            
        # Responsesã‚·ãƒ¼ãƒˆã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        df_raw = pd.read_excel(excel_path, sheet_name='Responses', header=None)
        
        if len(df_raw) < 2:
            return None
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚«ãƒ©ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0ãƒ™ãƒ¼ã‚¹ï¼‰
        start_time_col = 2  # å›ç­”é–‹å§‹ï¼ˆåˆ—3ï¼‰
        end_time_col = 3    # å›ç­”å®Œäº†ï¼ˆåˆ—4ï¼‰
        
        timestamp_data = []
        
        # ãƒ‡ãƒ¼ã‚¿è¡Œï¼ˆ2è¡Œç›®ä»¥é™ï¼‰ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
        for row_idx in range(1, len(df_raw)):
            try:
                start_time_raw = df_raw.iloc[row_idx, start_time_col]
                end_time_raw = df_raw.iloc[row_idx, end_time_col]
                
                if pd.notna(start_time_raw) and pd.notna(end_time_raw):
                    # æ—¥æœ¬èªæ—¥æ™‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ãƒ‘ãƒ¼ã‚¹
                    start_time = parse_japanese_datetime(str(start_time_raw))
                    end_time = parse_japanese_datetime(str(end_time_raw))
                    
                    if start_time and end_time:
                        duration_minutes = (end_time - start_time).total_seconds() / 60
                        timestamp_data.append({
                            'response_id': row_idx,
                            'start_time': start_time,
                            'end_time': end_time,
                            'duration_minutes': duration_minutes,
                            'date': start_time.date(),
                            'hour': start_time.hour,
                            'weekday': start_time.weekday(),
                            'weekday_name': ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥'][start_time.weekday()]
                        })
            except Exception as e:
                continue
        
        return pd.DataFrame(timestamp_data) if timestamp_data else None
        
    except Exception as e:
        st.error(f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def parse_japanese_datetime(datetime_str):
    """æ—¥æœ¬èªã®æ—¥æ™‚æ–‡å­—åˆ—ã‚’è§£æã—ã¦datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
    try:
        # "6æœˆ 08, 2025 08:39:24 åˆå¾Œ" ã®ã‚ˆã†ãªå½¢å¼ã‚’å‡¦ç†
        import re
        
        # æœˆåã®æ—¥æœ¬èªã‚’è‹±èªã«å¤‰æ›
        month_map = {
            '1æœˆ': 'Jan', '2æœˆ': 'Feb', '3æœˆ': 'Mar', '4æœˆ': 'Apr',
            '5æœˆ': 'May', '6æœˆ': 'Jun', '7æœˆ': 'Jul', '8æœˆ': 'Aug', 
            '9æœˆ': 'Sep', '10æœˆ': 'Oct', '11æœˆ': 'Nov', '12æœˆ': 'Dec'
        }
        
        datetime_str = str(datetime_str).strip()
        
        # åˆå‰/åˆå¾Œã®å‡¦ç†
        is_pm = 'åˆå¾Œ' in datetime_str
        datetime_str = datetime_str.replace('åˆå‰', '').replace('åˆå¾Œ', '').strip()
        
        # æ—¥æœ¬èªã®æœˆã‚’è‹±èªã«å¤‰æ›
        for jp_month, en_month in month_map.items():
            if jp_month in datetime_str:
                datetime_str = datetime_str.replace(jp_month, en_month)
                break
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§æ—¥æ™‚è¦ç´ ã‚’æŠ½å‡º
        pattern = r'(\w+)\s+(\d+),\s+(\d+)\s+(\d+):(\d+):(\d+)'
        match = re.search(pattern, datetime_str)
        
        if match:
            month_str, day, year, hour, minute, second = match.groups()
            
            # æœˆåã‚’æ•°å€¤ã«å¤‰æ›
            month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
            month = month_names.index(month_str) + 1
            
            # åˆå¾Œã®å ´åˆã¯12æ™‚é–“ã‚’è¿½åŠ ï¼ˆãŸã ã—12æ™‚ã®å ´åˆã¯é™¤ãï¼‰
            hour_int = int(hour)
            if is_pm and hour_int != 12:
                hour_int += 12
            elif not is_pm and hour_int == 12:
                hour_int = 0
            
            return datetime(int(year), month, int(day), hour_int, int(minute), int(second))
        
        return None
        
    except Exception as e:
        return None

def process_real_survey_data(df):
    """å®Ÿéš›ã®èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹"""
    processed_data = []
    
    # å„å›ç­”è€…ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
    for idx, row in df.iterrows():
        try:
            # åŸºæœ¬æƒ…å ±ã®æŠ½å‡º
            employee_data = {
                'response_id': idx + 1,
                'department': extract_value(row, 'æ‰€å±äº‹æ¥­éƒ¨', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°éƒ¨'),
                'position': extract_value(row, 'å½¹è·', 'å½¹è·ãªã—'),
                'employment_type': extract_value(row, 'é›‡ç”¨å½¢æ…‹', 'æ­£ç¤¾å“¡'),
                'job_category': extract_value(row, 'è·ç¨®', 'ãã®ä»–'),
                'start_year': extract_numeric_value(row, 'å…¥ç¤¾å¹´åº¦ã‚’æ•™ãˆã¦ãã ã•ã„', 2019),
                'annual_salary': extract_numeric_value(row, 'æ¦‚ç®—å¹´åã‚’æ•™ãˆã¦ãã ã•ã„', 500),
                'monthly_overtime': extract_numeric_value(row, '1ãƒ¶æœˆå½“ãŸã‚Šã®å¹³å‡æ®‹æ¥­æ™‚é–“', 20),
                'paid_leave_rate': extract_numeric_value(row, '1å¹´é–“å½“ãŸã‚Šã®å¹³å‡æœ‰çµ¦ä¼‘æš‡å–å¾—ç‡', 50),
            }
            
            # ä¸»è¦è©•ä¾¡æŒ‡æ¨™ã®æŠ½å‡º
            employee_data.update({
                'nps_score': extract_nps_score(row, 'ç·åˆè©•ä¾¡ï¼šè‡ªåˆ†ã®è¦ªã—ã„å‹äººã‚„å®¶æ—ã«å¯¾ã—ã¦ã€ã“ã®ä¼šç¤¾ã¸ã®è»¢è·ãƒ»å°±è·ã‚’ã©ã®ç¨‹åº¦å‹§ã‚ãŸã„ã¨æ€ã„ã¾ã™ã‹ï¼Ÿ'),
                'overall_satisfaction': extract_satisfaction_score(row, 'ç·åˆæº€è¶³åº¦ï¼šè‡ªç¤¾ã®ç¾åœ¨ã®åƒãç’°å¢ƒã‚„æ¡ä»¶ã€å‘¨ã‚Šã®äººé–“é–¢ä¿‚ãªã©ã‚‚å«ã‚ã‚ãªãŸã¯ã©ã®ç¨‹åº¦æº€è¶³ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ'),
                'long_term_intention': extract_satisfaction_score(row, 'ã‚ãªãŸã¯ã“ã®ä¼šç¤¾ã§ã“ã‚Œã‹ã‚‰ã‚‚é•·ãåƒããŸã„ã¨æ€ã‚ã‚Œã¾ã™ã‹ï¼Ÿ'),
                'contribution_score': extract_satisfaction_score(row, 'æ´»èºè²¢çŒ®åº¦ï¼šç¾åœ¨ã®ä¼šç¤¾ã‚„æ‰€å±çµ„ç¹”ã§ã‚ãªãŸã¯ã©ã®ç¨‹åº¦ã€æ´»èºè²¢çŒ®ã§ãã¦ã„ã‚‹ã¨æ„Ÿã˜ã¾ã™ã‹ï¼Ÿ'),
            })
            
            # æœŸå¾…åº¦é …ç›®ã®æŠ½å‡º
            expectation_items = [
                ('å‹¤å‹™æ™‚é–“', 'è‡ªåˆ†ã«åˆã£ãŸå‹¤å‹™æ™‚é–“ã§åƒã‘ã‚‹è·å ´'),
                ('ä¼‘æ—¥ä¼‘æš‡', 'ä¼‘æ—¥ä¼‘æš‡ãŒã¡ã‚ƒã‚“ã¨å–ã‚Œã‚‹è·å ´'),
                ('æœ‰çµ¦ä¼‘æš‡', 'æœ‰çµ¦ä¼‘æš‡ãŒã¡ã‚ƒã‚“ã¨å–ã‚Œã‚‹è·å ´'),
                ('å‹¤å‹™ä½“ç³»', 'æŸ”è»Ÿãªå‹¤å‹™ä½“ç³»ï¼ˆãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã€æ™‚çŸ­å‹¤å‹™ã€ãƒ•ãƒ¬ãƒƒã‚¯ã‚¹åˆ¶ãªã©ï¼‰ã®ã‚‚ã¨ã§åƒã‘ã‚‹è·å ´'),
                ('æ˜‡çµ¦æ˜‡æ ¼', 'æˆæœã«å¿œã˜ã¦æ—©æœŸã®æ˜‡çµ¦ãƒ»æ˜‡æ ¼ãŒæœ›ã‚ã‚‹è·å ´'),
                ('äººé–“é–¢ä¿‚', 'äººé–“é–¢ä¿‚ãŒè‰¯å¥½ãªè·å ´'),
                ('åƒãç’°å¢ƒ', 'åƒãã‚„ã™ã„ä»•äº‹ç’°å¢ƒã‚„ã‚ªãƒ•ã‚£ã‚¹ç’°å¢ƒãŒã‚ã‚‹ä¼šç¤¾'),
                ('æˆé•·å®Ÿæ„Ÿ', 'å°‚é–€çš„ãªã‚¹ã‚­ãƒ«ã‚„æŠ€è¡“ãƒ»çŸ¥è­˜ã‚„çµŒé¨“ã‚’ç²å¾—ã§ãã‚‹è·å ´'),
                ('å°†æ¥ã‚­ãƒ£ãƒªã‚¢', 'è‡ªåˆ†ã«åˆã£ãŸå°†æ¥ã®ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ã‚’ã—ã£ã‹ã‚Šè¨­è¨ˆã—ã¦ãã‚Œã‚‹è·å ´'),
                ('ç¦åˆ©åšç”Ÿ', 'å……å®Ÿã—ãŸç¦åˆ©åšç”ŸãŒã‚ã‚‹è·å ´'),
                ('è©•ä¾¡åˆ¶åº¦', 'è‡ªèº«ã®è¡Œã£ãŸä»•äº‹ãŒæ­£å½“ã«è©•ä¾¡ã•ã‚Œã‚‹è·å ´'),
            ]
            
            for category, keyword in expectation_items:
                expectation_score = extract_expectation_score(row, keyword)
                employee_data[f'{category}_æœŸå¾…åº¦'] = expectation_score
            
            # æº€è¶³åº¦é …ç›®ã®æŠ½å‡º
            satisfaction_items = [
                ('å‹¤å‹™æ™‚é–“', 'è‡ªåˆ†ã«åˆã£ãŸå‹¤å‹™æ™‚é–“ã§åƒã‘ã‚‹'),
                ('ä¼‘æ—¥ä¼‘æš‡', 'ä¼‘æ—¥ä¼‘æš‡ãŒã¡ã‚ƒã‚“ã¨å–ã‚Œã‚‹'),
                ('æœ‰çµ¦ä¼‘æš‡', 'æœ‰çµ¦ä¼‘æš‡ãŒã¡ã‚ƒã‚“ã¨å–ã‚Œã‚‹'),
                ('å‹¤å‹™ä½“ç³»', 'æŸ”è»Ÿãªå‹¤å‹™ä½“ç³»ï¼ˆãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã€æ™‚çŸ­å‹¤å‹™ã€ãƒ•ãƒ¬ãƒƒã‚¯ã‚¹åˆ¶ãªã©ï¼‰ã®ã‚‚ã¨ã§åƒã‘ã‚‹'),
                ('æ˜‡çµ¦æ˜‡æ ¼', 'æˆæœã«å¿œã˜ã¦æ—©æœŸã®æ˜‡çµ¦ãƒ»æ˜‡æ ¼ãŒæœ›ã‚ã‚‹ä½“åˆ¶ã«ã¤ã„ã¦'),
                ('äººé–“é–¢ä¿‚', 'äººé–“é–¢ä¿‚ãŒè‰¯å¥½ãªç’°å¢ƒã«ã¤ã„ã¦'),
                ('åƒãç’°å¢ƒ', 'åƒãã‚„ã™ã„ä»•äº‹ç’°å¢ƒã‚„ã‚ªãƒ•ã‚£ã‚¹ç’°å¢ƒãŒã‚ã‚‹ä¼šç¤¾'),
                ('æˆé•·å®Ÿæ„Ÿ', 'å°‚é–€çš„ãªã‚¹ã‚­ãƒ«ã‚„æŠ€è¡“ãƒ»çŸ¥è­˜ã‚„çµŒé¨“ã®ç²å¾—ã«ã¤ã„ã¦'),
                ('å°†æ¥ã‚­ãƒ£ãƒªã‚¢', 'è‡ªåˆ†ã«åˆã£ãŸå°†æ¥ã®ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹è¨­è¨ˆã«ã¤ã„ã¦'),
                ('ç¦åˆ©åšç”Ÿ', 'å……å®Ÿã—ãŸç¦åˆ©åšç”Ÿã«ã¤ã„ã¦'),
                ('è©•ä¾¡åˆ¶åº¦', 'è‡ªèº«ã®è¡Œã£ãŸä»•äº‹ãŒæ­£å½“ã«è©•ä¾¡ã•ã‚Œã‚‹ä½“åˆ¶ã«ã¤ã„ã¦'),
            ]
            
            for category, keyword in satisfaction_items:
                satisfaction_score = extract_satisfaction_score_detailed(row, keyword)
                employee_data[f'{category}_æº€è¶³åº¦'] = satisfaction_score
            
            processed_data.append(employee_data)
            
        except Exception as e:
            st.warning(f"è¡Œ {idx + 1} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    if processed_data:
        return {'employee_data': pd.DataFrame(processed_data)}
    else:
        return create_dummy_data()

def extract_value(row, keyword, default=''):
    """è¡Œã‹ã‚‰ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€åˆ—ã®å€¤ã‚’æŠ½å‡º"""
    for col in row.index:
        if str(col).find(keyword) != -1:
            value = row[col]
            return str(value) if pd.notna(value) else default
    return default

def extract_numeric_value(row, keyword, default=0):
    """è¡Œã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º"""
    for col in row.index:
        if str(col).find(keyword) != -1:
            value = row[col]
            if pd.notna(value):
                try:
                    return float(str(value).replace(',', ''))
                except:
                    return default
    return default

def extract_nps_score(row, keyword):
    """NPS ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡ºï¼ˆ0-10ã®å€¤ï¼‰"""
    for col in row.index:
        if str(col).find(keyword) != -1:
            value = str(row[col])
            if 'Promoter' in value:
                return 9
            elif 'Passive' in value:
                return 7
            elif 'Detractor' in value:
                return 4
            else:
                try:
                    return int(float(value))
                except:
                    return 5
    return 5

def extract_satisfaction_score(row, keyword):
    """æº€è¶³åº¦ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡ºï¼ˆ1-5ã®å€¤ï¼‰"""
    for col in row.index:
        if str(col).find(keyword) != -1:
            value = str(row[col])
            if 'Promoter' in value:
                return 5
            elif 'Passive' in value:
                return 3
            elif 'Detractor' in value:
                return 2
            else:
                try:
                    return int(float(value))
                except:
                    return 3
    return 3

def extract_expectation_score(row, keyword):
    """æœŸå¾…åº¦ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º"""
    for col in row.index:
        if str(col).find(keyword) != -1:
            value = row[col]
            if pd.notna(value):
                try:
                    return int(float(value))
                except:
                    return 3
    return 3

def extract_satisfaction_score_detailed(row, keyword):
    """è©³ç´°æº€è¶³åº¦ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º"""
    for col in row.index:
        if str(col).find(keyword) != -1:
            value = row[col]
            if pd.notna(value):
                try:
                    return int(float(value))
                except:
                    return 3
    return 3

def create_dummy_data():
    """ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ãªã„å ´åˆï¼‰"""
    np.random.seed(42)
    n_employees = 50
    
    employee_data = pd.DataFrame({
        'response_id': range(1, n_employees + 1),
        'department': np.random.choice(['ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°éƒ¨', 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°éƒ¨', 'äººäº‹éƒ¨', 'å–¶æ¥­éƒ¨'], n_employees),
        'position': np.random.choice(['å½¹è·ãªã—', 'ãƒãƒ¼ãƒ ãƒªãƒ¼ãƒ€ãƒ¼', 'ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼'], n_employees),
        'start_year': np.random.choice(range(2018, 2025), n_employees),
        'annual_salary': np.random.normal(500, 100, n_employees).clip(300, 1000).astype(int),
        'monthly_overtime': np.random.normal(20, 10, n_employees).clip(0, 60).astype(int),
        'paid_leave_rate': np.random.normal(60, 20, n_employees).clip(10, 100).astype(int),
        'nps_score': np.random.choice(range(0, 11), n_employees),
        'overall_satisfaction': np.random.choice(range(1, 6), n_employees),
        'long_term_intention': np.random.choice(range(1, 6), n_employees),
        'contribution_score': np.random.choice(range(1, 6), n_employees),
    })
    
    categories = ['å‹¤å‹™æ™‚é–“', 'ä¼‘æ—¥ä¼‘æš‡', 'æœ‰çµ¦ä¼‘æš‡', 'å‹¤å‹™ä½“ç³»', 'æ˜‡çµ¦æ˜‡æ ¼', 'äººé–“é–¢ä¿‚', 
                 'åƒãç’°å¢ƒ', 'æˆé•·å®Ÿæ„Ÿ', 'å°†æ¥ã‚­ãƒ£ãƒªã‚¢', 'ç¦åˆ©åšç”Ÿ', 'è©•ä¾¡åˆ¶åº¦']
    
    for category in categories:
        employee_data[f'{category}_æœŸå¾…åº¦'] = np.random.choice(range(1, 6), n_employees)
        employee_data[f'{category}_æº€è¶³åº¦'] = np.random.choice(range(1, 6), n_employees)
    
    return {'employee_data': employee_data}

@st.cache_data
def calculate_kpis(data):
    """KPIã‚’è¨ˆç®—ã™ã‚‹"""
    if 'employee_data' not in data:
        return {}
    
    df = data['employee_data']
    
    # NPSè¨ˆç®—
    promoters = len(df[df['nps_score'] >= 9])
    detractors = len(df[df['nps_score'] <= 6])
    nps = ((promoters - detractors) / len(df)) * 100 if len(df) > 0 else 0
    
    # æº€è¶³åº¦ã‚«ãƒ†ã‚´ãƒªãƒ¼
    categories = ['å‹¤å‹™æ™‚é–“', 'ä¼‘æ—¥ä¼‘æš‡', 'æœ‰çµ¦ä¼‘æš‡', 'å‹¤å‹™ä½“ç³»', 'æ˜‡çµ¦æ˜‡æ ¼', 'äººé–“é–¢ä¿‚', 
                 'åƒãç’°å¢ƒ', 'æˆé•·å®Ÿæ„Ÿ', 'å°†æ¥ã‚­ãƒ£ãƒªã‚¢', 'ç¦åˆ©åšç”Ÿ', 'è©•ä¾¡åˆ¶åº¦']
    
    satisfaction_by_category = {}
    expectation_by_category = {}
    gap_by_category = {}
    
    for category in categories:
        sat_col = f'{category}_æº€è¶³åº¦'
        exp_col = f'{category}_æœŸå¾…åº¦'
        if sat_col in df.columns and exp_col in df.columns:
            satisfaction_by_category[category] = df[sat_col].mean()
            expectation_by_category[category] = df[exp_col].mean()
            gap_by_category[category] = df[sat_col].mean() - df[exp_col].mean()
    
    return {
        'total_employees': len(df),
        'nps': nps,
        'avg_nps_score': df['nps_score'].mean(),
        'avg_satisfaction': df['overall_satisfaction'].mean(),
        'avg_contribution': df['contribution_score'].mean(),
        'avg_long_term_intention': df['long_term_intention'].mean(),
        'avg_salary': df['annual_salary'].mean(),
        'avg_overtime': df['monthly_overtime'].mean(),
        'avg_leave_usage': df['paid_leave_rate'].mean(),
        'satisfaction_by_category': satisfaction_by_category,
        'expectation_by_category': expectation_by_category,
        'gap_by_category': gap_by_category,
    }

def show_kpi_overview(data, kpis):
    """KPIæ¦‚è¦ã‚’è¡¨ç¤ºï¼ˆStreamlitæ¨™æº–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆä½¿ç”¨ï¼‰"""
    st.header("ğŸ“Š ç·åˆKPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    
    # ãƒ‡ãƒ¼ã‚¿çŠ¶æ³ã®è¡¨ç¤º
    df = data['employee_data']
    
    # æƒ…å ±ãƒœãƒƒã‚¯ã‚¹
    st.info(f"ğŸ“… **ãƒ‡ãƒ¼ã‚¿æœ€çµ‚æ›´æ–°:** {datetime.now().strftime('%Y/%m/%d %H:%M')} | ğŸ‘¥ **å›ç­”è€…æ•°:** {len(df)}å | ğŸ“‹ **èª¿æŸ»é …ç›®:** {len(df.columns)}é …ç›®")
    
    if not kpis:
        st.error("KPIãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return
    
    st.subheader("ğŸ¯ ä¸»è¦æŒ‡æ¨™")
    
    # ãƒ¡ã‚¤ãƒ³KPIï¼ˆStreamlitæ¨™æº–ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½¿ç”¨ï¼‰
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        nps_delta = "ğŸ“ˆ è‰¯å¥½" if kpis['nps'] > 0 else "ğŸ“‰ è¦æ”¹å–„" if kpis['nps'] < -10 else "âš ï¸ æ™®é€š"
        nps_color = "normal" if kpis['nps'] > 0 else "inverse" if kpis['nps'] < -10 else "off"
        st.metric(
            label="ğŸ“ˆ eNPS",
            value=f"{kpis['nps']:.1f}",
            delta=nps_delta,
            delta_color=nps_color
        )
        st.caption("å¾“æ¥­å“¡æ¨å¥¨åº¦æŒ‡æ¨™")
    
    with col2:
        satisfaction = kpis['avg_satisfaction']
        sat_delta = f"ğŸ˜Š è‰¯å¥½" if satisfaction >= 4 else "ğŸ˜” è¦æ”¹å–„" if satisfaction <= 2.5 else "ğŸ˜ æ™®é€š"
        sat_color = "normal" if satisfaction >= 4 else "inverse" if satisfaction <= 2.5 else "off"
        st.metric(
            label="ğŸ˜Š ç·åˆæº€è¶³åº¦",
            value=f"{satisfaction:.1f}/5",
            delta=sat_delta,
            delta_color=sat_color
        )
        st.caption("å…¨ä½“çš„ãªæº€è¶³åº¦ãƒ¬ãƒ™ãƒ«")
    
    with col3:
        contribution = kpis['avg_contribution']
        cont_delta = "â­ é«˜ã„" if contribution >= 4 else "ğŸ’” ä½ã„" if contribution <= 2.5 else "âš–ï¸ æ™®é€š"
        cont_color = "normal" if contribution >= 4 else "inverse" if contribution <= 2.5 else "off"
        st.metric(
            label="â­ æ´»èºè²¢çŒ®åº¦",
            value=f"{contribution:.1f}/5",
            delta=cont_delta,
            delta_color=cont_color
        )
        st.caption("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡")
    
    with col4:
        intention = kpis['avg_long_term_intention']
        int_delta = "ğŸ¢ é«˜ã„" if intention >= 4 else "âš ï¸ ä½ã„" if intention <= 2.5 else "â– æ™®é€š"
        int_color = "normal" if intention >= 4 else "inverse" if intention <= 2.5 else "off"
        st.metric(
            label="ğŸ¢ å‹¤ç¶šæ„å‘",
            value=f"{intention:.1f}/5",
            delta=int_delta,
            delta_color=int_color
        )
        st.caption("é•·æœŸå®šç€æ„å‘")
    
    st.divider()
    st.subheader("ğŸ“Š åŸºæœ¬æŒ‡æ¨™")
    
    # ã‚µãƒ–KPI
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            label="ğŸ’° å¹³å‡å¹´å",
            value=f"{kpis['avg_salary']:.0f}ä¸‡å††",
            delta="å¹´åãƒ¬ãƒ™ãƒ«"
        )
        st.caption("çµ¦ä¸æ°´æº–")
    
    with col2:
        overtime_delta = "âš ï¸ å¤šã„" if kpis['avg_overtime'] >= 40 else "âœ… é©æ­£" if kpis['avg_overtime'] <= 20 else "âš–ï¸ æ™®é€š"
        overtime_color = "inverse" if kpis['avg_overtime'] >= 40 else "normal" if kpis['avg_overtime'] <= 20 else "off"
        st.metric(
            label="â° æœˆå¹³å‡æ®‹æ¥­æ™‚é–“",
            value=f"{kpis['avg_overtime']:.1f}h",
            delta=overtime_delta,
            delta_color=overtime_color
        )
        st.caption("åŠ´åƒæ™‚é–“ç®¡ç†")
    
    with col3:
        leave_delta = "âœ… è‰¯å¥½" if kpis['avg_leave_usage'] >= 80 else "âš ï¸ ä½ã„" if kpis['avg_leave_usage'] <= 50 else "â– æ™®é€š"
        leave_color = "normal" if kpis['avg_leave_usage'] >= 80 else "inverse" if kpis['avg_leave_usage'] <= 50 else "off"
        st.metric(
            label="ğŸ–ï¸ æœ‰çµ¦å–å¾—ç‡",
            value=f"{kpis['avg_leave_usage']:.1f}%",
            delta=leave_delta,
            delta_color=leave_color
        )
        st.caption("ä¼‘æš‡åˆ©ç”¨çŠ¶æ³")
    
    with col4:
        st.metric(
            label="ğŸ‘¥ å›ç­”è€…æ•°",
            value=f"{kpis['total_employees']}å",
            delta="å…¨å›ç­”è€…"
        )
        st.caption("èª¿æŸ»å‚åŠ è€…")

def show_satisfaction_analysis(data, kpis):
    """æº€è¶³åº¦åˆ†æã‚’è¡¨ç¤º"""
    st.header("ğŸ“ˆ æº€è¶³åº¦ãƒ»æœŸå¾…åº¦åˆ†æ")
    
    if not kpis or 'satisfaction_by_category' not in kpis:
        st.error("æº€è¶³åº¦ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return
    
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ", "ğŸ“‹ æº€è¶³åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "ğŸ¯ æœŸå¾…åº¦ã‚®ãƒ£ãƒƒãƒ—åˆ†æ"])
    
    with tab1:
        col1, col2 = st.columns(2)
        
        with col1:
            # æº€è¶³åº¦ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
            categories = list(kpis['satisfaction_by_category'].keys())
            satisfaction_values = list(kpis['satisfaction_by_category'].values())
            
            fig = go.Figure()
            fig.add_trace(go.Scatterpolar(
                r=satisfaction_values,
                theta=categories,
                fill='toself',
                name='æº€è¶³åº¦',
                marker_color='rgba(46, 204, 113, 0.6)',
                line=dict(color='rgba(46, 204, 113, 1)', width=3)
            ))
            
            fig.update_layout(
                polar=dict(
                    radialaxis=dict(visible=True, range=[0, 5], tickfont=dict(size=10)),
                    angularaxis=dict(tickfont=dict(size=9))
                ),
                showlegend=False,
                title="æº€è¶³åº¦ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ",
                height=400
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # æœŸå¾…åº¦ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
            if 'expectation_by_category' in kpis:
                expectation_values = list(kpis['expectation_by_category'].values())
                
                fig = go.Figure()
                fig.add_trace(go.Scatterpolar(
                    r=satisfaction_values,
                    theta=categories,
                    fill='toself',
                    name='æº€è¶³åº¦',
                    marker_color='rgba(46, 204, 113, 0.6)',
                    line=dict(color='rgba(46, 204, 113, 1)', width=2)
                ))
                fig.add_trace(go.Scatterpolar(
                    r=expectation_values,
                    theta=categories,
                    fill='toself',
                    name='æœŸå¾…åº¦',
                    marker_color='rgba(52, 152, 219, 0.4)',
                    line=dict(color='rgba(52, 152, 219, 1)', width=2)
                ))
                
                fig.update_layout(
                    polar=dict(
                        radialaxis=dict(visible=True, range=[0, 5], tickfont=dict(size=10)),
                        angularaxis=dict(tickfont=dict(size=9))
                    ),
                    title="æº€è¶³åº¦ vs æœŸå¾…åº¦",
                    height=400
                )
                
                st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        # æº€è¶³åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        categories = list(kpis['satisfaction_by_category'].keys())
        satisfaction_values = list(kpis['satisfaction_by_category'].values())
        
        satisfaction_df = pd.DataFrame({
            'ã‚«ãƒ†ã‚´ãƒª': categories,
            'æº€è¶³åº¦': satisfaction_values
        }).sort_values('æº€è¶³åº¦', ascending=True)
        
        fig = px.bar(
            satisfaction_df,
            x='æº€è¶³åº¦',
            y='ã‚«ãƒ†ã‚´ãƒª',
            orientation='h',
            title="ã‚«ãƒ†ã‚´ãƒªåˆ¥æº€è¶³åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
            color='æº€è¶³åº¦',
            color_continuous_scale='RdYlGn',
            range_color=[1, 5],
            height=600
        )
        
        fig.update_layout(
            xaxis_title="æº€è¶³åº¦ (1-5ç‚¹)",
            yaxis_title="",
            coloraxis_colorbar=dict(title="æº€è¶³åº¦ã‚¹ã‚³ã‚¢")
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with tab3:
        # æœŸå¾…åº¦ã‚®ãƒ£ãƒƒãƒ—åˆ†æ
        if ('gap_by_category' in kpis and 
            'expectation_by_category' in kpis and 
            len(satisfaction_values) > 0):
            
            try:
                gap_df = pd.DataFrame({
                    'ã‚«ãƒ†ã‚´ãƒª': list(kpis['gap_by_category'].keys()),
                    'ã‚®ãƒ£ãƒƒãƒ—': list(kpis['gap_by_category'].values()),
                    'æº€è¶³åº¦': satisfaction_values,
                    'æœŸå¾…åº¦': list(kpis['expectation_by_category'].values())
                })
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
                if gap_df.empty or len(gap_df) == 0:
                    st.warning("æœŸå¾…åº¦ã‚®ãƒ£ãƒƒãƒ—åˆ†æç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
                    return
                    
            except Exception as e:
                st.error(f"ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                return
            
            # 4è±¡é™ãƒ—ãƒ­ãƒƒãƒˆï¼ˆå¤§å¹…æ”¹å–„ç‰ˆï¼‰
            fig = go.Figure()
            
            # 4è±¡é™ã®èƒŒæ™¯è‰²ã‚’è¿½åŠ 
            mid_x, mid_y = 3, 3  # ä¸­å¤®å€¤
            
            # è±¡é™åˆ†é¡é–¢æ•°ã‚’äº‹å‰å®šç¾©
            def classify_quadrant(row):
                x, y = row['æº€è¶³åº¦'], row['æœŸå¾…åº¦']
                if x >= 3 and y >= 3:
                    return 'ğŸ¯ ç†æƒ³çš„'
                elif x < 3 and y >= 3:
                    return 'ğŸš¨ è¦æ”¹å–„'
                elif x >= 3 and y < 3:
                    return 'ğŸ’ æº€è¶³è¶…é'
                else:
                    return 'ğŸ”„ æ©Ÿä¼šé ˜åŸŸ'
            
            # è±¡é™ã®èƒŒæ™¯è‰²ï¼ˆæ‹¡å¤§ç‰ˆ - æ–‡å­—ã®è¦–èªæ€§å‘ä¸Šï¼‰
            fig.add_shape(
                type="rect", x0=0.5, y0=mid_y, x1=mid_x, y1=5.5,
                fillcolor="rgba(245, 101, 101, 0.15)", line=dict(width=0),
                name="è¦æ”¹å–„ï¼ˆä½æº€è¶³ãƒ»é«˜æœŸå¾…ï¼‰"
            )
            fig.add_shape(
                type="rect", x0=mid_x, y0=mid_y, x1=5.5, y1=5.5,
                fillcolor="rgba(72, 187, 120, 0.15)", line=dict(width=0),
                name="ç†æƒ³çš„ï¼ˆé«˜æº€è¶³ãƒ»é«˜æœŸå¾…ï¼‰"
            )
            fig.add_shape(
                type="rect", x0=0.5, y0=0.5, x1=mid_x, y1=mid_y,
                fillcolor="rgba(237, 137, 54, 0.15)", line=dict(width=0),
                name="æ©Ÿä¼šé ˜åŸŸï¼ˆä½æº€è¶³ãƒ»ä½æœŸå¾…ï¼‰"
            )
            fig.add_shape(
                type="rect", x0=mid_x, y0=0.5, x1=5.5, y1=mid_y,
                fillcolor="rgba(159, 122, 234, 0.15)", line=dict(width=0),
                name="æº€è¶³è¶…éï¼ˆé«˜æº€è¶³ãƒ»ä½æœŸå¾…ï¼‰"
            )
            
            # åŒºåˆ‡ã‚Šç·šã‚’è¿½åŠ 
            fig.add_hline(y=mid_y, line_dash="dash", line_color="rgba(128, 128, 128, 0.8)", line_width=2)
            fig.add_vline(x=mid_x, line_dash="dash", line_color="rgba(128, 128, 128, 0.8)", line_width=2)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’è¿½åŠ ï¼ˆãƒ†ã‚­ã‚¹ãƒˆé‡ãªã‚Šå›é¿ç‰ˆï¼‰
            colors = []
            sizes = []
            symbols = []
            text_positions = []
            
            # ãƒ†ã‚­ã‚¹ãƒˆä½ç½®ã‚’å‹•çš„ã«æ±ºå®šã™ã‚‹é–¢æ•°
            def get_optimal_text_position(x, y, index, total_points):
                positions = [
                    "top center", "bottom center", "middle left", "middle right",
                    "top left", "top right", "bottom left", "bottom right"
                ]
                
                # è±¡é™ãƒ™ãƒ¼ã‚¹ã®åŸºæœ¬ä½ç½®ã‚’æ±ºå®š
                if x >= mid_x and y >= mid_y:  # ç†æƒ³çš„
                    base_pos = ["top center", "top right", "middle right"]
                elif x < mid_x and y >= mid_y:  # è¦æ”¹å–„
                    base_pos = ["top center", "top left", "middle left"]
                elif x >= mid_x and y < mid_y:  # æº€è¶³è¶…é
                    base_pos = ["bottom center", "bottom right", "middle right"]
                else:  # æ©Ÿä¼šé ˜åŸŸ
                    base_pos = ["bottom center", "bottom left", "middle left"]
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åŸºã¥ã„ã¦ä½ç½®ã‚’å¾ªç’°é¸æŠ
                return base_pos[index % len(base_pos)]
            
            for i, (_, row) in enumerate(gap_df.iterrows()):
                x, y = row['æº€è¶³åº¦'], row['æœŸå¾…åº¦']
                gap = row['ã‚®ãƒ£ãƒƒãƒ—']
                
                # è±¡é™ã«ã‚ˆã£ã¦è‰²ã‚’æ±ºå®šï¼ˆã™ã¹ã¦å††å½¢ã§çµ±ä¸€ï¼‰
                if x >= mid_x and y >= mid_y:
                    colors.append('#48BB78')  # ç·‘ - ç†æƒ³çš„
                    symbols.append('circle')
                elif x < mid_x and y >= mid_y:
                    colors.append('#F56565')  # èµ¤ - è¦æ”¹å–„
                    symbols.append('circle')
                elif x >= mid_x and y < mid_y:
                    colors.append('#9F7AEA')  # ç´« - æº€è¶³è¶…é
                    symbols.append('circle')
                else:
                    colors.append('#ED8936')  # ã‚ªãƒ¬ãƒ³ã‚¸ - æ©Ÿä¼šé ˜åŸŸ
                    symbols.append('circle')
                
                sizes.append(20)  # çµ±ä¸€ã‚µã‚¤ã‚º
                text_positions.append(get_optimal_text_position(x, y, i, len(gap_df)))
            
            # ãƒãƒ¼ã‚«ãƒ¼ã®ã¿ã‚’è¡¨ç¤ºï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯åˆ†é›¢ï¼‰
            fig.add_trace(go.Scatter(
                x=gap_df['æº€è¶³åº¦'],
                y=gap_df['æœŸå¾…åº¦'],
                mode='markers',
                marker=dict(
                    size=sizes,
                    color=colors,
                    symbol=symbols,
                    line=dict(width=2, color='white'),
                    opacity=0.9
                ),
                hovertemplate='<b>%{customdata[0]}</b><br>' +
                            'æº€è¶³åº¦: %{x:.1f}<br>' +
                            'æœŸå¾…åº¦: %{y:.1f}<br>' +
                            'ã‚®ãƒ£ãƒƒãƒ—: %{customdata[1]:.2f}<br>' +
                            'è±¡é™: %{customdata[2]}<extra></extra>',
                customdata=list(zip(
                    gap_df['ã‚«ãƒ†ã‚´ãƒª'], 
                    gap_df['ã‚®ãƒ£ãƒƒãƒ—'],
                    [classify_quadrant(row) for _, row in gap_df.iterrows()]
                )),
                showlegend=False,
                name=""
            ))
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’å€‹åˆ¥ã«è¿½åŠ ï¼ˆé‡ãªã‚Šå›é¿ï¼‰
            for i, (_, row) in enumerate(gap_df.iterrows()):
                x, y = row['æº€è¶³åº¦'], row['æœŸå¾…åº¦']
                category = row['ã‚«ãƒ†ã‚´ãƒª']
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¨ˆç®—ï¼ˆè±¡é™ãƒ©ãƒ™ãƒ«ã¨ã®é‡ãªã‚Šã‚’é¿ã‘ã‚‹ï¼‰
                offset_map = {
                    "top center": (0, 0.2),
                    "bottom center": (0, -0.2),
                    "middle left": (-0.3, 0),
                    "middle right": (0.3, 0),
                    "top left": (-0.25, 0.2),
                    "top right": (0.25, 0.2),
                    "bottom left": (-0.25, -0.2),
                    "bottom right": (0.25, -0.2)
                }
                
                text_pos = text_positions[i]
                offset_x, offset_y = offset_map.get(text_pos, (0, 0.15))
                
                fig.add_annotation(
                    x=x + offset_x,
                    y=y + offset_y,
                    text=f"<b>{category}</b>",
                    showarrow=True,
                    arrowhead=2,
                    arrowsize=1,
                    arrowwidth=1,
                    arrowcolor=colors[i],
                    ax=0,
                    ay=-15 if "top" in text_pos else 15 if "bottom" in text_pos else 0,
                    font=dict(size=9, color='#1f2937'),
                    bgcolor='rgba(255, 255, 255, 0.8)',
                    bordercolor=colors[i],
                    borderwidth=1
                )
            
            # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š
            fig.update_layout(
                title={
                    'text': "æœŸå¾…åº¦ vs æº€è¶³åº¦ 4è±¡é™åˆ†æ",
                    'x': 0.5,
                    'font': {'size': 18, 'color': '#1f2937'}
                },
                xaxis=dict(
                    title="æº€è¶³åº¦ â†’",
                    range=[0.3, 5.7],
                    showgrid=True,
                    gridcolor='rgba(128, 128, 128, 0.2)',
                    dtick=1
                ),
                yaxis=dict(
                    title="â†‘ æœŸå¾…åº¦",
                    range=[0.3, 5.7],
                    showgrid=True,
                    gridcolor='rgba(128, 128, 128, 0.2)',
                    dtick=1
                ),
                height=650,
                plot_bgcolor='white',
                paper_bgcolor='white'
            )
            
            # è±¡é™ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ ï¼ˆé‡ãªã‚Šå›é¿ã®ãŸã‚å¤–å´ã«é…ç½®ï¼‰
            annotations = [
                dict(x=4.8, y=4.8, text="<b>ğŸ¯ ç†æƒ³çš„</b><br>(é«˜æº€è¶³ãƒ»é«˜æœŸå¾…)", 
                     showarrow=False, font=dict(size=10, color='#22543d'), 
                     bgcolor='rgba(72, 187, 120, 0.2)', bordercolor='#48BB78',
                     xanchor='center', yanchor='middle'),
                dict(x=1.2, y=4.8, text="<b>ğŸš¨ è¦æ”¹å–„</b><br>(ä½æº€è¶³ãƒ»é«˜æœŸå¾…)", 
                     showarrow=False, font=dict(size=10, color='#742a2a'), 
                     bgcolor='rgba(245, 101, 101, 0.2)', bordercolor='#F56565',
                     xanchor='center', yanchor='middle'),
                dict(x=4.8, y=1.2, text="<b>ğŸ’ æº€è¶³è¶…é</b><br>(é«˜æº€è¶³ãƒ»ä½æœŸå¾…)", 
                     showarrow=False, font=dict(size=10, color='#553c9a'), 
                     bgcolor='rgba(159, 122, 234, 0.2)', bordercolor='#9F7AEA',
                     xanchor='center', yanchor='middle'),
                dict(x=1.2, y=1.2, text="<b>ğŸ”„ æ©Ÿä¼šé ˜åŸŸ</b><br>(ä½æº€è¶³ãƒ»ä½æœŸå¾…)", 
                     showarrow=False, font=dict(size=10, color='#c05621'), 
                     bgcolor='rgba(237, 137, 54, 0.2)', bordercolor='#ED8936',
                     xanchor='center', yanchor='middle')
            ]
            
            for ann in annotations:
                fig.add_annotation(**ann)
            
            st.plotly_chart(fig, use_container_width=True)
            
            # è±¡é™åˆ¥ã®èª¬æ˜
            st.info("""
            **ğŸ“Š 4è±¡é™ã®è§£é‡ˆ**
            - ğŸ¯ **ç†æƒ³çš„ï¼ˆå³ä¸Šï¼‰**: æº€è¶³åº¦ãƒ»æœŸå¾…åº¦ã¨ã‚‚ã«é«˜ã„é …ç›®ã€‚ç¾çŠ¶ç¶­æŒãƒ»ã•ã‚‰ãªã‚‹å¼·åŒ–
            - ğŸš¨ **è¦æ”¹å–„ï¼ˆå·¦ä¸Šï¼‰**: æœŸå¾…ã¯é«˜ã„ãŒæº€è¶³åº¦ãŒä½ã„é …ç›®ã€‚æœ€å„ªå…ˆã§æ”¹å–„ãŒå¿…è¦
            - ğŸ’ **æº€è¶³è¶…éï¼ˆå³ä¸‹ï¼‰**: æº€è¶³åº¦ã¯é«˜ã„ãŒæœŸå¾…åº¦ãŒä½ã„é …ç›®ã€‚ã‚¢ãƒ”ãƒ¼ãƒ«ã‚„èªçŸ¥å‘ä¸Šã®æ©Ÿä¼š
            - ğŸ”„ **æ©Ÿä¼šé ˜åŸŸï¼ˆå·¦ä¸‹ï¼‰**: æœŸå¾…ãƒ»æº€è¶³ã¨ã‚‚ã«ä½ã„é …ç›®ã€‚å°†æ¥çš„ãªæ”¹å–„æ¤œè¨é ˜åŸŸ
            """)
            
            # ã‚®ãƒ£ãƒƒãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆæ”¹å–„ç‰ˆï¼‰
            st.subheader("ğŸ“‹ è±¡é™åˆ¥åˆ†æçµæœ")
            
            try:
                gap_display = gap_df.copy()
                gap_display['è±¡é™'] = gap_display.apply(classify_quadrant, axis=1)
                gap_display['ã‚®ãƒ£ãƒƒãƒ—è©•ä¾¡'] = gap_display['ã‚®ãƒ£ãƒƒãƒ—'].apply(
                    lambda x: 'ğŸ˜Š æº€è¶³>æœŸå¾…' if x > 0.3 else 'ğŸ˜” æœŸå¾…>æº€è¶³' if x < -0.3 else 'ğŸ˜ ã»ã¼åŒç­‰'
                )
                
                # å„ªå…ˆåº¦ã‚’è¨­å®š
                priority_map = {'ğŸš¨ è¦æ”¹å–„': 1, 'ğŸ”„ æ©Ÿä¼šé ˜åŸŸ': 2, 'ğŸ’ æº€è¶³è¶…é': 3, 'ğŸ¯ ç†æƒ³çš„': 4}
                gap_display['å„ªå…ˆåº¦'] = gap_display['è±¡é™'].map(priority_map)
                
                # NaNã®å‡¦ç†
                gap_display['å„ªå…ˆåº¦'] = gap_display['å„ªå…ˆåº¦'].fillna(5)
                
                # å¿…è¦ãªã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                required_columns = ['ã‚«ãƒ†ã‚´ãƒª', 'æº€è¶³åº¦', 'æœŸå¾…åº¦', 'ã‚®ãƒ£ãƒƒãƒ—', 'è±¡é™', 'ã‚®ãƒ£ãƒƒãƒ—è©•ä¾¡', 'å„ªå…ˆåº¦']
                if all(col in gap_display.columns for col in required_columns):
                    # è¡¨ç¤ºç”¨ã«æ•´ç†
                    display_df = gap_display[required_columns].sort_values('å„ªå…ˆåº¦')
                    display_df = display_df.round({'æº€è¶³åº¦': 1, 'æœŸå¾…åº¦': 1, 'ã‚®ãƒ£ãƒƒãƒ—': 2})
                    display_df = display_df.drop_duplicates().reset_index(drop=True)
                    
                    # å„ªå…ˆåº¦ã‚«ãƒ©ãƒ ã‚’é™¤å¤–ã—ã¦è¡¨ç¤º
                    st.dataframe(
                        display_df[['ã‚«ãƒ†ã‚´ãƒª', 'æº€è¶³åº¦', 'æœŸå¾…åº¦', 'ã‚®ãƒ£ãƒƒãƒ—', 'è±¡é™', 'ã‚®ãƒ£ãƒƒãƒ—è©•ä¾¡']], 
                        use_container_width=True,
                        hide_index=True
                    )
                else:
                    st.error("å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
                    
            except Exception as e:
                st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                st.info("åŸºæœ¬çš„ãªæº€è¶³åº¦ãƒ»æœŸå¾…åº¦ãƒ‡ãƒ¼ã‚¿ã¯åˆ©ç”¨å¯èƒ½ã§ã™ã€‚è©³ç´°ãªåˆ†æè¡¨ç¤ºã®ã¿ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚")
                
        else:
            st.warning("æœŸå¾…åº¦ã‚®ãƒ£ãƒƒãƒ—åˆ†æã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚æº€è¶³åº¦ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯æœŸå¾…åº¦ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")

def show_text_mining_analysis():
    """ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°åˆ†æã‚’è¡¨ç¤º"""
    st.header("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°åˆ†æ")
    
    # ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    with st.spinner("ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        comments = load_comment_data()
    
    if not comments:
        st.error("ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    # ã‚¿ãƒ–ã‚’ä½œæˆ
    tabs = st.tabs(["ğŸ“Š ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ", "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ“‹ ã‚³ãƒ¡ãƒ³ãƒˆä¸€è¦§"])
    
    with tabs[0]:  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
        st.subheader("ğŸ” é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ")
        
        comment_type = st.selectbox(
            "åˆ†æã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆç¨®åˆ¥ã‚’é¸æŠ:",
            list(comments.keys()),
            key="keyword_analysis_type"
        )
        
        if comment_type in comments and comments[comment_type]:
            keywords = extract_keywords_janome(comments[comment_type], max_features=20)
            
            if keywords:
                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
                keyword_df = pd.DataFrame(keywords, columns=['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'å‡ºç¾å›æ•°'])
                
                # æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º
                fig = px.bar(
                    keyword_df, 
                    x='å‡ºç¾å›æ•°', 
                    y='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰',
                    orientation='h',
                    title=f"{comment_type} - é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
                    color='å‡ºç¾å›æ•°',
                    color_continuous_scale='viridis'
                )
                fig.update_layout(height=600, yaxis={'categoryorder':'total ascending'})
                st.plotly_chart(fig, use_container_width=True)
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
                st.subheader("ğŸ“‹ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è©³ç´°")
                st.dataframe(keyword_df, use_container_width=True, hide_index=True)
            else:
                st.info("æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            st.info(f"{comment_type}ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    with tabs[1]:  # å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        st.subheader("ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ")
        
        comment_type = st.selectbox(
            "åˆ†æã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆç¨®åˆ¥ã‚’é¸æŠ:",
            list(comments.keys()),
            key="network_analysis_type"
        )
        
        min_cooccurrence = st.slider("æœ€å°å…±èµ·å›æ•°", 1, 5, 1)
        
        if comment_type in comments and comments[comment_type]:
            G, cooccurrence = build_cooccurrence_network(comments[comment_type], min_cooccurrence)
            
            if G and len(G.nodes()) > 0:
                # NetworkXã‚’ä½¿ã£ã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’ä½œæˆ
                pos = nx.spring_layout(G, k=3, iterations=50)
                
                # ã‚¨ãƒƒã‚¸ã®æƒ…å ±ã‚’å–å¾—
                edge_x = []
                edge_y = []
                edge_info = []
                
                for edge in G.edges():
                    x0, y0 = pos[edge[0]]
                    x1, y1 = pos[edge[1]]
                    edge_x.extend([x0, x1, None])
                    edge_y.extend([y0, y1, None])
                    weight = G[edge[0]][edge[1]]['weight']
                    edge_info.append(f"{edge[0]} - {edge[1]}: {weight}å›")
                
                # ãƒãƒ¼ãƒ‰ã®æƒ…å ±ã‚’å–å¾—
                node_x = []
                node_y = []
                node_text = []
                node_size = []
                
                for node in G.nodes():
                    x, y = pos[node]
                    node_x.append(x)
                    node_y.append(y)
                    node_text.append(node)
                    node_size.append(10 + G.degree(node) * 5)
                
                # Plotlyã§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’ä½œæˆ
                fig = go.Figure()
                
                # ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
                fig.add_trace(go.Scatter(
                    x=edge_x, y=edge_y,
                    line=dict(width=2, color='#888'),
                    hoverinfo='none',
                    mode='lines',
                    showlegend=False
                ))
                
                # ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
                fig.add_trace(go.Scatter(
                    x=node_x, y=node_y,
                    mode='markers+text',
                    hoverinfo='text',
                    text=node_text,
                    textposition="middle center",
                    textfont=dict(size=12, color='white'),
                    marker=dict(
                        size=node_size,
                        color=node_size,
                        colorscale='viridis',
                        line=dict(width=2, color='white')
                    ),
                    showlegend=False
                ))
                
                fig.update_layout(
                    title=f"{comment_type} - å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³",
                    showlegend=False,
                    hovermode='closest',
                    margin=dict(b=20,l=5,r=5,t=40),
                    annotations=[ dict(
                        text="ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚º: æ¥ç¶šæ•°ã€ç·š: å…±èµ·é–¢ä¿‚",
                        showarrow=False,
                        xref="paper", yref="paper",
                        x=0.005, y=-0.002 ,
                        xanchor="left", yanchor="bottom",
                        font=dict(color="gray", size=12)
                    )],
                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                    height=600
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # å…±èµ·é–¢ä¿‚ã®è©³ç´°
                st.subheader("ğŸ“Š å…±èµ·é–¢ä¿‚è©³ç´°")
                cooccurrence_df = pd.DataFrame([
                    {'èª1': pair[0], 'èª2': pair[1], 'å…±èµ·å›æ•°': count}
                    for pair, count in sorted(cooccurrence.items(), key=lambda x: x[1], reverse=True)
                ])
                st.dataframe(cooccurrence_df, use_container_width=True, hide_index=True)
            else:
                st.info("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æœ€å°å…±èµ·å›æ•°ã‚’ä¸‹ã’ã¦ã¿ã¦ãã ã•ã„ã€‚")
        else:
            st.info(f"{comment_type}ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    with tabs[2]:  # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
        st.subheader("â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
        
        comment_type = st.selectbox(
            "åˆ†æã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆç¨®åˆ¥ã‚’é¸æŠ:",
            list(comments.keys()),
            key="wordcloud_analysis_type"
        )
        
        if comment_type in comments and comments[comment_type]:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            keywords = extract_keywords_janome(comments[comment_type], max_features=50)
            
            if keywords:
                # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
                wordcloud_dict = dict(keywords)
                
                try:
                    # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ
                    wc = WordCloud(
                        font_path=None,  # ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨
                        width=800, height=400,
                        background_color='white',
                        max_words=50,
                        colormap='viridis'
                    ).generate_from_frequencies(wordcloud_dict)
                    
                    # matplotlib ã§è¡¨ç¤º
                    fig, ax = plt.subplots(figsize=(12, 6))
                    ax.imshow(wc, interpolation='bilinear')
                    ax.axis('off')
                    ax.set_title(f'{comment_type} - ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰', fontsize=16, pad=20)
                    
                    st.pyplot(fig)
                    
                except Exception as e:
                    st.warning(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                    st.info("ä»£ã‚ã‚Šã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’è¡¨ç¤ºã—ã¾ã™:")
                    keyword_df = pd.DataFrame(keywords, columns=['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'å‡ºç¾å›æ•°'])
                    st.dataframe(keyword_df, use_container_width=True, hide_index=True)
            else:
                st.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.info(f"{comment_type}ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    with tabs[3]:  # ã‚³ãƒ¡ãƒ³ãƒˆä¸€è¦§
        st.subheader("ğŸ“‹ ã‚³ãƒ¡ãƒ³ãƒˆä¸€è¦§")
        
        comment_type = st.selectbox(
            "è¡¨ç¤ºã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆç¨®åˆ¥ã‚’é¸æŠ:",
            list(comments.keys()),
            key="comment_list_type"
        )
        
        if comment_type in comments and comments[comment_type]:
            st.write(f"**{comment_type}** ({len(comments[comment_type])}ä»¶)")
            
            for i, comment in enumerate(comments[comment_type], 1):
                with st.expander(f"ã‚³ãƒ¡ãƒ³ãƒˆ {i}"):
                    st.write(comment)
        else:
            st.info(f"{comment_type}ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

def show_time_series_analysis():
    """KPIæ™‚ç³»åˆ—åˆ†æã‚’è¡¨ç¤º"""
    st.header("ğŸ“ˆ KPIæ™‚ç³»åˆ—åˆ†æ")
    
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    with st.spinner("ğŸ“Š èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        data = load_employee_data()
        
    if not data or 'employee_data' not in data:
        st.error("èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    # ç¾åœ¨ã¯1å›åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã—ã‹ãªã„ãŸã‚ã€ãƒ‡ãƒ¢ç”¨ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    st.info("ğŸ“Š ç¾åœ¨ã¯1å›åˆ†ã®èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ã®ã¿ã®ãŸã‚ã€ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¾ã™")
    
    # ãƒ‡ãƒ¢ç”¨ã®æ™‚ç³»åˆ—KPIãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    monthly_kpi_data = create_dummy_monthly_kpi_data()
    
    # ãƒ¡ã‚¤ãƒ³æŒ‡æ¨™ã®æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•
    st.subheader("ğŸ“ˆ ä¸»è¦KPIæŒ‡æ¨™ã®æœˆåˆ¥æ¨ç§»")
    
    # 4ã¤ã®ä¸»è¦æŒ‡æ¨™ã‚’2x2ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§è¡¨ç¤º
    col1, col2 = st.columns(2)
    
    with col1:
        # eNPSæ¨ç§»
        fig_nps = px.line(
            monthly_kpi_data,
            x='å¹´æœˆ',
            y='eNPS',
            title='eNPS (Employee Net Promoter Score) æ¨ç§»',
            markers=True,
            color_discrete_sequence=['#FF6B6B']
        )
        fig_nps.update_layout(
            height=300,
            yaxis_title='eNPS (%)',
            xaxis_title='å¹´æœˆ'
        )
        fig_nps.add_hline(y=0, line_dash="dash", line_color="gray", annotation_text="åŸºæº–ç·š (0)")
        st.plotly_chart(fig_nps, use_container_width=True)
        
        # æ´»èºè²¢çŒ®åº¦æ¨ç§»
        fig_contribution = px.line(
            monthly_kpi_data,
            x='å¹´æœˆ',
            y='æ´»èºè²¢çŒ®åº¦',
            title='æ´»èºè²¢çŒ®åº¦ æ¨ç§»',
            markers=True,
            color_discrete_sequence=['#4ECDC4']
        )
        fig_contribution.update_layout(
            height=300,
            yaxis_title='æ´»èºè²¢çŒ®åº¦ (1-5ç‚¹)',
            xaxis_title='å¹´æœˆ',
            yaxis=dict(range=[1, 5])
        )
        st.plotly_chart(fig_contribution, use_container_width=True)
    
    with col2:
        # ç·åˆæº€è¶³åº¦æ¨ç§»
        fig_satisfaction = px.line(
            monthly_kpi_data,
            x='å¹´æœˆ',
            y='ç·åˆæº€è¶³åº¦',
            title='ç·åˆæº€è¶³åº¦ æ¨ç§»',
            markers=True,
            color_discrete_sequence=['#45B7D1']
        )
        fig_satisfaction.update_layout(
            height=300,
            yaxis_title='ç·åˆæº€è¶³åº¦ (1-5ç‚¹)',
            xaxis_title='å¹´æœˆ',
            yaxis=dict(range=[1, 5])
        )
        st.plotly_chart(fig_satisfaction, use_container_width=True)
        
        # å‹¤ç¶šæ„å‘æ¨ç§»
        fig_retention = px.line(
            monthly_kpi_data,
            x='å¹´æœˆ',
            y='å‹¤ç¶šæ„å‘',
            title='å‹¤ç¶šæ„å‘ æ¨ç§»',
            markers=True,
            color_discrete_sequence=['#96CEB4']
        )
        fig_retention.update_layout(
            height=300,
            yaxis_title='å‹¤ç¶šæ„å‘ (1-5ç‚¹)',
            xaxis_title='å¹´æœˆ',
            yaxis=dict(range=[1, 5])
        )
        st.plotly_chart(fig_retention, use_container_width=True)
    
    # å…¨æŒ‡æ¨™ã‚’1ã¤ã®ã‚°ãƒ©ãƒ•ã§æ¯”è¼ƒ
    st.subheader("ğŸ“Š ä¸»è¦æŒ‡æ¨™æ¯”è¼ƒ (æ­£è¦åŒ–)")
    
    # æ­£è¦åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆ0-1ã®ç¯„å›²ã«æ­£è¦åŒ–ï¼‰
    normalized_data = monthly_kpi_data.copy()
    normalized_data['eNPS_æ­£è¦åŒ–'] = (normalized_data['eNPS'] + 100) / 200  # -100~100 â†’ 0~1
    normalized_data['ç·åˆæº€è¶³åº¦_æ­£è¦åŒ–'] = (normalized_data['ç·åˆæº€è¶³åº¦'] - 1) / 4  # 1~5 â†’ 0~1
    normalized_data['æ´»èºè²¢çŒ®åº¦_æ­£è¦åŒ–'] = (normalized_data['æ´»èºè²¢çŒ®åº¦'] - 1) / 4  # 1~5 â†’ 0~1
    normalized_data['å‹¤ç¶šæ„å‘_æ­£è¦åŒ–'] = (normalized_data['å‹¤ç¶šæ„å‘'] - 1) / 4  # 1~5 â†’ 0~1
    
    # æ¯”è¼ƒã‚°ãƒ©ãƒ•
    fig_compare = go.Figure()
    
    fig_compare.add_trace(go.Scatter(
        x=normalized_data['å¹´æœˆ'],
        y=normalized_data['eNPS_æ­£è¦åŒ–'],
        mode='lines+markers',
        name='eNPS',
        line=dict(color='#FF6B6B')
    ))
    
    fig_compare.add_trace(go.Scatter(
        x=normalized_data['å¹´æœˆ'],
        y=normalized_data['ç·åˆæº€è¶³åº¦_æ­£è¦åŒ–'],
        mode='lines+markers',
        name='ç·åˆæº€è¶³åº¦',
        line=dict(color='#45B7D1')
    ))
    
    fig_compare.add_trace(go.Scatter(
        x=normalized_data['å¹´æœˆ'],
        y=normalized_data['æ´»èºè²¢çŒ®åº¦_æ­£è¦åŒ–'],
        mode='lines+markers',
        name='æ´»èºè²¢çŒ®åº¦',
        line=dict(color='#4ECDC4')
    ))
    
    fig_compare.add_trace(go.Scatter(
        x=normalized_data['å¹´æœˆ'],
        y=normalized_data['å‹¤ç¶šæ„å‘_æ­£è¦åŒ–'],
        mode='lines+markers',
        name='å‹¤ç¶šæ„å‘',
        line=dict(color='#96CEB4')
    ))
    
    fig_compare.update_layout(
        title='ä¸»è¦KPIæŒ‡æ¨™ã®æ¨ç§»æ¯”è¼ƒ (æ­£è¦åŒ–æ¸ˆã¿)',
        xaxis_title='å¹´æœˆ',
        yaxis_title='æ­£è¦åŒ–å€¤ (0-1)',
        height=400,
        hovermode='x unified'
    )
    
    st.plotly_chart(fig_compare, use_container_width=True)
    
    # æœˆåˆ¥è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
    st.subheader("ğŸ“‹ æœˆåˆ¥KPIè©³ç´°ãƒ‡ãƒ¼ã‚¿")
    
    # è¡¨ç¤ºç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
    display_data = monthly_kpi_data.copy()
    display_data['eNPS'] = display_data['eNPS'].round(1)
    display_data['ç·åˆæº€è¶³åº¦'] = display_data['ç·åˆæº€è¶³åº¦'].round(2)
    display_data['æ´»èºè²¢çŒ®åº¦'] = display_data['æ´»èºè²¢çŒ®åº¦'].round(2)
    display_data['å‹¤ç¶šæ„å‘'] = display_data['å‹¤ç¶šæ„å‘'].round(2)
    display_data['å›ç­”è€…æ•°'] = display_data['å›ç­”è€…æ•°'].astype(int)
    
    st.dataframe(display_data, use_container_width=True, hide_index=True)
    
    # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
    st.subheader("ğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        nps_trend = monthly_kpi_data['eNPS'].iloc[-1] - monthly_kpi_data['eNPS'].iloc[0]
        st.metric(
            "eNPSå¤‰åŒ–", 
            f"{monthly_kpi_data['eNPS'].iloc[-1]:.1f}%",
            delta=f"{nps_trend:+.1f}%"
        )
    
    with col2:
        satisfaction_trend = monthly_kpi_data['ç·åˆæº€è¶³åº¦'].iloc[-1] - monthly_kpi_data['ç·åˆæº€è¶³åº¦'].iloc[0]
        st.metric(
            "ç·åˆæº€è¶³åº¦å¤‰åŒ–",
            f"{monthly_kpi_data['ç·åˆæº€è¶³åº¦'].iloc[-1]:.2f}ç‚¹",
            delta=f"{satisfaction_trend:+.2f}ç‚¹"
        )
    
    with col3:
        contribution_trend = monthly_kpi_data['æ´»èºè²¢çŒ®åº¦'].iloc[-1] - monthly_kpi_data['æ´»èºè²¢çŒ®åº¦'].iloc[0]
        st.metric(
            "æ´»èºè²¢çŒ®åº¦å¤‰åŒ–",
            f"{monthly_kpi_data['æ´»èºè²¢çŒ®åº¦'].iloc[-1]:.2f}ç‚¹",
            delta=f"{contribution_trend:+.2f}ç‚¹"
        )
    
    with col4:
        retention_trend = monthly_kpi_data['å‹¤ç¶šæ„å‘'].iloc[-1] - monthly_kpi_data['å‹¤ç¶šæ„å‘'].iloc[0]
        st.metric(
            "å‹¤ç¶šæ„å‘å¤‰åŒ–",
            f"{monthly_kpi_data['å‹¤ç¶šæ„å‘'].iloc[-1]:.2f}ç‚¹",
            delta=f"{retention_trend:+.2f}ç‚¹"
        )

def create_dummy_monthly_kpi_data():
    """ãƒ‡ãƒ¢ç”¨ã®æœˆåˆ¥KPIãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    np.random.seed(42)
    
    # éå»12ãƒ¶æœˆã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    months = []
    base_date = datetime.now().replace(day=1) - timedelta(days=365)
    
    for i in range(12):
        current_date = base_date + timedelta(days=30*i)
        months.append(current_date.strftime('%Y-%m'))
    
    # åŸºæº–å€¤ã‹ã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    base_enps = -10  # åŸºæº–eNPS
    base_satisfaction = 3.2  # åŸºæº–æº€è¶³åº¦
    base_contribution = 3.5  # åŸºæº–æ´»èºè²¢çŒ®åº¦
    base_retention = 3.1  # åŸºæº–å‹¤ç¶šæ„å‘
    
    monthly_data = []
    
    for i, month in enumerate(months):
        # å­£ç¯€æ€§ã¨ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å«ã‚“ã ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        seasonal_factor = 0.1 * np.sin(2 * np.pi * i / 12)  # å­£ç¯€å¤‰å‹•
        trend_factor = i * 0.02  # ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰
        
        # ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚º
        noise_enps = np.random.normal(0, 5)
        noise_satisfaction = np.random.normal(0, 0.1)
        noise_contribution = np.random.normal(0, 0.1)
        noise_retention = np.random.normal(0, 0.1)
        
        enps = base_enps + trend_factor * 50 + seasonal_factor * 10 + noise_enps
        satisfaction = base_satisfaction + trend_factor * 2 + seasonal_factor * 0.2 + noise_satisfaction
        contribution = base_contribution + trend_factor * 1.5 + seasonal_factor * 0.15 + noise_contribution
        retention = base_retention + trend_factor * 1.8 + seasonal_factor * 0.18 + noise_retention
        
        # å€¤ã®ç¯„å›²åˆ¶é™
        enps = max(-100, min(100, enps))
        satisfaction = max(1, min(5, satisfaction))
        contribution = max(1, min(5, contribution))
        retention = max(1, min(5, retention))
        
        monthly_data.append({
            'å¹´æœˆ': month,
            'eNPS': enps,
            'ç·åˆæº€è¶³åº¦': satisfaction,
            'æ´»èºè²¢çŒ®åº¦': contribution,
            'å‹¤ç¶šæ„å‘': retention,
            'å›ç­”è€…æ•°': np.random.randint(45, 55)  # å›ç­”è€…æ•°ã‚‚å¤‰å‹•
        })
    
    return pd.DataFrame(monthly_data)

def show_department_analysis(data, kpis):
    """éƒ¨ç½²åˆ¥åˆ†æã‚’è¡¨ç¤º"""
    st.header("ğŸ¢ éƒ¨ç½²åˆ¥ãƒ»è©³ç´°åˆ†æ")
    
    if 'employee_data' not in data:
        st.error("éƒ¨ç½²åˆ¥ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return
    
    df = data['employee_data']
    
    # ã‚¿ãƒ–ã‚’ä½œæˆã—ã¦å†…å®¹ã‚’æ•´ç†
    tabs = st.tabs(["ğŸ“Š éƒ¨ç½²åˆ¥åˆ†æ", "ğŸ’ª å¼·ã¿ãƒ»å¼±ã¿åˆ†æ"])
    
    with tabs[0]:  # éƒ¨ç½²åˆ¥åˆ†æã‚¿ãƒ–
        # éƒ¨ç½²åˆ¥çµ±è¨ˆ
        if 'department' in df.columns:
            dept_stats = df.groupby('department').agg({
                'overall_satisfaction': 'mean',
                'nps_score': 'mean',
                'contribution_score': 'mean',
                'long_term_intention': 'mean',
                'annual_salary': 'mean',
                'monthly_overtime': 'mean',
                'response_id': 'count'
            }).round(2)
            
            dept_stats.columns = ['ç·åˆæº€è¶³åº¦', 'NPS', 'æ´»èºè²¢çŒ®åº¦', 'å‹¤ç¶šæ„å‘', 'å¹³å‡å¹´å', 'å¹³å‡æ®‹æ¥­æ™‚é–“', 'å›ç­”è€…æ•°']
            
            # éƒ¨ç½²åˆ¥æº€è¶³åº¦æ¯”è¼ƒ
            fig = px.bar(
                dept_stats.reset_index(),
                x='department',
                y='ç·åˆæº€è¶³åº¦',
                title='éƒ¨ç½²åˆ¥ç·åˆæº€è¶³åº¦',
                color='ç·åˆæº€è¶³åº¦',
                color_continuous_scale='RdYlGn',
                text='ç·åˆæº€è¶³åº¦'
            )
            fig.update_traces(texttemplate='%{text:.2f}', textposition='outside')
            fig.update_layout(height=400, xaxis_title='éƒ¨ç½²', yaxis_title='æº€è¶³åº¦')
            st.plotly_chart(fig, use_container_width=True)
            
            # è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
            st.subheader("éƒ¨ç½²åˆ¥è©³ç´°ãƒ‡ãƒ¼ã‚¿")
            st.dataframe(dept_stats, use_container_width=True)
        else:
            st.info("éƒ¨ç½²æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€å€‹åˆ¥å›ç­”è€…ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¾ã™")
            
            # å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            display_cols = ['response_id', 'overall_satisfaction', 'nps_score', 'contribution_score', 
                           'annual_salary', 'monthly_overtime', 'paid_leave_rate']
            available_cols = [col for col in display_cols if col in df.columns]
            
            if available_cols:
                st.dataframe(df[available_cols], use_container_width=True)
    
    with tabs[1]:  # å¼·ã¿ãƒ»å¼±ã¿åˆ†æã‚¿ãƒ–
        show_strengths_weaknesses_analysis(data, kpis)

def show_strengths_weaknesses_analysis(data, kpis):
    """å¼·ã¿ãƒ»å¼±ã¿åˆ†æã‚’è¡¨ç¤º"""
    st.subheader("ğŸ’ª çµ„ç¹”ã®å¼·ã¿ãƒ»å¼±ã¿åˆ†æ")
    
    if 'satisfaction_by_category' not in kpis or not kpis['satisfaction_by_category']:
        st.warning("æº€è¶³åº¦ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return
    
    satisfaction_data = kpis['satisfaction_by_category']
    
    # æº€è¶³åº¦ã‚’é™é †ã§ã‚½ãƒ¼ãƒˆ
    sorted_satisfaction = sorted(satisfaction_data.items(), key=lambda x: x[1], reverse=True)
    
    # TOP5ã¨BOTTOM5ã‚’æŠ½å‡º
    top5_strengths = sorted_satisfaction[:5]
    bottom5_weaknesses = sorted_satisfaction[-5:]
    
    # 2åˆ—ã«åˆ†ã‘ã¦è¡¨ç¤º
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### ğŸŒŸ **TOP5 å¼·ã¿é ˜åŸŸ**")
        
        # å¼·ã¿é ˜åŸŸã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        strengths_df = pd.DataFrame(top5_strengths, columns=['ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'æº€è¶³åº¦'])
        strengths_df['æº€è¶³åº¦'] = strengths_df['æº€è¶³åº¦'].round(2)
        
        # å¼·ã¿é ˜åŸŸã®æ£’ã‚°ãƒ©ãƒ•
        fig_strengths = px.bar(
            strengths_df,
            x='æº€è¶³åº¦',
            y='ã‚«ãƒ†ã‚´ãƒªãƒ¼',
            orientation='h',
            title='çµ„ç¹”ã®å¼·ã¿ TOP5',
            color='æº€è¶³åº¦',
            color_continuous_scale='Greens',
            text='æº€è¶³åº¦',
            height=400
        )
        fig_strengths.update_traces(texttemplate='%{text:.2f}', textposition='outside')
        fig_strengths.update_layout(
            yaxis={'categoryorder':'total ascending'},
            xaxis=dict(range=[0, 5])
        )
        st.plotly_chart(fig_strengths, use_container_width=True)
        
        # å¼·ã¿è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
        st.markdown("#### ğŸ“ˆ å¼·ã¿è©³ç´°")
        for i, (category, score) in enumerate(top5_strengths, 1):
            st.markdown(f"**{i}ä½** {category}: **{score:.2f}ç‚¹**")
    
    with col2:
        st.markdown("### âš ï¸ **æ”¹å–„è¦æœ› TOP5**")
        
        # å¼±ã¿é ˜åŸŸã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆï¼ˆæ˜‡é †ã§è¡¨ç¤ºï¼‰
        weaknesses_df = pd.DataFrame(bottom5_weaknesses, columns=['ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'æº€è¶³åº¦'])
        weaknesses_df['æº€è¶³åº¦'] = weaknesses_df['æº€è¶³åº¦'].round(2)
        
        # å¼±ã¿é ˜åŸŸã®æ£’ã‚°ãƒ©ãƒ•
        fig_weaknesses = px.bar(
            weaknesses_df,
            x='æº€è¶³åº¦',
            y='ã‚«ãƒ†ã‚´ãƒªãƒ¼',
            orientation='h',
            title='æ”¹å–„è¦æœ› TOP5',
            color='æº€è¶³åº¦',
            color_continuous_scale='Reds',
            text='æº€è¶³åº¦',
            height=400
        )
        fig_weaknesses.update_traces(texttemplate='%{text:.2f}', textposition='outside')
        fig_weaknesses.update_layout(
            yaxis={'categoryorder':'total ascending'},
            xaxis=dict(range=[0, 5])
        )
        st.plotly_chart(fig_weaknesses, use_container_width=True)
        
        # å¼±ã¿è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
        st.markdown("#### ğŸ“‰ æ”¹å–„è¦æœ›è©³ç´°")
        for i, (category, score) in enumerate(reversed(bottom5_weaknesses), 1):
            st.markdown(f"**{i}ä½** {category}: **{score:.2f}ç‚¹**")
    
    # å…¨ä½“ã®æº€è¶³åº¦åˆ†å¸ƒ
    st.markdown("---")
    st.subheader("ğŸ“Š å…¨ã‚«ãƒ†ã‚´ãƒªãƒ¼æº€è¶³åº¦åˆ†å¸ƒ")
    
    # å…¨ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    all_categories_df = pd.DataFrame(sorted_satisfaction, columns=['ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'æº€è¶³åº¦'])
    all_categories_df['æº€è¶³åº¦'] = all_categories_df['æº€è¶³åº¦'].round(2)
    all_categories_df['é †ä½'] = range(1, len(all_categories_df) + 1)
    
    # è‰²åˆ†ã‘ç”¨ã®åˆ—ã‚’è¿½åŠ 
    all_categories_df['é ˜åŸŸ'] = all_categories_df.apply(
        lambda row: 'å¼·ã¿' if row['é †ä½'] <= 5 else ('æ”¹å–„è¦æœ›' if row['é †ä½'] > len(all_categories_df) - 5 else 'æ¨™æº–'), 
        axis=1
    )
    
    # å…¨ä½“ã®æ£’ã‚°ãƒ©ãƒ•
    fig_all = px.bar(
        all_categories_df,
        x='æº€è¶³åº¦',
        y='ã‚«ãƒ†ã‚´ãƒªãƒ¼',
        orientation='h',
        title='å…¨ã‚«ãƒ†ã‚´ãƒªãƒ¼æº€è¶³åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°',
        color='é ˜åŸŸ',
        color_discrete_map={
            'å¼·ã¿': '#2E8B57',
            'æ¨™æº–': '#4682B4', 
            'æ”¹å–„è¦æœ›': '#DC143C'
        },
        text='æº€è¶³åº¦',
        height=600
    )
    fig_all.update_traces(texttemplate='%{text:.2f}', textposition='outside')
    fig_all.update_layout(
        yaxis={'categoryorder':'total ascending'},
        xaxis=dict(range=[0, 5])
    )
    st.plotly_chart(fig_all, use_container_width=True)
    
    # æº€è¶³åº¦ã‚µãƒãƒªãƒ¼
    avg_satisfaction = sum(satisfaction_data.values()) / len(satisfaction_data)
    st.markdown("---")
    st.markdown("### ğŸ“‹ æº€è¶³åº¦ã‚µãƒãƒªãƒ¼")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("å¹³å‡æº€è¶³åº¦", f"{avg_satisfaction:.2f}ç‚¹", delta=f"{avg_satisfaction - 3:.2f}")
    with col2:
        top_score = top5_strengths[0][1]
        st.metric("æœ€é«˜æº€è¶³åº¦", f"{top_score:.2f}ç‚¹", delta=f"{top_score - avg_satisfaction:.2f}")
    with col3:
        bottom_score = bottom5_weaknesses[0][1]
        st.metric("æœ€ä½æº€è¶³åº¦", f"{bottom_score:.2f}ç‚¹", delta=f"{bottom_score - avg_satisfaction:.2f}")

def main():
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.markdown("""
        <div class="sidebar-header">
            <div class="sidebar-logo">
                <span style='color: #667eea; font-size: 24px;'>ğŸ‘¥</span>
            </div>
            <div style='color: white; font-weight: bold; font-size: 18px;'>å¾“æ¥­å“¡èª¿æŸ»åˆ†æ</div>
        </div>
        """, unsafe_allow_html=True)
        
        st.divider()
        
        # ãƒšãƒ¼ã‚¸é¸æŠ
        page = st.radio(
            "ğŸ“‹ åˆ†æãƒšãƒ¼ã‚¸é¸æŠ",
            ["ğŸ“Š KPIæ¦‚è¦", "ğŸ“ˆ æº€è¶³åº¦åˆ†æ", "ğŸ¢ è©³ç´°åˆ†æ", "ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°", "â° æ™‚ç³»åˆ—åˆ†æ"],
            index=0
        )
        
        st.divider()
        
        # ãƒ‡ãƒ¼ã‚¿æ›´æ–°ãƒœã‚¿ãƒ³
        if st.button("ğŸ”„ ãƒ‡ãƒ¼ã‚¿æ›´æ–°", use_container_width=True):
            st.cache_data.clear()
            st.rerun()
        
        st.info("ğŸ’¡ Excelãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°å¾Œã¯ã€Œãƒ‡ãƒ¼ã‚¿æ›´æ–°ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„")
        
        st.divider()
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        st.subheader("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        if st.button("ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›", use_container_width=True):
            st.success("å®Ÿè£…æ™‚ã«ã¯åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’PDF/Excelã§å‡ºåŠ›ã§ãã¾ã™")
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    st.title("ğŸ‘¥ å¾“æ¥­å“¡èª¿æŸ»å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    st.markdown("---")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with st.spinner("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        data = load_employee_data()
        kpis = calculate_kpis(data)
    
    # ãƒšãƒ¼ã‚¸è¡¨ç¤º
    if page == "ğŸ“Š KPIæ¦‚è¦":
        show_kpi_overview(data, kpis)
    elif page == "ğŸ“ˆ æº€è¶³åº¦åˆ†æ":
        show_satisfaction_analysis(data, kpis)
    elif page == "ğŸ¢ è©³ç´°åˆ†æ":
        show_department_analysis(data, kpis)
    elif page == "ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°":
        show_text_mining_analysis()
    elif page == "â° æ™‚ç³»åˆ—åˆ†æ":
        show_time_series_analysis()

if __name__ == "__main__":
    main()