#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ç‰ˆ
å¾“æ¥­å“¡æº€è¶³åº¦äºˆæ¸¬ã¨ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æã®ãŸã‚ã®æ©Ÿæ¢°å­¦ç¿’ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="AI ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ—¥æœ¬èªã®è‡ªç„¶è¨€èªå‡¦ç†ç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import janome
    from janome.tokenizer import Tokenizer
    TOKENIZER_TYPE = "janome"
    tokenizer = Tokenizer()
    st.sidebar.success("âœ… Janomeå½¢æ…‹ç´ è§£æã‚¨ãƒ³ã‚¸ãƒ³åˆ©ç”¨å¯èƒ½")
except ImportError:
    TOKENIZER_TYPE = "simple"
    st.sidebar.warning("âš ï¸ ã‚·ãƒ³ãƒ—ãƒ«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨")

def japanese_tokenizer(text):
    """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®å½¢æ…‹ç´ è§£æ"""
    if not text or pd.isna(text):
        return []
    
    text = str(text).strip()
    if not text:
        return []
    
    try:
        if TOKENIZER_TYPE == "janome":
            tokens = [token.surface for token in tokenizer.tokenize(text, wakati=True)]
        else:
            # ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†å‰²ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            tokens = re.findall(r'\w+', text)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_tokens = []
        for token in tokens:
            if len(token) >= 2 and not token.isdigit():
                filtered_tokens.append(token)
        
        return filtered_tokens
    except Exception:
        return []

@st.cache_data
def create_enhanced_sample_data(n_samples=200):
    """å¼·åŒ–ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
    np.random.seed(42)
    
    data = []
    
    # ä½æº€è¶³åº¦ã‚°ãƒ«ãƒ¼ãƒ— (ä¸‹ä½20% - ãƒ©ãƒ™ãƒ«1)
    low_satisfaction_samples = int(n_samples * 0.2)
    for i in range(low_satisfaction_samples):
        recommend_score = np.random.choice([0, 1, 2, 3, 4, 5, 6], p=[0.1, 0.15, 0.2, 0.25, 0.15, 0.1, 0.05])
        overall_satisfaction = np.random.choice([1, 2, 3], p=[0.4, 0.4, 0.2])
        long_term_intention = np.random.choice([1, 2, 3], p=[0.5, 0.3, 0.2])
        sense_of_contribution = np.random.choice([1, 2, 3], p=[0.4, 0.4, 0.2])
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªä¸æº€ã‚³ãƒ¡ãƒ³ãƒˆ
        negative_templates = [
            "ä¸Šå¸ã¨ã®{keyword1}ã«å•é¡ŒãŒã‚ã‚Šã€{keyword2}ã‚’æ„Ÿã˜ã¦ã„ã¾ã™ã€‚æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚",
            "{keyword1}ã®åˆ¶åº¦ã«ä¸æº€ãŒã‚ã‚Šã¾ã™ã€‚ç‰¹ã«{keyword2}ã®ç‚¹ã§èª²é¡ŒãŒã‚ã‚Šã¾ã™ã€‚",
            "è·å ´ã®{keyword1}ãŒå³ã—ãã€{keyword2}ã«è² æ‹…ã‚’æ„Ÿã˜ã¦ã„ã¾ã™ã€‚",
            "{keyword1}ã«ã¤ã„ã¦ã®æœŸå¾…ã¨ç¾å®Ÿã«ã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚Šã€{keyword2}ã‚’æ„Ÿã˜ã¦ã„ã¾ã™ã€‚",
            "ä¼šç¤¾ã®{keyword1}ã«é–¢ã™ã‚‹æ–¹é‡ãŒä¸æ˜ç¢ºã§ã€{keyword2}ã«ãªã£ã¦ã„ã¾ã™ã€‚"
        ]
        
        negative_keywords1 = ["äººé–“é–¢ä¿‚", "è©•ä¾¡åˆ¶åº¦", "åŠ´åƒç’°å¢ƒ", "æ¥­å‹™é‡", "ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹", "çµ¦ä¸ä½“ç³»", "ç¦åˆ©åšç”Ÿ"]
        negative_keywords2 = ["ä¸å®‰", "ã‚¹ãƒˆãƒ¬ã‚¹", "ä¸æº€", "ç–²åŠ´", "å›°æƒ‘", "å¤±æœ›", "å¿ƒé…"]
        
        template = np.random.choice(negative_templates)
        keyword1 = np.random.choice(negative_keywords1)
        keyword2 = np.random.choice(negative_keywords2)
        comment = template.format(keyword1=keyword1, keyword2=keyword2)
        
        data.append({
            'recommend_score': recommend_score,
            'overall_satisfaction': overall_satisfaction,
            'long_term_intention': long_term_intention,
            'sense_of_contribution': sense_of_contribution,
            'comment': comment,
            'is_low_satisfaction': 1
        })
    
    # ä¸­ãƒ»é«˜æº€è¶³åº¦ã‚°ãƒ«ãƒ¼ãƒ— (ä¸Šä½80% - ãƒ©ãƒ™ãƒ«0)
    high_satisfaction_samples = n_samples - low_satisfaction_samples
    for i in range(high_satisfaction_samples):
        recommend_score = np.random.choice([6, 7, 8, 9, 10], p=[0.1, 0.2, 0.3, 0.25, 0.15])
        overall_satisfaction = np.random.choice([3, 4, 5], p=[0.3, 0.4, 0.3])
        long_term_intention = np.random.choice([3, 4, 5], p=[0.3, 0.4, 0.3])
        sense_of_contribution = np.random.choice([3, 4, 5], p=[0.3, 0.4, 0.3])
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªæº€è¶³ã‚³ãƒ¡ãƒ³ãƒˆ
        positive_templates = [
            "{keyword1}ã«æº€è¶³ã—ã¦ãŠã‚Šã€{keyword2}ã‚’æ„Ÿã˜ã¦ã„ã¾ã™ã€‚ç¶™ç¶šã—ã¦åƒããŸã„ã§ã™ã€‚",
            "è·å ´ã®{keyword1}ãŒå……å®Ÿã—ã¦ã„ã¦ã€{keyword2}ã«ç¹‹ãŒã£ã¦ã„ã¾ã™ã€‚",
            "{keyword1}ã®åˆ¶åº¦ãŒæ•´ã£ã¦ãŠã‚Šã€{keyword2}ã‚’å®Ÿæ„Ÿã—ã¦ã„ã¾ã™ã€‚",
            "åŒåƒšã‚„ä¸Šå¸ã¨ã®{keyword1}ãŒè‰¯å¥½ã§ã€{keyword2}ã‚’æ„Ÿã˜ã¦ã„ã¾ã™ã€‚",
            "ä¼šç¤¾ã®{keyword1}ã«å…±æ„Ÿã§ãã€{keyword2}ã‚’æŒã£ã¦åƒã„ã¦ã„ã¾ã™ã€‚"
        ]
        
        positive_keywords1 = ["æˆé•·æ©Ÿä¼š", "ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯", "è©•ä¾¡åˆ¶åº¦", "åŠ´åƒç’°å¢ƒ", "ç¦åˆ©åšç”Ÿ", "æ•™è‚²åˆ¶åº¦", "ãƒ“ã‚¸ãƒ§ãƒ³"]
        positive_keywords2 = ["ã‚„ã‚ŠãŒã„", "é”æˆæ„Ÿ", "å®‰å¿ƒæ„Ÿ", "æº€è¶³æ„Ÿ", "æˆé•·å®Ÿæ„Ÿ", "èª‡ã‚Š", "å¸Œæœ›"]
        
        template = np.random.choice(positive_templates)
        keyword1 = np.random.choice(positive_keywords1)
        keyword2 = np.random.choice(positive_keywords2)
        comment = template.format(keyword1=keyword1, keyword2=keyword2)
        
        data.append({
            'recommend_score': recommend_score,
            'overall_satisfaction': overall_satisfaction,
            'long_term_intention': long_term_intention,
            'sense_of_contribution': sense_of_contribution,
            'comment': comment,
            'is_low_satisfaction': 0
        })
    
    return pd.DataFrame(data)

def preprocess_text_features(comments):
    """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ã¨ç‰¹å¾´é‡æŠ½å‡º"""
    if len(comments) == 0:
        return pd.DataFrame(), None
    
    # ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    cleaned_comments = []
    for comment in comments:
        if pd.isna(comment):
            cleaned_comments.append("")
        else:
            text = str(comment).strip()
            text = re.sub(r'[^\w\s]', ' ', text)
            text = re.sub(r'\s+', ' ', text)
            cleaned_comments.append(text)
    
    try:
        # ã‚«ã‚¹ã‚¿ãƒ å½¢æ…‹ç´ è§£æå™¨ã‚’ä½¿ç”¨
        def custom_tokenizer(text):
            tokens = japanese_tokenizer(text)
            return tokens if tokens else ['']
        
        vectorizer = TfidfVectorizer(
            tokenizer=custom_tokenizer,
            max_features=50,  # ç‰¹å¾´é‡æ•°ã‚’èª¿æ•´
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2)
        )
        
        tfidf_matrix = vectorizer.fit_transform(cleaned_comments)
        feature_names = vectorizer.get_feature_names_out()
        
        tfidf_df = pd.DataFrame(
            tfidf_matrix.toarray(),
            columns=[f"word_{name}" for name in feature_names]
        )
        
        return tfidf_df, vectorizer
        
    except Exception as e:
        st.error(f"ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame(), None

def train_ensemble_models(X, y):
    """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    models = {
        'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=8),
        'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42, max_depth=8),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=50, random_state=42, max_depth=5)
    }
    
    trained_models = {}
    model_scores = {}
    
    for name, model in models.items():
        try:
            model.fit(X_train, y_train)
            cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')
            test_score = model.score(X_test, y_test)
            
            trained_models[name] = model
            model_scores[name] = {
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'test_score': test_score,
                'train_score': model.score(X_train, y_train)
            }
        except Exception as e:
            st.warning(f"{name}ã®è¨“ç·´ã§ã‚¨ãƒ©ãƒ¼: {e}")
    
    return trained_models, model_scores, X_test, y_test

def visualize_feature_importance(models, feature_names, top_n=15):
    """ç‰¹å¾´é‡é‡è¦æ€§ã®å¯è¦–åŒ–"""
    fig = make_subplots(
        rows=len(models), cols=1,
        subplot_titles=[f"{name} - Top {top_n} Important Features" for name in models.keys()],
        vertical_spacing=0.15
    )
    
    for i, (model_name, model) in enumerate(models.items(), 1):
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
            indices = np.argsort(importances)[::-1][:top_n]
            top_features = [feature_names[idx].replace('word_', '') for idx in indices]
            top_importances = importances[indices]
            
            fig.add_trace(
                go.Bar(
                    y=top_features[::-1],
                    x=top_importances[::-1],
                    orientation='h',
                    name=model_name,
                    showlegend=False,
                    marker_color=px.colors.qualitative.Set3[i-1]
                ),
                row=i, col=1
            )
    
    fig.update_layout(
        height=250 * len(models),
        title="ğŸ¯ ç‰¹å¾´é‡é‡è¦æ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆã©ã®å˜èªãŒä½æº€è¶³åº¦ã‚’äºˆæ¸¬ã™ã‚‹ã‹ï¼‰",
        title_font_size=18,
        title_x=0.5
    )
    
    fig.update_xaxes(title_text="é‡è¦æ€§ã‚¹ã‚³ã‚¢")
    
    return fig

def create_prediction_summary(models, model_scores):
    """äºˆæ¸¬çµæœã‚µãƒãƒªãƒ¼ã®ä½œæˆ"""
    summary_data = []
    
    for model_name, scores in model_scores.items():
        summary_data.append({
            'ãƒ¢ãƒ‡ãƒ«': model_name,
            'è¨“ç·´ç²¾åº¦': f"{scores['train_score']:.3f}",
            'CVç²¾åº¦': f"{scores['cv_mean']:.3f} (Â±{scores['cv_std']:.3f})",
            'ãƒ†ã‚¹ãƒˆç²¾åº¦': f"{scores['test_score']:.3f}",
            'æ±åŒ–æ€§èƒ½': "è‰¯å¥½" if scores['test_score'] > 0.7 else "è¦æ”¹å–„"
        })
    
    return pd.DataFrame(summary_data)

def main():
    # ãƒ¡ã‚¤ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼
    st.markdown("""
    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                color: white; padding: 2rem; border-radius: 12px; margin-bottom: 2rem; text-align: center;">
        <h1 style="margin: 0; color: white;">ğŸ¤– AI ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰</h1>
        <p style="margin: 0.5rem 0 0 0; opacity: 0.9;">å¾“æ¥­å“¡ã‚³ãƒ¡ãƒ³ãƒˆÃ—æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æº€è¶³åº¦äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ </p>
    </div>
    """, unsafe_allow_html=True)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.markdown("## ğŸ”§ è¨­å®š")
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨­å®š
        sample_size = st.slider("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æ•°", 100, 500, 200, 50)
        
        # åˆ†æå¯¾è±¡KPIé¸æŠ
        target_kpi = st.selectbox(
            "åˆ†æå¯¾è±¡KPI",
            ["ç·åˆæº€è¶³åº¦", "æ¨å¥¨ã‚¹ã‚³ã‚¢", "é•·æœŸå°±æ¥­æ„å‘", "æ´»èºè²¢çŒ®åº¦"]
        )
        
        st.markdown("---")
        st.markdown("### ğŸ“– ä½¿ç”¨æ–¹æ³•")
        st.info("""
        1. **ãƒ‡ãƒ¼ã‚¿æº–å‚™**: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        2. **ãƒ¢ãƒ‡ãƒ«è¨“ç·´**: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’å®Ÿè¡Œ
        3. **çµæœåˆ†æ**: ç‰¹å¾´é‡é‡è¦æ€§ã‚’ç¢ºèª
        4. **è§£é‡ˆ**: ã©ã®å˜èªãŒäºˆæ¸¬ã«é‡è¦ã‹ã‚’åˆ†æ
        """)
        
        st.markdown("---")
        st.markdown("### ğŸ¯ æŠ€è¡“ä»•æ§˜")
        st.code("""
        â€¢ å½¢æ…‹ç´ è§£æ: Janome
        â€¢ ç‰¹å¾´é‡æŠ½å‡º: TF-IDF
        â€¢ ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : 
          - Decision Tree
          - Random Forest  
          - Gradient Boosting
        â€¢ è©•ä¾¡: Cross Validation
        """)
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    with st.spinner("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­..."):
        df = create_enhanced_sample_data(sample_size)
        st.success(f"âœ… åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™å®Œäº†: {len(df)}ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«")
    
    # åŸºæœ¬çµ±è¨ˆ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        low_count = df['is_low_satisfaction'].sum()
        st.metric("ğŸ’¡ æ”¹å–„å¯¾è±¡è€…æ•°", f"{low_count}äºº", 
                 f"{low_count/len(df)*100:.1f}%")
    
    with col2:
        avg_recommend = df['recommend_score'].mean()
        st.metric("ğŸ“ˆ å¹³å‡æ¨å¥¨ã‚¹ã‚³ã‚¢", f"{avg_recommend:.1f}")
    
    with col3:
        avg_satisfaction = df['overall_satisfaction'].mean()
        st.metric("ğŸ˜Š å¹³å‡æº€è¶³åº¦", f"{avg_satisfaction:.1f}/5")
    
    with col4:
        unique_words = len(set(' '.join(df['comment']).split()))
        st.metric("ğŸ“ ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°", f"{unique_words:,}")
    
    # ã‚¿ãƒ–è¨­å®š
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ“Š ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–", 
        "ğŸ¤– æ©Ÿæ¢°å­¦ç¿’", 
        "ğŸ¯ ç‰¹å¾´é‡åˆ†æ", 
        "ğŸ’­ ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æ"
    ])
    
    with tab1:
        st.markdown("### ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã¨å‚¾å‘åˆ†æ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # æº€è¶³åº¦åˆ†å¸ƒ
            fig1 = px.histogram(
                df, x='overall_satisfaction', color='is_low_satisfaction',
                title="æº€è¶³åº¦åˆ†å¸ƒï¼ˆæ”¹å–„å¯¾è±¡è€…ã®ç‰¹å®šï¼‰",
                labels={'is_low_satisfaction': 'æ”¹å–„å¯¾è±¡', 'overall_satisfaction': 'ç·åˆæº€è¶³åº¦'},
                color_discrete_map={0: '#2E86AB', 1: '#F24236'},
                barmode='overlay'
            )
            fig1.update_layout(height=400)
            st.plotly_chart(fig1, use_container_width=True)
        
        with col2:
            # æ¨å¥¨ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
            fig2 = px.histogram(
                df, x='recommend_score', color='is_low_satisfaction',
                title="æ¨å¥¨ã‚¹ã‚³ã‚¢åˆ†å¸ƒï¼ˆNPSåˆ†æï¼‰",
                labels={'is_low_satisfaction': 'æ”¹å–„å¯¾è±¡', 'recommend_score': 'æ¨å¥¨ã‚¹ã‚³ã‚¢'},
                color_discrete_map={0: '#2E86AB', 1: '#F24236'},
                barmode='overlay'
            )
            fig2.update_layout(height=400)
            st.plotly_chart(fig2, use_container_width=True)
        
        # ç›¸é–¢åˆ†æ
        st.markdown("### ğŸ”— KPIé–“ã®ç›¸é–¢é–¢ä¿‚")
        corr_data = df[['recommend_score', 'overall_satisfaction', 'long_term_intention', 'sense_of_contribution']].corr()
        
        fig3 = px.imshow(
            corr_data,
            title="KPIç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹",
            color_continuous_scale='RdBu',
            zmin=-1, zmax=1
        )
        fig3.update_layout(height=400)
        st.plotly_chart(fig3, use_container_width=True)
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¡ãƒ³ãƒˆ
        st.markdown("### ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆã‚µãƒ³ãƒ—ãƒ«")
        sample_comments = df.sample(5)[['overall_satisfaction', 'recommend_score', 'is_low_satisfaction', 'comment']]
        sample_comments.columns = ['æº€è¶³åº¦', 'æ¨å¥¨ã‚¹ã‚³ã‚¢', 'æ”¹å–„å¯¾è±¡', 'ã‚³ãƒ¡ãƒ³ãƒˆ']
        st.dataframe(sample_comments, use_container_width=True)
    
    with tab2:
        st.markdown("### ğŸ¤– ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«")
        
        if st.button("ğŸš€ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚’å®Ÿè¡Œ", type="primary", use_container_width=True):
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡æŠ½å‡º
            status_text.text("ã‚¹ãƒ†ãƒƒãƒ— 1/3: ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡æŠ½å‡ºä¸­...")
            progress_bar.progress(33)
            
            with st.spinner("å½¢æ…‹ç´ è§£æã¨TF-IDFç‰¹å¾´é‡æŠ½å‡ºä¸­..."):
                text_features, vectorizer = preprocess_text_features(df['comment'])
                
                if len(text_features.columns) > 0:
                    # ã‚¹ãƒ†ãƒƒãƒ—2: ç‰¹å¾´é‡çµåˆ
                    status_text.text("ã‚¹ãƒ†ãƒƒãƒ— 2/3: ç‰¹å¾´é‡çµåˆä¸­...")
                    progress_bar.progress(66)
                    
                    numeric_features = df[['recommend_score', 'overall_satisfaction', 'long_term_intention', 'sense_of_contribution']]
                    X = pd.concat([numeric_features, text_features], axis=1)
                    y = df['is_low_satisfaction']
                    
                    st.success(f"âœ… ç‰¹å¾´é‡æº–å‚™å®Œäº†: {X.shape[1]}å€‹ã®ç‰¹å¾´é‡")
                    
                    # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                    status_text.text("ã‚¹ãƒ†ãƒƒãƒ— 3/3: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
                    progress_bar.progress(100)
                    
                    with st.spinner("3ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­..."):
                        models, scores, X_test, y_test = train_ensemble_models(X, y)
                        
                        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
                        st.session_state['ml_models'] = models
                        st.session_state['ml_scores'] = scores
                        st.session_state['ml_feature_names'] = X.columns.tolist()
                        st.session_state['ml_vectorizer'] = vectorizer
                        st.session_state['ml_X'] = X
                        st.session_state['ml_y'] = y
                        
                        status_text.text("âœ… è¨“ç·´å®Œäº†!")
                        progress_bar.empty()
                        
                        st.balloons()
                        st.success("ğŸ‰ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                        
                        # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚µãƒãƒªãƒ¼
                        st.markdown("### ğŸ“ˆ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ")
                        summary_df = create_prediction_summary(models, scores)
                        st.dataframe(summary_df, use_container_width=True)
                        
                        # æ€§èƒ½è©•ä¾¡ã®ã‚¬ã‚¤ãƒ‰
                        st.info("""
                        **æ€§èƒ½è©•ä¾¡ã®è¦‹æ–¹:**
                        - **è¨“ç·´ç²¾åº¦**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ç²¾åº¦
                        - **CVç²¾åº¦**: ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç²¾åº¦ï¼ˆæ±åŒ–æ€§èƒ½ã®æŒ‡æ¨™ï¼‰
                        - **ãƒ†ã‚¹ãƒˆç²¾åº¦**: æœªçŸ¥ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ç²¾åº¦
                        - **æ±åŒ–æ€§èƒ½**: éå­¦ç¿’ã—ã¦ã„ãªã„ã‹ã®åˆ¤å®š
                        """)
                        
                else:
                    st.error("âŒ ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    with tab3:
        st.markdown("### ğŸ¯ ç‰¹å¾´é‡é‡è¦æ€§åˆ†æ")
        
        if 'ml_models' in st.session_state:
            models = st.session_state['ml_models']
            feature_names = st.session_state['ml_feature_names']
            
            # ç‰¹å¾´é‡é‡è¦æ€§å¯è¦–åŒ–
            fig_importance = visualize_feature_importance(models, feature_names, top_n=12)
            st.plotly_chart(fig_importance, use_container_width=True)
            
            # è©³ç´°åˆ†æ
            st.markdown("### ğŸ“ é‡è¦ç‰¹å¾´é‡ã®è©³ç´°åˆ†æ")
            
            selected_model = st.selectbox(
                "è©³ç´°åˆ†æã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ:",
                list(models.keys())
            )
            
            if selected_model in models:
                model = models[selected_model]
                if hasattr(model, 'feature_importances_'):
                    importances = model.feature_importances_
                    indices = np.argsort(importances)[::-1][:15]
                    
                    detailed_data = []
                    for i, idx in enumerate(indices):
                        feature_name = feature_names[idx]
                        importance = importances[idx]
                        
                        if feature_name.startswith('word_'):
                            word = feature_name.replace('word_', '')
                            feature_type = "ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡"
                            impact = "ã“ã®å˜èªãŒå«ã¾ã‚Œã‚‹ã¨ä½æº€è¶³åº¦ã«ãªã‚Šã‚„ã™ã„" if importance > 0.05 else "å½±éŸ¿ã¯è»½å¾®"
                        else:
                            word = feature_name
                            feature_type = "ğŸ“Š æ•°å€¤ç‰¹å¾´é‡"
                            impact = "KPIå€¤ãŒä½ã„ã¨ä½æº€è¶³åº¦ã«ãªã‚Šã‚„ã™ã„" if importance > 0.1 else "å½±éŸ¿ã¯è»½å¾®"
                        
                        detailed_data.append({
                            'ãƒ©ãƒ³ã‚¯': i + 1,
                            'ç‰¹å¾´é‡': word,
                            'ã‚¿ã‚¤ãƒ—': feature_type,
                            'é‡è¦æ€§': f"{importance:.4f}",
                            'å½±éŸ¿åº¦': impact
                        })
                    
                    st.dataframe(pd.DataFrame(detailed_data), use_container_width=True)
            
            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³
            st.markdown("### ğŸ’¡ æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³")
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### ğŸ¯ å„ªå…ˆæ”¹å–„é ˜åŸŸ")
                st.success("""
                **é«˜é‡è¦åº¦ã®ãƒã‚¬ãƒ†ã‚£ãƒ–å˜èªã‚’ç‰¹å®šã—ã€ä»¥ä¸‹ã‚’å®Ÿæ–½:**
                - è©²å½“ã™ã‚‹å•é¡Œã®æ ¹æœ¬åŸå› åˆ†æ
                - å…·ä½“çš„ãªæ”¹å–„æ–½ç­–ã®ç«‹æ¡ˆ
                - æ”¹å–„åŠ¹æœã®æ¸¬å®šã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
                """)
            
            with col2:
                st.markdown("#### ğŸ“‹ ç¶™ç¶šæ–½ç­–")
                st.info("""
                **ãƒã‚¸ãƒ†ã‚£ãƒ–å˜èªã®é‡è¦åº¦ã‚’é«˜ã‚ã‚‹ãŸã‚ã«:**
                - æº€è¶³åº¦å‘ä¸Šæ–½ç­–ã®ç¶™ç¶šå®Ÿæ–½
                - å¾“æ¥­å“¡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå‘ä¸Š
                - å®šæœŸçš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†
                """)
                
        else:
            st.info("ã¾ãšã€ŒğŸ¤– æ©Ÿæ¢°å­¦ç¿’ã€ã‚¿ãƒ–ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦ãã ã•ã„ã€‚")
    
    with tab4:
        st.markdown("### ğŸ’­ ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æã¨ã‚¤ãƒ³ã‚µã‚¤ãƒˆ")
        
        # ã‚³ãƒ¡ãƒ³ãƒˆé•·åˆ†æ
        df['comment_length'] = df['comment'].str.len()
        
        col1, col2 = st.columns(2)
        
        with col1:
            fig_length = px.histogram(
                df, x='comment_length', color='is_low_satisfaction',
                title="ã‚³ãƒ¡ãƒ³ãƒˆæ–‡å­—æ•°åˆ†å¸ƒ",
                labels={'comment_length': 'æ–‡å­—æ•°', 'is_low_satisfaction': 'æ”¹å–„å¯¾è±¡'},
                color_discrete_map={0: '#2E86AB', 1: '#F24236'}
            )
            st.plotly_chart(fig_length, use_container_width=True)
        
        with col2:
            # æ„Ÿæƒ…åˆ†æé¢¨ã®å¯è¦–åŒ–
            sentiment_data = []
            for idx, row in df.iterrows():
                positive_words = ['æº€è¶³', 'è‰¯ã„', 'ç´ æ™´ã‚‰ã—ã„', 'å……å®Ÿ', 'æˆé•·', 'ã‚„ã‚ŠãŒã„', 'é”æˆ']
                negative_words = ['ä¸æº€', 'å•é¡Œ', 'èª²é¡Œ', 'å³ã—ã„', 'å¤§å¤‰', 'å›°é›£', 'ã‚¹ãƒˆãƒ¬ã‚¹']
                
                pos_count = sum(1 for word in positive_words if word in row['comment'])
                neg_count = sum(1 for word in negative_words if word in row['comment'])
                sentiment_score = pos_count - neg_count
                
                sentiment_data.append({
                    'sentiment_score': sentiment_score,
                    'is_low_satisfaction': row['is_low_satisfaction']
                })
            
            sentiment_df = pd.DataFrame(sentiment_data)
            fig_sentiment = px.histogram(
                sentiment_df, x='sentiment_score', color='is_low_satisfaction',
                title="æ„Ÿæƒ…ã‚¹ã‚³ã‚¢åˆ†å¸ƒï¼ˆæ­£-è² å˜èªãƒãƒ©ãƒ³ã‚¹ï¼‰",
                labels={'sentiment_score': 'æ„Ÿæƒ…ã‚¹ã‚³ã‚¢', 'is_low_satisfaction': 'æ”¹å–„å¯¾è±¡'},
                color_discrete_map={0: '#2E86AB', 1: '#F24236'}
            )
            st.plotly_chart(fig_sentiment, use_container_width=True)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
        st.markdown("### ğŸ” é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ")
        
        # ä½æº€è¶³åº¦ã‚°ãƒ«ãƒ¼ãƒ—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        low_comments = df[df['is_low_satisfaction'] == 1]['comment'].str.cat(sep=' ')
        high_comments = df[df['is_low_satisfaction'] == 0]['comment'].str.cat(sep=' ')
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### âš ï¸ æ”¹å–„å¯¾è±¡è€…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
            low_words = japanese_tokenizer(low_comments)
            low_word_freq = pd.Series(low_words).value_counts().head(10)
            
            fig_low = px.bar(
                x=low_word_freq.values,
                y=low_word_freq.index,
                orientation='h',
                title="é »å‡ºãƒã‚¬ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
                color_discrete_sequence=['#F24236']
            )
            fig_low.update_layout(height=400)
            st.plotly_chart(fig_low, use_container_width=True)
        
        with col2:
            st.markdown("#### âœ… æº€è¶³è€…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
            high_words = japanese_tokenizer(high_comments)
            high_word_freq = pd.Series(high_words).value_counts().head(10)
            
            fig_high = px.bar(
                x=high_word_freq.values,
                y=high_word_freq.index,
                orientation='h',
                title="é »å‡ºãƒã‚¸ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
                color_discrete_sequence=['#2E86AB']
            )
            fig_high.update_layout(height=400)
            st.plotly_chart(fig_high, use_container_width=True)

if __name__ == "__main__":
    main()